name: Benchmark PR Comparison

on:
  pull_request:
    branches: [master, performance_benchmarkdotnet_workflow]
    types: [opened, labeled, synchronize, reopened]
  push:
    branches: [master]
  workflow_dispatch:

concurrency:
  group: benchmark-pr-comparison-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write

jobs:
  benchmark-shards:
    if: >
      github.event_name != 'pull_request' ||
      contains(github.event.pull_request.labels.*.name, 'performance is good')
    strategy:
      fail-fast: false
      max-parallel: 5
      matrix:
        shard: [1, 2, 3, 4, 5]
    runs-on: [benchmark]
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up .NET
        uses: actions/setup-dotnet@v4

      - name: Run benchmark shard
        shell: bash
        env:
          NETH_BENCHMARK_INCLUDE_PRECOMPILES: "false"
          NETH_BENCHMARK_JOB: "medium"
          NETH_BENCHMARK_INPROCESS: "false"
        run: |
          set -euo pipefail

          artifact_root=".benchmark-artifacts/shard-${{ matrix.shard }}"
          filters=()

          case "${{ matrix.shard }}" in
            1)
              filters=(
                "*Nethermind.Benchmarks.Blockchain.BlockProcessingBenchmarks*"
                "*Nethermind.Evm.Benchmark.EvmBenchmarks*"
              )
              ;;
            2)
              filters=(
                "*Nethermind.Evm.Benchmark.EvmStackBenchmarks.PushPopUInt256Loop*"
                "*Nethermind.Evm.Benchmark.EvmStackBenchmarks.PushPopByteLoop*"
              )
              ;;
            3)
              filters=(
                "*Nethermind.Evm.Benchmark.EvmStackBenchmarks.DupDeep*"
                "*Nethermind.Evm.Benchmark.EvmStackBenchmarks.SwapDeep*"
              )
              ;;
            4)
              filters=(
                "*Nethermind.Evm.Benchmark.EvmStackBenchmarks.MixedStackPattern*"
                "*Nethermind.Benchmarks.Evm.SignExtendBenchmark*"
                "*Nethermind.Benchmarks.Evm.JumpDestinationsBenchmark*"
              )
              ;;
            5)
              filters=(
                "*Nethermind.Benchmarks.Evm.BitwiseAndBenchmark*"
                "*Nethermind.Benchmarks.Evm.BitwiseOrBenchmark*"
                "*Nethermind.Benchmarks.Evm.BitwiseXorBenchmark*"
                "*Nethermind.Benchmarks.Evm.BitwiseNotBenchmark*"
                "*Nethermind.Benchmarks.Evm.Blake2Benchmark*"
                "*Nethermind.Benchmarks.Evm.EcRecoverBenchmark*"
                "*Nethermind.Benchmarks.Evm.MemoryCostBenchmark*"
              )
              ;;
            *)
              echo "Unsupported benchmark shard '${{ matrix.shard }}'."
              exit 1
              ;;
          esac

          echo "Running benchmark shard ${{ matrix.shard }} with filters:"
          for filter in "${filters[@]}"; do
            echo " - $filter"
          done

          dotnet run --project src/Nethermind/Nethermind.Benchmark.Runner/Nethermind.Benchmark.Runner.csproj -c release -- --filter "${filters[@]}" --artifacts "$artifact_root"

      - name: Fail shard on benchmark execution issues
        shell: bash
        run: |
          set -euo pipefail

          results_dir=".benchmark-artifacts/shard-${{ matrix.shard }}/results"
          if [ ! -d "$results_dir" ]; then
            echo "Missing results directory: $results_dir"
            exit 1
          fi

          pybin=""
          if command -v python3 >/dev/null 2>&1; then
            pybin="python3"
          elif command -v python >/dev/null 2>&1; then
            pybin="python"
          else
            echo "Python is required to validate CSV benchmark results."
            exit 1
          fi

          {
            echo "import csv"
            echo "import glob"
            echo "import os"
            echo "import sys"
            echo "results_dir = sys.argv[1]"
            echo "csv_files = sorted(glob.glob(os.path.join(results_dir, '*-report.csv')))"
            echo "if not csv_files:"
            echo "    raise SystemExit(f\"No benchmark report CSV files were produced for shard in '{results_dir}'.\")"
            echo "failed = []"
            echo "for path in csv_files:"
            echo "    with open(path, newline='', encoding='utf-8') as f:"
            echo "        reader = csv.DictReader(f)"
            echo "        for row in reader:"
            echo "            mean = (row.get('Mean') or '').strip()"
            echo "            if mean.upper() == 'NA':"
            echo "                failed.append(f\"{os.path.basename(path)}:{(row.get('Method') or '').strip()}\")"
            echo "if failed:"
            echo "    print('Benchmarks with non-numeric mean values detected:')"
            echo "    for entry in failed:"
            echo "        print(entry)"
            echo "    raise SystemExit('Benchmark execution issues detected (Mean=NA).')"
          } | "$pybin" - "$results_dir"

      - name: Upload shard benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-shard-${{ matrix.shard }}
          path: .benchmark-artifacts/shard-${{ matrix.shard }}/results/**
          retention-days: 7

  aggregate-and-compare:
    if: >
      github.event_name != 'pull_request' ||
      contains(github.event.pull_request.labels.*.name, 'performance is good')
    needs: benchmark-shards
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Compute baseline cache key
        id: baseline-key
        shell: bash
        run: |
          set -euo pipefail
          files=(
            "global.json"
            "src/Nethermind/Nethermind.Benchmark.Runner/Nethermind.Benchmark.Runner.csproj"
            "src/Nethermind/Nethermind.Benchmark/Nethermind.Benchmark.csproj"
            "src/Nethermind/Nethermind.Evm.Benchmark/Nethermind.Evm.Benchmark.csproj"
          )
          fingerprint="$(sha256sum "${files[@]}" | sha256sum | cut -d' ' -f1)"
          echo "prefix=nethermind-benchmark-baseline-linux-${fingerprint}-" >> "$GITHUB_OUTPUT"

      - name: Restore master benchmark baseline
        if: github.event_name == 'pull_request'
        id: restore-baseline
        uses: actions/cache/restore@v4
        with:
          path: .benchmark-baseline
          key: ${{ steps.baseline-key.outputs.prefix }}current-run
          restore-keys: |
            ${{ steps.baseline-key.outputs.prefix }}

      - name: Download benchmark shard artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-shard-*
          path: .benchmark-shards

      - name: Combine benchmark shard reports
        shell: pwsh
        run: |
          New-Item -Path .benchmark-artifacts/results -ItemType Directory -Force | Out-Null

          $csvFiles = @(Get-ChildItem -Path .benchmark-shards -Recurse -Filter "*-report.csv" -File)
          if ($csvFiles.Count -eq 0) {
            throw "No benchmark CSV files were downloaded from shards."
          }

          foreach ($csv in $csvFiles) {
            $destination = Join-Path .benchmark-artifacts/results $csv.Name
            if (Test-Path $destination) {
              throw "Duplicate benchmark report '$($csv.Name)' detected across shard artifacts."
            }

            Copy-Item -Path $csv.FullName -Destination $destination
          }

          Write-Host "Combined $($csvFiles.Count) benchmark report files."

      - name: Fail on benchmark execution issues
        shell: pwsh
        run: |
          $failed = @()
          $csvFiles = @(Get-ChildItem -Path .benchmark-artifacts/results -Filter "*-report.csv" -File)
          foreach ($csv in $csvFiles) {
            $rows = @(Import-Csv -Path $csv.FullName)
            foreach ($row in $rows) {
              if ([string]::Equals(([string]$row.Mean).Trim(), "NA", [System.StringComparison]::OrdinalIgnoreCase)) {
                $failed += "$($csv.Name):$($row.Method)"
              }
            }
          }

          if ($failed.Count -gt 0) {
            Write-Host "Benchmarks with non-numeric mean values detected:"
            foreach ($entry in $failed) {
              Write-Host $entry
            }
            throw "Benchmark execution issues detected (Mean=NA)."
          }

      - name: Aggregate benchmark results
        shell: pwsh
        run: |
          ./scripts/benchmarks/aggregate-benchmark-results.ps1 `
            -ResultsDir .benchmark-artifacts/results `
            -OutputPath .benchmark-results/benchmark-summary.json

      - name: Prepare and save master baseline cache
        if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
        run: |
          mkdir -p .benchmark-baseline
          cp .benchmark-results/benchmark-summary.json .benchmark-baseline/benchmark-summary.json

      - name: Save master benchmark baseline
        if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
        uses: actions/cache/save@v4
        with:
          path: .benchmark-baseline
          key: ${{ steps.baseline-key.outputs.prefix }}${{ github.sha }}

      - name: Compare PR benchmarks with master baseline
        if: github.event_name == 'pull_request'
        shell: pwsh
        run: |
          ./scripts/benchmarks/compare-benchmark-results.ps1 `
            -CurrentPath .benchmark-results/benchmark-summary.json `
            -BaselinePath .benchmark-baseline/benchmark-summary.json `
            -OutputJsonPath .benchmark-results/benchmark-comparison.json `
            -OutputMarkdownPath .benchmark-results/benchmark-comparison.md `
            -RegressionThresholdPercent 12 `
            -MinAbsoluteDeltaNs 50000 `
            -TopCount 15

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            .benchmark-artifacts/results/**
            .benchmark-results/benchmark-summary.json
            .benchmark-results/benchmark-comparison.json
            .benchmark-results/benchmark-comparison.md
          retention-days: 7

      - name: Comment benchmark comparison on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require("fs");
            const marker = "<!-- benchmark-comparison -->";
            const path = ".benchmark-results/benchmark-comparison.md";
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            let body = fs.readFileSync(path, "utf8");
            body += `\n\n[Workflow run details](${runUrl})`;
            const issue_number = context.payload.pull_request.number;
            const { owner, repo } = context.repo;

            const comments = await github.paginate(github.rest.issues.listComments, {
              owner,
              repo,
              issue_number,
              per_page: 100
            });

            const existing = comments.find((comment) => comment.body && comment.body.includes(marker));

            if (existing) {
              await github.rest.issues.updateComment({
                owner,
                repo,
                comment_id: existing.id,
                body
              });
            } else {
              await github.rest.issues.createComment({
                owner,
                repo,
                issue_number,
                body
              });
            }
