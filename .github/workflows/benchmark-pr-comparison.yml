name: Benchmark PR Comparison

on:
  pull_request:
    branches: [master, performance_benchmarkdotnet_workflow]
    types: [opened, labeled, synchronize, reopened]
  push:
    branches: [master]
  workflow_dispatch:

concurrency:
  group: benchmark-pr-comparison-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write

jobs:
  prepare-shards:
    if: >
      github.event_name != 'pull_request' ||
      contains(github.event.pull_request.labels.*.name, 'performance is good')
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up .NET
        uses: actions/setup-dotnet@v4

      - name: Build dynamic shard plan
        shell: bash
        env:
          SHARD_COUNT: "5"
          NETH_BENCHMARK_INCLUDE_PRECOMPILES: "false"
          NETH_BENCHMARK_JOB: "dry"
          NETH_BENCHMARK_INPROCESS: "false"
        run: |
          set -euo pipefail

          mkdir -p .benchmark-shards
          list_file=".benchmark-shards/flat-list.txt"

          dotnet run --project src/Nethermind/Nethermind.Benchmark.Runner/Nethermind.Benchmark.Runner.csproj -c release -- \
            --filter '*Nethermind.Benchmarks.Blockchain.BlockProcessingBenchmarks*' '*Nethermind.Benchmarks.Evm.*' '*Nethermind.Evm.Benchmark.EvmStackBenchmarks*' '*Nethermind.Evm.Benchmark.EvmBenchmarks*' \
            --list flat > "$list_file"

          python3 scripts/benchmarks/plan-benchmark-shards.py \
            --input "$list_file" \
            --output-dir .benchmark-shards \
            --shard-count "$SHARD_COUNT"

      - name: Upload shard plan
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-shard-plan
          path: .benchmark-shards/shard-*.txt
          retention-days: 7

  benchmark-shards:
    if: >
      github.event_name != 'pull_request' ||
      contains(github.event.pull_request.labels.*.name, 'performance is good')
    needs: prepare-shards
    strategy:
      fail-fast: false
      max-parallel: 5
      matrix:
        shard: [1, 2, 3, 4, 5]
    runs-on: [self-hosted, benchmark]
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up .NET
        uses: actions/setup-dotnet@v4

      - name: Download shard plan
        uses: actions/download-artifact@v4
        with:
          name: benchmark-shard-plan
          path: .benchmark-shards

      - name: Run benchmark shard
        id: run-shard
        shell: bash
        env:
          NETH_BENCHMARK_INCLUDE_PRECOMPILES: "false"
          NETH_BENCHMARK_JOB: "medium"
          NETH_BENCHMARK_INPROCESS: "false"
        run: |
          set -euo pipefail

          artifact_root=".benchmark-artifacts/shard-${{ matrix.shard }}"
          plan_file=".benchmark-shards/shard-${{ matrix.shard }}.txt"
          if [ ! -f "$plan_file" ]; then
            echo "Missing shard plan file: $plan_file"
            exit 1
          fi

          mapfile -t filters < <(grep -vE '^[[:space:]]*$' "$plan_file" || true)
          if [ "${#filters[@]}" -eq 0 ]; then
            echo "No filters assigned to shard ${{ matrix.shard }}. Skipping execution."
            mkdir -p "$artifact_root/results"
            echo "has_filters=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "has_filters=true" >> "$GITHUB_OUTPUT"

          echo "Running benchmark shard ${{ matrix.shard }} with filters:"
          for filter in "${filters[@]}"; do
            echo " - $filter"
          done

          dotnet run --project src/Nethermind/Nethermind.Benchmark.Runner/Nethermind.Benchmark.Runner.csproj -c release -- --filter "${filters[@]}" --artifacts "$artifact_root"

      - name: Fail shard on benchmark execution issues
        if: steps.run-shard.outputs.has_filters == 'true'
        shell: bash
        run: |
          set -euo pipefail

          results_dir=".benchmark-artifacts/shard-${{ matrix.shard }}/results"
          if [ ! -d "$results_dir" ]; then
            echo "Missing results directory: $results_dir"
            exit 1
          fi

          python3 scripts/benchmarks/validate-benchmark-results.py --results-dir "$results_dir"

      - name: Upload shard benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-shard-${{ matrix.shard }}
          path: .benchmark-artifacts/shard-${{ matrix.shard }}/results/**
          if-no-files-found: warn
          retention-days: 7

  aggregate-and-compare:
    if: >
      github.event_name != 'pull_request' ||
      contains(github.event.pull_request.labels.*.name, 'performance is good')
    needs: benchmark-shards
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Compute baseline cache key
        id: baseline-key
        shell: bash
        run: |
          set -euo pipefail
          files=(
            "global.json"
            "src/Nethermind/Nethermind.Benchmark.Runner/Nethermind.Benchmark.Runner.csproj"
            "src/Nethermind/Nethermind.Benchmark/Nethermind.Benchmark.csproj"
            "src/Nethermind/Nethermind.Evm.Benchmark/Nethermind.Evm.Benchmark.csproj"
          )
          fingerprint="$(sha256sum "${files[@]}" | sha256sum | cut -d' ' -f1)"
          echo "prefix=nethermind-benchmark-baseline-linux-${fingerprint}-" >> "$GITHUB_OUTPUT"

      - name: Restore master benchmark baseline
        if: github.event_name == 'pull_request'
        id: restore-baseline
        uses: actions/cache/restore@v4
        with:
          path: .benchmark-baseline
          key: ${{ steps.baseline-key.outputs.prefix }}current-run
          restore-keys: |
            ${{ steps.baseline-key.outputs.prefix }}

      - name: Download benchmark shard artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-shard-*
          path: .benchmark-shards

      - name: Combine benchmark shard reports
        shell: pwsh
        run: |
          New-Item -Path .benchmark-artifacts/results -ItemType Directory -Force | Out-Null

          $csvFiles = @(Get-ChildItem -Path .benchmark-shards -Recurse -Filter "*-report.csv" -File)
          if ($csvFiles.Count -eq 0) {
            throw "No benchmark CSV files were downloaded from shards."
          }

          foreach ($csv in $csvFiles) {
            $destination = Join-Path .benchmark-artifacts/results $csv.Name
            if (Test-Path $destination) {
              throw "Duplicate benchmark report '$($csv.Name)' detected across shard artifacts."
            }

            Copy-Item -Path $csv.FullName -Destination $destination
          }

          Write-Host "Combined $($csvFiles.Count) benchmark report files."

      - name: Fail on benchmark execution issues
        shell: pwsh
        run: |
          $failed = @()
          $csvFiles = @(Get-ChildItem -Path .benchmark-artifacts/results -Filter "*-report.csv" -File)
          foreach ($csv in $csvFiles) {
            $rows = @(Import-Csv -Path $csv.FullName)
            foreach ($row in $rows) {
              if ([string]::Equals(([string]$row.Mean).Trim(), "NA", [System.StringComparison]::OrdinalIgnoreCase)) {
                $failed += "$($csv.Name):$($row.Method)"
              }
            }
          }

          if ($failed.Count -gt 0) {
            Write-Host "Benchmarks with non-numeric mean values detected:"
            foreach ($entry in $failed) {
              Write-Host $entry
            }
            throw "Benchmark execution issues detected (Mean=NA)."
          }

      - name: Aggregate benchmark results
        shell: pwsh
        run: |
          ./scripts/benchmarks/aggregate-benchmark-results.ps1 `
            -ResultsDir .benchmark-artifacts/results `
            -OutputPath .benchmark-results/benchmark-summary.json

      - name: Prepare and save master baseline cache
        if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
        run: |
          mkdir -p .benchmark-baseline
          cp .benchmark-results/benchmark-summary.json .benchmark-baseline/benchmark-summary.json

      - name: Save master benchmark baseline
        if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
        uses: actions/cache/save@v4
        with:
          path: .benchmark-baseline
          key: ${{ steps.baseline-key.outputs.prefix }}${{ github.sha }}

      - name: Compare PR benchmarks with master baseline
        if: github.event_name == 'pull_request'
        shell: pwsh
        run: |
          ./scripts/benchmarks/compare-benchmark-results.ps1 `
            -CurrentPath .benchmark-results/benchmark-summary.json `
            -BaselinePath .benchmark-baseline/benchmark-summary.json `
            -OutputJsonPath .benchmark-results/benchmark-comparison.json `
            -OutputMarkdownPath .benchmark-results/benchmark-comparison.md `
            -RegressionThresholdPercent 12 `
            -MinAbsoluteDeltaNs 50000 `
            -TopCount 15

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            .benchmark-artifacts/results/**
            .benchmark-results/benchmark-summary.json
            .benchmark-results/benchmark-comparison.json
            .benchmark-results/benchmark-comparison.md
          retention-days: 7

      - name: Comment benchmark comparison on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require("fs");
            const marker = "<!-- benchmark-comparison -->";
            const path = ".benchmark-results/benchmark-comparison.md";
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            let body = fs.readFileSync(path, "utf8");
            body += `\n\n[Workflow run details](${runUrl})`;
            const issue_number = context.payload.pull_request.number;
            const { owner, repo } = context.repo;

            const comments = await github.paginate(github.rest.issues.listComments, {
              owner,
              repo,
              issue_number,
              per_page: 100
            });

            const existing = comments.find((comment) => comment.body && comment.body.includes(marker));

            if (existing) {
              await github.rest.issues.updateComment({
                owner,
                repo,
                comment_id: existing.id,
                body
              });
            } else {
              await github.rest.issues.createComment({
                owner,
                repo,
                issue_number,
                body
              });
            }
