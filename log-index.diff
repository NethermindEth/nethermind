diff --git a/src/Nethermind/Nethermind.Api/IApiWithStores.cs b/src/Nethermind/Nethermind.Api/IApiWithStores.cs
index 1bdda50b43..b5aaa4e01d 100644
--- a/src/Nethermind/Nethermind.Api/IApiWithStores.cs
+++ b/src/Nethermind/Nethermind.Api/IApiWithStores.cs
@@ -7,6 +7,7 @@ using Nethermind.Blockchain.Receipts;
 using Nethermind.Consensus;
 using Nethermind.Core;
 using Nethermind.Crypto;
+using Nethermind.Db;
 using Nethermind.Db.Blooms;
 using Nethermind.Facade.Find;
 using Nethermind.History;
@@ -23,6 +24,7 @@ namespace Nethermind.Api
         IBloomStorage? BloomStorage { get; set; }
         IChainLevelInfoRepository? ChainLevelInfoRepository { get; set; }
         ILogFinder? LogFinder { get; set; }
+        ILogIndexStorage? LogIndexStorage { get; set; }
         ISigner? EngineSigner { get; set; }
         ISignerStore? EngineSignerStore { get; set; }
         [SkipServiceCollection]
diff --git a/src/Nethermind/Nethermind.Api/NethermindApi.cs b/src/Nethermind/Nethermind.Api/NethermindApi.cs
index 2a35b766e2..33df38d899 100644
--- a/src/Nethermind/Nethermind.Api/NethermindApi.cs
+++ b/src/Nethermind/Nethermind.Api/NethermindApi.cs
@@ -94,6 +94,7 @@ namespace Nethermind.Api
         public IJsonSerializer EthereumJsonSerializer => _dependencies.JsonSerializer;
         public IKeyStore? KeyStore { get; set; }
         public ILogFinder? LogFinder { get; set; }
+        public ILogIndexStorage? LogIndexStorage { get; set; }
         public ILogManager LogManager => _dependencies.LogManager;
         public IMessageSerializationService MessageSerializationService => Context.Resolve<IMessageSerializationService>();
         public IGossipPolicy GossipPolicy { get; set; } = Policy.FullGossip;
diff --git a/src/Nethermind/Nethermind.Blockchain.Test/Find/LogFinderTests.cs b/src/Nethermind/Nethermind.Blockchain.Test/Find/LogFinderTests.cs
index e8ae192041..6be3f39e93 100644
--- a/src/Nethermind/Nethermind.Blockchain.Test/Find/LogFinderTests.cs
+++ b/src/Nethermind/Nethermind.Blockchain.Test/Find/LogFinderTests.cs
@@ -26,6 +26,7 @@ using NUnit.Framework;
 
 namespace Nethermind.Blockchain.Test.Find;
 
+// TODO: tests with different LogIndexStorage states
 public class LogFinderTests
 {
     private IBlockTree _blockTree = null!;
@@ -35,6 +36,7 @@ public class LogFinderTests
     private IBloomStorage _bloomStorage = null!;
     private IReceiptsRecovery _receiptsRecovery = null!;
     private Block _headTestBlock = null!;
+    private ILogIndexStorage? _logIndexStorage = null;
 
     [SetUp]
     public void SetUp()
@@ -43,7 +45,10 @@ public class LogFinderTests
     }
 
     [TearDown]
-    public void TearDown() => _bloomStorage?.Dispose();
+    public void TearDown()
+    {
+        _bloomStorage?.Dispose();
+    }
 
     private void SetUp(bool allowReceiptIterator)
     {
@@ -57,7 +62,7 @@ public class LogFinderTests
         _blockTree = _rawBlockTree;
         _bloomStorage = new BloomStorage(new BloomConfig(), new MemDb(), new InMemoryDictionaryFileStoreFactory());
         _receiptsRecovery = Substitute.For<IReceiptsRecovery>();
-        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery);
+        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery, _logIndexStorage);
     }
 
     private void SetupHeadWithNoTransaction()
@@ -145,7 +150,7 @@ public class LogFinderTests
     {
         StoreTreeBlooms(withBloomDb);
         _receiptStorage = NullReceiptStorage.Instance;
-        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery);
+        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery, _logIndexStorage);
 
         var logFilter = AllBlockFilter().Build();
 
@@ -158,7 +163,7 @@ public class LogFinderTests
     public void when_receipts_are_missing_and_header_has_no_receipt_root_do_not_throw_exception_()
     {
         _receiptStorage = NullReceiptStorage.Instance;
-        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery);
+        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery, _logIndexStorage);
 
         SetupHeadWithNoTransaction();
 
@@ -174,7 +179,7 @@ public class LogFinderTests
     {
         StoreTreeBlooms(withBloomDb);
         var blockFinder = Substitute.For<IBlockFinder>();
-        _logFinder = new LogFinder(blockFinder, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery);
+        _logFinder = new LogFinder(blockFinder, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery, _logIndexStorage);
         var logFilter = AllBlockFilter().Build();
         var action = new Func<IEnumerable<FilterLog>>(() => _logFinder.FindLogs(logFilter));
         action.Should().Throw<ResourceNotFoundException>();
@@ -277,7 +282,7 @@ public class LogFinderTests
     public void filter_by_blocks_with_limit([ValueSource(nameof(WithBloomValues))] bool withBloomDb)
     {
         StoreTreeBlooms(withBloomDb);
-        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery, 2);
+        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery, _logIndexStorage);
         var filter = FilterBuilder.New().FromLatestBlock().ToLatestBlock().Build();
         var logs = _logFinder.FindLogs(filter).ToArray();
 
@@ -323,7 +328,7 @@ public class LogFinderTests
         CancellationToken cancellationToken = cancellationTokenSource.Token;
 
         StoreTreeBlooms(true);
-        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery);
+        _logFinder = new LogFinder(_blockTree, _receiptStorage, _receiptStorage, _bloomStorage, LimboLogs.Instance, _receiptsRecovery, _logIndexStorage);
         var logFilter = AllBlockFilter().Build();
         var logs = _logFinder.FindLogs(logFilter, cancellationToken);
 
diff --git a/src/Nethermind/Nethermind.Blockchain.Test/Find/LogIndexStorageFilterTests.cs b/src/Nethermind/Nethermind.Blockchain.Test/Find/LogIndexStorageFilterTests.cs
new file mode 100644
index 0000000000..4b36b81392
--- /dev/null
+++ b/src/Nethermind/Nethermind.Blockchain.Test/Find/LogIndexStorageFilterTests.cs
@@ -0,0 +1,270 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Collections;
+using System.Collections.Generic;
+using System.Linq;
+using System.Threading;
+using System.Threading.Tasks;
+using Nethermind.Blockchain.Filters;
+using Nethermind.Blockchain.Test.Builders;
+using Nethermind.Core;
+using Nethermind.Core.Crypto;
+using Nethermind.Core.Test.Builders;
+using Nethermind.Db;
+using Nethermind.Facade.Find;
+using NSubstitute;
+using NUnit.Framework;
+
+namespace Nethermind.Blockchain.Test.Find;
+
+public class LogIndexStorageFilterTests
+{
+    private record Ranges(Dictionary<Address, List<int>> Address, Dictionary<Hash256, List<int>>[] Topic)
+    {
+        public List<int> this[Address address] => Address[address];
+        public List<int> this[int topicIndex, Hash256 hash] => Topic[topicIndex][hash];
+    }
+
+    private ILogIndexStorage _logIndexStorage = null!;
+
+    [SetUp]
+    public void SetUp()
+    {
+        _logIndexStorage = Substitute.For<ILogIndexStorage>();
+    }
+
+    [TearDown]
+    public async Task TearDownAsync()
+    {
+        await _logIndexStorage.DisposeAsync();
+    }
+
+    public static IEnumerable LogIndexTestsData
+    {
+        get
+        {
+            yield return new TestCaseData(
+                "AddressA",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithAddress(TestItem.AddressA)
+                    .Build(),
+
+                LogIndexRanges[TestItem.AddressA]
+            );
+
+            yield return new TestCaseData(
+                "AddressA or AddressA",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithAddresses(TestItem.AddressA, TestItem.AddressA)
+                    .Build(),
+
+                LogIndexRanges[TestItem.AddressA]
+            );
+
+            yield return new TestCaseData(
+                "AddressA or AddressB",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithAddresses(TestItem.AddressA, TestItem.AddressB)
+                    .Build(),
+
+                LogIndexRanges[TestItem.AddressA].Union(LogIndexRanges[TestItem.AddressB])
+                    .Distinct().Order().ToList()
+            );
+
+            yield return new TestCaseData(
+                "TopicA",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithTopicExpressions(
+                        TestTopicExpressions.Specific(TestItem.KeccakA)
+                    ).Build(),
+
+                LogIndexRanges[0, TestItem.KeccakA]
+            );
+
+            yield return new TestCaseData(
+                "TopicA or TopicA",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithTopicExpressions(
+                        TestTopicExpressions.Or(TestItem.KeccakA, TestItem.KeccakA)
+                    ).Build(),
+
+                LogIndexRanges[0, TestItem.KeccakA]
+            );
+
+            yield return new TestCaseData(
+                "TopicA or TopicB",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithTopicExpressions(
+                        TestTopicExpressions.Or(TestItem.KeccakA, TestItem.KeccakB)
+                    ).Build(),
+
+                LogIndexRanges[0, TestItem.KeccakA].Union(LogIndexRanges[0, TestItem.KeccakB])
+                    .Distinct().Order().ToList()
+            );
+
+            yield return new TestCaseData(
+                "TopicA, TopicB",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithTopicExpressions(
+                        TestTopicExpressions.Specific(TestItem.KeccakA),
+                        TestTopicExpressions.Specific(TestItem.KeccakB)
+                    ).Build(),
+
+                LogIndexRanges[0, TestItem.KeccakA]
+                    .Intersect(LogIndexRanges[1, TestItem.KeccakB])
+                    .Distinct().Order().ToList()
+            );
+
+            yield return new TestCaseData(
+                "TopicA, -, TopicA",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithTopicExpressions(
+                        TestTopicExpressions.Specific(TestItem.KeccakA),
+                        TestTopicExpressions.Any,
+                        TestTopicExpressions.Specific(TestItem.KeccakA)
+                    ).Build(),
+
+                LogIndexRanges[0, TestItem.KeccakA]
+                    .Intersect(LogIndexRanges[2, TestItem.KeccakA])
+                    .Distinct().Order().ToList()
+            );
+
+            // TODO: cases with the same topic on different positions
+
+            yield return new TestCaseData(
+                "TopicA, -, TopicB or TopicC",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithTopicExpressions(
+                        TestTopicExpressions.Specific(TestItem.KeccakA),
+                        TestTopicExpressions.Any,
+                        TestTopicExpressions.Or(TestItem.KeccakB, TestItem.KeccakC)
+                    ).Build(),
+                LogIndexRanges[0, TestItem.KeccakA]
+                    .Intersect(LogIndexRanges[2, TestItem.KeccakB].Union(LogIndexRanges[2, TestItem.KeccakC]))
+                    .Distinct().Order().ToList()
+            );
+
+            yield return new TestCaseData(
+                "AddressA | TopicA or TopicB, TopicC",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithAddress(TestItem.AddressA)
+                    .WithTopicExpressions(
+                        TestTopicExpressions.Or(TestItem.KeccakA, TestItem.KeccakB),
+                        TestTopicExpressions.Specific(TestItem.KeccakC)
+                    ).Build(),
+
+                LogIndexRanges[0, TestItem.KeccakA].Union(LogIndexRanges[0, TestItem.KeccakB])
+                    .Intersect(LogIndexRanges[1, TestItem.KeccakC])
+                    .Intersect(LogIndexRanges[TestItem.AddressA])
+                    .Distinct().Order().ToList()
+            );
+
+            yield return new TestCaseData(
+                "AddressA or AddressB | TopicA or TopicB, -, TopicC, TopicD or TopicE",
+
+                FilterBuilder.New()
+                    .FromBlock(LogIndexFrom).ToBlock(LogIndexTo)
+                    .WithAddresses(TestItem.AddressA, TestItem.AddressB)
+                    .WithTopicExpressions(
+                        TestTopicExpressions.Or(TestItem.KeccakA, TestItem.KeccakB),
+                        TestTopicExpressions.Any,
+                        TestTopicExpressions.Specific(TestItem.KeccakC),
+                        TestTopicExpressions.Or(TestItem.KeccakD, TestItem.KeccakE)
+                    ).Build(),
+
+                LogIndexRanges[TestItem.AddressA].Union(LogIndexRanges[TestItem.AddressB])
+                    .Intersect(LogIndexRanges[0, TestItem.KeccakA].Union(LogIndexRanges[0, TestItem.KeccakB]))
+                    .Intersect(LogIndexRanges[2, TestItem.KeccakC])
+                    .Intersect(LogIndexRanges[3, TestItem.KeccakD].Union(LogIndexRanges[3, TestItem.KeccakE]))
+                    .Distinct().Order().ToList()
+            );
+        }
+    }
+
+    [TestCaseSource(nameof(LogIndexTestsData))]
+    public void Filter_Test(string name, LogFilter filter, List<int> expected)
+    {
+        Assert.That(expected, Is.Not.Empty, "Unreliable test: no block numbers are selected.");
+        Assert.That(expected, Has.Count.LessThan(LogIndexTo - LogIndexFrom + 1), "Unreliable test: all block numbers are selected.");
+
+        MockLogIndex();
+
+        List<int> blockNums = _logIndexStorage.GetBlockNumbersFor(filter, LogIndexFrom, LogIndexTo, CancellationToken.None);
+
+        Assert.That(blockNums, Is.EquivalentTo(expected));
+    }
+
+    private const long LogIndexFrom = 0;
+    private const long LogIndexTo = 99;
+
+    private static readonly Ranges LogIndexRanges = GenerateLogIndexRanges();
+
+    private static Ranges GenerateLogIndexRanges()
+    {
+        var random = new Random(42);
+
+        var addressRanges = new Dictionary<Address, List<int>>();
+        foreach (Address address in new[] { TestItem.AddressA, TestItem.AddressB, TestItem.AddressC, TestItem.AddressD, TestItem.AddressE })
+        {
+            var range = Enumerable.Range((int)LogIndexFrom, (int)(LogIndexTo + 1)).Where(_ => random.NextDouble() < 0.3).ToList();
+            addressRanges.Add(address, range);
+        }
+
+        Dictionary<Hash256, List<int>>[] topicRanges = Enumerable
+            .Range(0, LogIndexStorage.MaxTopics)
+            .Select(_ => new Dictionary<Hash256, List<int>>()).ToArray();
+
+        foreach (Dictionary<Hash256, List<int>> ranges in topicRanges)
+        {
+            foreach (Hash256 topic in new[] { TestItem.KeccakA, TestItem.KeccakB, TestItem.KeccakC, TestItem.KeccakD, TestItem.KeccakE })
+            {
+                var range = Enumerable.Range((int)LogIndexFrom, (int)(LogIndexTo + 1)).Where(_ => random.NextDouble() < 0.2).ToList();
+                ranges.Add(topic, range);
+            }
+        }
+
+        return new(addressRanges, topicRanges);
+    }
+
+    private void MockLogIndex()
+    {
+        foreach ((Address address, List<int> range) in LogIndexRanges.Address)
+        {
+            _logIndexStorage
+                .GetBlockNumbersFor(address, Arg.Any<int>(), Arg.Any<int>())
+                .Returns(info => range.SkipWhile(x => x < info.ArgAt<int>(1)).TakeWhile(x => x <= info.ArgAt<int>(2)).ToList());
+        }
+
+        for (var i = 0; i < LogIndexRanges.Topic.Length; i++)
+        {
+            foreach ((Hash256 topic, List<int> range) in LogIndexRanges.Topic[i])
+            {
+                _logIndexStorage
+                    .GetBlockNumbersFor(Arg.Is(i), topic, Arg.Any<int>(), Arg.Any<int>())
+                    .Returns(info => range.SkipWhile(x => x < info.ArgAt<int>(2)).TakeWhile(x => x <= info.ArgAt<int>(3)).ToList());
+            }
+        }
+    }
+}
diff --git a/src/Nethermind/Nethermind.Blockchain.Test/Receipts/PersistentReceiptStorageTests.cs b/src/Nethermind/Nethermind.Blockchain.Test/Receipts/PersistentReceiptStorageTests.cs
index 0ac8d76f9d..6f8d35d04b 100644
--- a/src/Nethermind/Nethermind.Blockchain.Test/Receipts/PersistentReceiptStorageTests.cs
+++ b/src/Nethermind/Nethermind.Blockchain.Test/Receipts/PersistentReceiptStorageTests.cs
@@ -442,7 +442,7 @@ public class PersistentReceiptStorageTests
     }
 
     [Test]
-    public void When_NewHeadBlock_ClearOldTxIndex()
+    public void When_NewHeadBlock_ClearOldTxIndex_And_KeepsReceipts()
     {
         _receiptConfig.TxLookupLimit = 1000;
         CreateStorage();
@@ -466,6 +466,7 @@ public class PersistentReceiptStorageTests
             () => _receiptsDb.GetColumnDb(ReceiptsColumns.Transactions)[receipts[0].TxHash!.Bytes],
             Is.Null.After(1000, 100)
             );
+        Assert.That(_storage.HasBlock(receipts[0].BlockNumber, receipts[0].BlockHash!));
     }
 
     private (Block block, TxReceipt[] receipts) PrepareBlock(Block? block = null, bool isFinalized = false, long? headNumber = null)
diff --git a/src/Nethermind/Nethermind.Blockchain/Receipts/IReceiptConfig.cs b/src/Nethermind/Nethermind.Blockchain/Receipts/IReceiptConfig.cs
index f5729d9915..c37c7a2ad7 100644
--- a/src/Nethermind/Nethermind.Blockchain/Receipts/IReceiptConfig.cs
+++ b/src/Nethermind/Nethermind.Blockchain/Receipts/IReceiptConfig.cs
@@ -13,8 +13,11 @@ public interface IReceiptConfig : IConfig
     [ConfigItem(Description = "Whether to migrate the receipts database to the new schema.", DefaultValue = "false")]
     bool ReceiptsMigration { get; set; }
 
-    [ConfigItem(Description = "The degree of parallelism during receipt migration.", DefaultValue = "0", HiddenFromDocs = true)]
-    int ReceiptsMigrationDegreeOfParallelism { get; set; }
+    [ConfigItem(Description = "The degree of IO operations parallelism during receipts migration.", DefaultValue = "8", HiddenFromDocs = true)]
+    int LogIndexIOParallelism { get; set; }
+
+    [ConfigItem(Description = "Distance between blocks on when to force RocksDB compaction", DefaultValue = "262144", HiddenFromDocs = true)]
+    int LogIndexCompactionDistance { get; set; }
 
     [ConfigItem(Description = "Force receipt recovery if its not able to detect it.", DefaultValue = "false", HiddenFromDocs = true)]
     bool ForceReceiptsMigration { get; set; }
diff --git a/src/Nethermind/Nethermind.Blockchain/Receipts/IReceiptStorage.cs b/src/Nethermind/Nethermind.Blockchain/Receipts/IReceiptStorage.cs
index 65633a1fa7..daa79cf051 100644
--- a/src/Nethermind/Nethermind.Blockchain/Receipts/IReceiptStorage.cs
+++ b/src/Nethermind/Nethermind.Blockchain/Receipts/IReceiptStorage.cs
@@ -19,8 +19,13 @@ namespace Nethermind.Blockchain.Receipts
         void RemoveReceipts(Block block);
 
         /// <summary>
-        /// Receipts for a block are inserted
+        /// Receipts for a new main block are inserted
         /// </summary>
         event EventHandler<BlockReplacementEventArgs> ReceiptsInserted;
+
+        /// <summary>
+        /// Any receipts are inserted
+        /// </summary>
+        event EventHandler<ReceiptsEventArgs> AnyReceiptsInserted;
     }
 }
diff --git a/src/Nethermind/Nethermind.Blockchain/Receipts/InMemoryReceiptStorage.cs b/src/Nethermind/Nethermind.Blockchain/Receipts/InMemoryReceiptStorage.cs
index 1e3eec074a..ffdf56d1b8 100644
--- a/src/Nethermind/Nethermind.Blockchain/Receipts/InMemoryReceiptStorage.cs
+++ b/src/Nethermind/Nethermind.Blockchain/Receipts/InMemoryReceiptStorage.cs
@@ -19,6 +19,7 @@ namespace Nethermind.Blockchain.Receipts
 
 #pragma warning disable CS0067
         public event EventHandler<BlockReplacementEventArgs> ReceiptsInserted;
+        public event EventHandler<ReceiptsEventArgs>? AnyReceiptsInserted;
 #pragma warning restore CS0067
 
         public InMemoryReceiptStorage(bool allowReceiptIterator = true, IBlockTree? blockTree = null)
@@ -73,6 +74,8 @@ namespace Nethermind.Blockchain.Receipts
             {
                 EnsureCanonical(block);
             }
+
+            AnyReceiptsInserted?.Invoke(this, new(block.Header, txReceipts));
         }
 
         public bool HasBlock(long blockNumber, Hash256 hash)
diff --git a/src/Nethermind/Nethermind.Blockchain/Receipts/NullReceiptStorage.cs b/src/Nethermind/Nethermind.Blockchain/Receipts/NullReceiptStorage.cs
index 1c06f8b3d2..fd73788fcd 100644
--- a/src/Nethermind/Nethermind.Blockchain/Receipts/NullReceiptStorage.cs
+++ b/src/Nethermind/Nethermind.Blockchain/Receipts/NullReceiptStorage.cs
@@ -14,6 +14,7 @@ namespace Nethermind.Blockchain.Receipts
 
 #pragma warning disable CS0067
         public event EventHandler<BlockReplacementEventArgs> ReceiptsInserted;
+        public event EventHandler<ReceiptsEventArgs>? AnyReceiptsInserted;
 #pragma warning restore CS0067
 
         public Hash256? FindBlockHash(Hash256 hash) => null;
diff --git a/src/Nethermind/Nethermind.Blockchain/Receipts/PersistentReceiptStorage.cs b/src/Nethermind/Nethermind.Blockchain/Receipts/PersistentReceiptStorage.cs
index 4fb8d05f1e..9b9dafe6c2 100644
--- a/src/Nethermind/Nethermind.Blockchain/Receipts/PersistentReceiptStorage.cs
+++ b/src/Nethermind/Nethermind.Blockchain/Receipts/PersistentReceiptStorage.cs
@@ -39,6 +39,7 @@ namespace Nethermind.Blockchain.Receipts
         private readonly LruCache<ValueHash256, TxReceipt[]> _receiptsCache = new(CacheSize, CacheSize, "receipts");
 
         public event EventHandler<BlockReplacementEventArgs> ReceiptsInserted;
+        public event EventHandler<ReceiptsEventArgs>? AnyReceiptsInserted;
 
         public PersistentReceiptStorage(
             IColumnsDb<ReceiptsColumns> receiptsDb,
@@ -86,7 +87,7 @@ namespace Nethermind.Blockchain.Receipts
                     Block newOldTx = _blockTree.FindBlock(newMain.Number - _receiptConfig.TxLookupLimit.Value);
                     if (newOldTx is not null)
                     {
-                        RemoveReceipts(newOldTx);
+                        RemoveBlockTx(newOldTx);
                     }
                 }
             });
@@ -288,6 +289,8 @@ namespace Nethermind.Blockchain.Receipts
             {
                 EnsureCanonical(block, lastBlockNumber);
             }
+
+            AnyReceiptsInserted?.Invoke(this, new(block.Header, txReceipts));
         }
 
         public long MigratedBlockNumber
@@ -338,6 +341,11 @@ namespace Nethermind.Blockchain.Receipts
             GetBlockNumPrefixedKey(block.Number, block.Hash, blockNumPrefixed);
             _receiptsDb.Remove(blockNumPrefixed);
 
+            RemoveBlockTx(block);
+        }
+
+        private void RemoveBlockTx(Block block)
+        {
             using IWriteBatch writeBatch = _transactionDb.StartWriteBatch();
             foreach (Transaction tx in block.Transactions)
             {
diff --git a/src/Nethermind/Nethermind.Blockchain/Receipts/ReceiptConfig.cs b/src/Nethermind/Nethermind.Blockchain/Receipts/ReceiptConfig.cs
index fe7c36fa12..9a6f5c67fa 100644
--- a/src/Nethermind/Nethermind.Blockchain/Receipts/ReceiptConfig.cs
+++ b/src/Nethermind/Nethermind.Blockchain/Receipts/ReceiptConfig.cs
@@ -7,7 +7,8 @@ public class ReceiptConfig : IReceiptConfig
 {
     public bool StoreReceipts { get; set; } = true;
     public bool ReceiptsMigration { get; set; } = false;
-    public int ReceiptsMigrationDegreeOfParallelism { get; set; } = 0;
+    public int LogIndexIOParallelism { get; set; } = 8; // TODO: add validation
+    public int LogIndexCompactionDistance { get; set; } = 262_144;
     public bool ForceReceiptsMigration { get; set; } = false;
     public bool CompactReceiptStore { get; set; } = true;
     public bool CompactTxIndex { get; set; } = true;
diff --git a/src/Nethermind/Nethermind.Core.Test/AscListHelperTests.cs b/src/Nethermind/Nethermind.Core.Test/AscListHelperTests.cs
new file mode 100644
index 0000000000..664eef4fbd
--- /dev/null
+++ b/src/Nethermind/Nethermind.Core.Test/AscListHelperTests.cs
@@ -0,0 +1,91 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Collections.Generic;
+using System.Linq;
+using Nethermind.Core.Collections;
+using NUnit.Framework;
+
+namespace Nethermind.Core.Test;
+
+[TestFixture(42, 3, 1)]
+[TestFixture(43, 5, 2)]
+[TestFixture(44, 10, 3)]
+[TestFixture(45, 100, 10)]
+[TestFixture(46, 1000, 50)]
+[TestFixture(47, 10_000, 100)]
+[TestFixture(48, 10_000, 1_000)]
+[Parallelizable(ParallelScope.All)]
+public class AscListHelperTests(int seed, int size, int delta)
+{
+    [Test]
+    public void Union_Random()
+    {
+        var random = new Random(seed);
+        int[] l1 = Generate(random, size, delta);
+        int[] l2 = Generate(random, size, delta);
+
+        Assert.That(
+            AscListHelper.Union(l1, l2),
+            Is.EqualTo(l1.Concat(l2).Distinct().Order())
+        );
+    }
+
+    [Test]
+    public void UnionAll_Random()
+    {
+        var count = Math.Min(5, size / 2);
+
+        var random = new Random(seed);
+        var ls = Enumerable.Range(0, count).Select(_ => Generate(random, size, delta)).ToArray();
+
+        var union = new HashSet<int>(ls.SelectMany(l => l));
+        Assert.That(
+            AscListHelper.UnionAll(ls),
+            Is.EqualTo(union.Order())
+        );
+    }
+
+    [Test]
+    public void Intersect_Random()
+    {
+        var random = new Random(seed);
+        int[] l1 = Generate(random, size, delta);
+        int[] l2 = Generate(random, size, delta);
+
+        Assert.That(
+            AscListHelper.Intersect(l1, l2),
+            Is.EqualTo(l1.Intersect(l2).Order())
+        );
+    }
+
+    [Test]
+    public void IntersectAll_Random()
+    {
+        var count = Math.Min(5, size / 2);
+
+        var random = new Random(seed);
+        var ls = Enumerable.Range(0, count).Select(_ => Generate(random, size, delta)).ToArray();
+
+        var intersection = new HashSet<int>(ls[0]);
+        ls.Skip(1).ForEach(l => intersection.IntersectWith(l));
+
+        Assert.That(
+            AscListHelper.IntersectAll(ls),
+            Is.EqualTo(intersection.Order())
+        );
+    }
+
+    private static int[] Generate(Random random, int size, int delta)
+    {
+        size = random.Next(size / 2, size + 1);
+        var res = new int[size];
+
+        var p = 0;
+        for (var i = 0; i < size; i++)
+            p = res[i] = p + random.Next(1, delta + 1);
+
+        return res;
+    }
+}
diff --git a/src/Nethermind/Nethermind.Core.Test/Builders/RandomExtensions.cs b/src/Nethermind/Nethermind.Core.Test/Builders/RandomExtensions.cs
new file mode 100644
index 0000000000..2d45652e4a
--- /dev/null
+++ b/src/Nethermind/Nethermind.Core.Test/Builders/RandomExtensions.cs
@@ -0,0 +1,15 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Collections.Generic;
+using Nethermind.Crypto;
+
+namespace Nethermind.Core.Test.Builders;
+
+public static class RandomExtensions
+{
+    public static T NextValue<T>(this Random random, IReadOnlyList<T> values) => values[random.Next(values.Count)];
+
+    public static T NextValue<T>(this ICryptoRandom random, IReadOnlyList<T> values) => values[random.NextInt(values.Count)];
+}
diff --git a/src/Nethermind/Nethermind.Core.Test/Json/ByteArrayConverterTests.cs b/src/Nethermind/Nethermind.Core.Test/Json/ByteArrayConverterTests.cs
index d1413676fa..5888c30c65 100644
--- a/src/Nethermind/Nethermind.Core.Test/Json/ByteArrayConverterTests.cs
+++ b/src/Nethermind/Nethermind.Core.Test/Json/ByteArrayConverterTests.cs
@@ -8,6 +8,8 @@ using System.Linq;
 using System.Text.Json;
 using FluentAssertions;
 using Nethermind.Core.Extensions;
+using Nethermind.Core.Test.Builders;
+using Nethermind.Crypto;
 using Nethermind.Serialization.Json;
 using NUnit.Framework;
 
@@ -294,6 +296,22 @@ public class ByteArrayConverterTests : ConverterTestBase<byte[]>
         }
     }
 
+    [Test]
+    public void Test_DictionaryKey()
+    {
+        var random = new CryptoRandom();
+        var dictionary = new Dictionary<byte[], int?>
+        {
+            { Bytes.FromHexString("0x0"), null },
+            { Bytes.FromHexString("0x1"), random.NextInt(int.MaxValue) },
+            { Build.An.Address.TestObject.Bytes, random.NextInt(int.MaxValue) },
+            { random.GenerateRandomBytes(10), random.NextInt(int.MaxValue) },
+            { random.GenerateRandomBytes(32), random.NextInt(int.MaxValue) },
+        };
+
+        TestConverter(dictionary, new ByteArrayConverter());
+    }
+
     private static ReadOnlySequence<byte> JsonForLiteral(string literal) =>
         MakeSequence(Encoding.UTF8.GetBytes(literal));
 
diff --git a/src/Nethermind/Nethermind.Core.Test/Json/ConverterTestBase.cs b/src/Nethermind/Nethermind.Core.Test/Json/ConverterTestBase.cs
index 7f38126cda..6425a93215 100644
--- a/src/Nethermind/Nethermind.Core.Test/Json/ConverterTestBase.cs
+++ b/src/Nethermind/Nethermind.Core.Test/Json/ConverterTestBase.cs
@@ -2,6 +2,7 @@
 // SPDX-License-Identifier: LGPL-3.0-only
 
 using System;
+using System.Collections;
 using System.Text.Json;
 using System.Text.Json.Serialization;
 using NUnit.Framework;
@@ -10,7 +11,10 @@ namespace Nethermind.Core.Test.Json;
 
 public class ConverterTestBase<T>
 {
-    protected void TestConverter(T? item, Func<T, T, bool> equalityComparer, JsonConverter<T> converter)
+    protected void TestConverter(T? item, Func<T, T, bool> equalityComparer, JsonConverter<T> converter) =>
+        TestConverter(item, converter, equalityComparer);
+
+    protected void TestConverter<R>(R? item, JsonConverter<T> converter, Func<R, R, bool>? equalityComparer = null)
     {
         var options = new JsonSerializerOptions
         {
@@ -22,10 +26,15 @@ public class ConverterTestBase<T>
 
         string result = JsonSerializer.Serialize(item, options);
 
-        T? deserialized = JsonSerializer.Deserialize<T>(result, options);
+        R? deserialized = JsonSerializer.Deserialize<R>(result, options);
 
 #pragma warning disable CS8604
-        Assert.That(equalityComparer(item, deserialized), Is.True);
+        if (equalityComparer != null)
+            Assert.That(equalityComparer(item, deserialized), Is.True);
+        else if (item is IEnumerable itemE && deserialized is IEnumerable deserializedE)
+            Assert.That(deserializedE, Is.EquivalentTo(itemE));
+        else
+            Assert.That(deserialized, Is.EqualTo(item));
 #pragma warning restore CS8604
     }
 }
diff --git a/src/Nethermind/Nethermind.Core.Test/Modules/BlockTreeModule.cs b/src/Nethermind/Nethermind.Core.Test/Modules/BlockTreeModule.cs
index 59ec0f3c4d..ec96b20820 100644
--- a/src/Nethermind/Nethermind.Core.Test/Modules/BlockTreeModule.cs
+++ b/src/Nethermind/Nethermind.Core.Test/Modules/BlockTreeModule.cs
@@ -13,6 +13,7 @@ using Nethermind.Blockchain.Receipts;
 using Nethermind.Db;
 using Nethermind.Db.Blooms;
 using Nethermind.Facade.Find;
+using Nethermind.Logging;
 using Nethermind.State.Repositories;
 using Nethermind.TxPool;
 
@@ -33,6 +34,7 @@ public class BlockTreeModule : Autofac.Module
             .AddSingleton<IBlobTxStorage, IDbProvider, ITxPoolConfig>(CreateBlobTxStorage)
             .AddSingleton<IBlockTree, BlockTree>()
             .Bind<IBlockFinder, IBlockTree>()
+            .AddSingleton<ILogIndexStorage, LogIndexStorage>()
             .AddSingleton<ILogFinder, LogFinder>()
             ;
     }
diff --git a/src/Nethermind/Nethermind.Core/AscListHelper.cs b/src/Nethermind/Nethermind.Core/AscListHelper.cs
new file mode 100644
index 0000000000..52dc964c36
--- /dev/null
+++ b/src/Nethermind/Nethermind.Core/AscListHelper.cs
@@ -0,0 +1,114 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Collections.Generic;
+using System.Linq;
+
+namespace Nethermind.Core;
+
+/// <summary>
+/// Helper class for fast operations with strictly increasing lists of integers.
+/// </summary>
+/// <remarks>
+/// Can reuse parameters as return values to minimize allocations.
+/// </remarks>
+public static class AscListHelper
+{
+    public static T IntersectTo<T>(T destination, IReadOnlyList<int> source1, IReadOnlyList<int> source2)
+        where T : IList<int>
+    {
+        var i = 0;
+        var j = 0;
+
+        while (i < source1.Count && j < source2.Count)
+        {
+            switch (source1[i].CompareTo(source2[j]))
+            {
+                case -1:
+                    i++;
+                    continue;
+                case 1:
+                    j++;
+                    continue;
+                default:
+                    destination.Add(source1[i++]);
+                    j++;
+                    continue;
+            }
+        }
+
+        return destination;
+    }
+
+    public static List<int> Intersect(IReadOnlyList<int> source1, IReadOnlyList<int> source2)
+    {
+        var destination = new List<int>(Math.Min(source1.Count, source2.Count));
+        return IntersectTo(destination, source1, source2);
+    }
+
+    public static List<int> Intersect(List<int> source1, List<int> source2)
+    {
+        if (source1.Count == 0) return source2;
+        if (source2.Count == 0) return source1;
+        return Intersect((IReadOnlyList<int>)source1, source2);
+    }
+
+    public static T UnionTo<T>(T destination, IReadOnlyList<int> source1, IReadOnlyList<int> source2)
+        where T : IList<int>
+    {
+        var i = 0;
+        var j = 0;
+
+        while (i < source1.Count && j < source2.Count)
+        {
+            switch (source1[i].CompareTo(source2[j]))
+            {
+                case -1:
+                    destination.Add(source1[i++]);
+                    continue;
+                case 1:
+                    destination.Add(source2[j++]);
+                    continue;
+                default:
+                    destination.Add(source1[i++]);
+                    j++;
+                    continue;
+            }
+        }
+
+        for (var k = i; k < source1.Count; k++)
+            destination.Add(source1[k]);
+
+        for (var k = j; k < source2.Count; k++)
+            destination.Add(source2[k]);
+
+        return destination;
+    }
+
+    public static List<int> Union(IReadOnlyList<int> source1, IReadOnlyList<int> source2)
+    {
+        var destination = new List<int>(Math.Max(source1.Count, source2.Count));
+        return UnionTo(destination, source1, source2);
+    }
+
+    public static List<int> Union(List<int> source1, List<int> source2)
+    {
+        if (source1.Count == 0) return source2;
+        if (source2.Count == 0) return source1;
+        return Union((IReadOnlyList<int>)source1, source2);
+    }
+
+    // TODO: optimize memory usage/copying in *All methods?
+    public static List<int> IntersectAll(IEnumerable<IReadOnlyList<int>> sources) =>
+        sources.Aggregate<IReadOnlyList<int>, List<int>?>(null, (current, l) => current is null ? l.ToList() : Intersect(current, l)) ?? [];
+
+    public static List<int> IntersectAll(ICollection<List<int>> sources) =>
+        sources.Count == 1 ? sources.First() : IntersectAll(sources.AsEnumerable());
+
+    public static List<int> UnionAll(IEnumerable<IReadOnlyList<int>> sources) =>
+        sources.Aggregate<IReadOnlyList<int>, List<int>?>(null, (current, l) => current is null ? l.ToList() : Union(current, l)) ?? [];
+
+    public static List<int> UnionAll(ICollection<List<int>> sources) =>
+        sources.Count == 1 ? sources.First() : UnionAll(sources.AsEnumerable());
+}
diff --git a/src/Nethermind/Nethermind.Core/Collections/CollectionExtensions.cs b/src/Nethermind/Nethermind.Core/Collections/CollectionExtensions.cs
index 4a5f5f476a..6408673b22 100644
--- a/src/Nethermind/Nethermind.Core/Collections/CollectionExtensions.cs
+++ b/src/Nethermind/Nethermind.Core/Collections/CollectionExtensions.cs
@@ -87,6 +87,21 @@ namespace Nethermind.Core.Collections
             return true;
         }
 
+        public static TValue GetOrAdd<TKey, TValue>(this Dictionary<TKey, TValue> dictionary, TKey key, Func<TKey, TValue> factory)
+            where TKey : notnull
+        {
+            if (dictionary.TryGetValue(key, out TValue? value))
+            {
+                return value;
+            }
+            else
+            {
+                value = factory(key);
+                dictionary.Add(key, value);
+                return value;
+            }
+        }
+
         private static class ClearCache<TKey, TValue> where TKey : notnull
         {
             public static readonly Action<ConcurrentDictionary<TKey, TValue>> Clear = CreateNoResizeClearExpression();
diff --git a/src/Nethermind/Nethermind.Core/Extensions/Bytes.cs b/src/Nethermind/Nethermind.Core/Extensions/Bytes.cs
index f0b27a71da..79eb76b904 100644
--- a/src/Nethermind/Nethermind.Core/Extensions/Bytes.cs
+++ b/src/Nethermind/Nethermind.Core/Extensions/Bytes.cs
@@ -262,7 +262,7 @@ namespace Nethermind.Core.Extensions
             return result;
         }
 
-        public static byte[] PadRight(this byte[] bytes, int length)
+        public static byte[] PadRight(this byte[] bytes, int length, byte padding = 0)
         {
             if (bytes.Length == length)
             {
@@ -276,6 +276,15 @@ namespace Nethermind.Core.Extensions
 
             byte[] result = new byte[length];
             Buffer.BlockCopy(bytes, 0, result, 0, bytes.Length);
+
+            if (padding != 0)
+            {
+                for (int i = bytes.Length; i < length; i++)
+                {
+                    result[i] = padding;
+                }
+            }
+
             return result;
         }
 
diff --git a/src/Nethermind/Nethermind.Core/Extensions/ChannelExtensions.cs b/src/Nethermind/Nethermind.Core/Extensions/ChannelExtensions.cs
new file mode 100644
index 0000000000..da55a8f572
--- /dev/null
+++ b/src/Nethermind/Nethermind.Core/Extensions/ChannelExtensions.cs
@@ -0,0 +1,22 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System.Collections.Generic;
+using System.Threading.Channels;
+
+namespace Nethermind.Core.Extensions;
+
+public static class ChannelExtensions
+{
+    public static List<T> ReadBatch<T>(this ChannelReader<T> channel, int batchSize)
+    {
+        if (!channel.TryPeek(out _))
+            return [];
+
+        var result = new List<T>(batchSize);
+        while (result.Count < batchSize && channel.TryRead(out T? item))
+            result.Add(item);
+
+        return result;
+    }
+}
diff --git a/src/Nethermind/Nethermind.Core/Extensions/IntExtensions.cs b/src/Nethermind/Nethermind.Core/Extensions/IntExtensions.cs
index 1ecd2adc0d..4a19d9e4bf 100644
--- a/src/Nethermind/Nethermind.Core/Extensions/IntExtensions.cs
+++ b/src/Nethermind/Nethermind.Core/Extensions/IntExtensions.cs
@@ -29,10 +29,14 @@ public static class IntExtensions
         return (uint)@this * Unit.GWei;
     }
 
-    public static byte[] ToByteArray(this int value)
+    public static byte[] ToLittleEndianByteArray(this int value)
     {
-        byte[] bytes = new byte[sizeof(int)];
-        BinaryPrimitives.WriteInt32BigEndian(bytes, value);
+        byte[] bytes = BitConverter.GetBytes(value);
+        if (!BitConverter.IsLittleEndian)
+        {
+            Array.Reverse(bytes);
+        }
+
         return bytes;
     }
 
diff --git a/src/Nethermind/Nethermind.Core/Extensions/WaitHandleExtensions.cs b/src/Nethermind/Nethermind.Core/Extensions/WaitHandleExtensions.cs
index fd0616dca6..3c6a0df654 100644
--- a/src/Nethermind/Nethermind.Core/Extensions/WaitHandleExtensions.cs
+++ b/src/Nethermind/Nethermind.Core/Extensions/WaitHandleExtensions.cs
@@ -23,8 +23,12 @@ namespace Nethermind.Core.Extensions
                     millisecondsTimeout,
                     true);
                 tokenRegistration = cancellationToken.Register(
-                    static state => ((TaskCompletionSource<bool>)state!).TrySetCanceled(),
-                    tcs);
+                    static state =>
+                    {
+                        var arg = ((TaskCompletionSource<bool> tcs, CancellationToken ct))state!;
+                        arg.tcs.TrySetCanceled(arg.ct);
+                    },
+                    (tcs, cancellationToken));
 
                 return await tcs.Task;
             }
diff --git a/src/Nethermind/Nethermind.Core/FakeWriteBatch.cs b/src/Nethermind/Nethermind.Core/FakeWriteBatch.cs
index ce27831eb3..4580217d61 100644
--- a/src/Nethermind/Nethermind.Core/FakeWriteBatch.cs
+++ b/src/Nethermind/Nethermind.Core/FakeWriteBatch.cs
@@ -31,5 +31,10 @@ namespace Nethermind.Core
         {
             _storePretendingToSupportBatches.Set(key, value, flags);
         }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            throw new NotSupportedException("Merging is not supported by this implementation.");
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Core/IWriteBatch.cs b/src/Nethermind/Nethermind.Core/IWriteBatch.cs
index 909f184541..c6980293d9 100644
--- a/src/Nethermind/Nethermind.Core/IWriteBatch.cs
+++ b/src/Nethermind/Nethermind.Core/IWriteBatch.cs
@@ -5,5 +5,8 @@ using System;
 
 namespace Nethermind.Core
 {
-    public interface IWriteBatch : IDisposable, IWriteOnlyKeyValueStore;
+    public interface IWriteBatch : IDisposable, IWriteOnlyKeyValueStore
+    {
+        void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None);
+    }
 }
diff --git a/src/Nethermind/Nethermind.Core/JsonConverters/ByteArrayConverter.cs b/src/Nethermind/Nethermind.Core/JsonConverters/ByteArrayConverter.cs
index e67479f536..a40cb9f018 100644
--- a/src/Nethermind/Nethermind.Core/JsonConverters/ByteArrayConverter.cs
+++ b/src/Nethermind/Nethermind.Core/JsonConverters/ByteArrayConverter.cs
@@ -173,7 +173,7 @@ public class ByteArrayConverter : JsonConverter<byte[]>
     private static void ThrowFormatException() => throw new FormatException();
 
     [DoesNotReturn, StackTraceHidden]
-    private static void ThrowInvalidOperationException() => throw new InvalidOperationException();
+    private static Exception ThrowInvalidOperationException() => throw new InvalidOperationException();
 
     public override void Write(
         Utf8JsonWriter writer,
@@ -246,4 +246,14 @@ public class ByteArrayConverter : JsonConverter<byte[]>
         if (array is not null)
             ArrayPool<byte>.Shared.Return(array);
     }
+
+    public override byte[] ReadAsPropertyName(ref Utf8JsonReader reader, Type typeToConvert, JsonSerializerOptions options)
+    {
+        return Convert(ref reader) ?? throw ThrowInvalidOperationException();
+    }
+
+    public override void WriteAsPropertyName(Utf8JsonWriter writer, byte[] value, JsonSerializerOptions options)
+    {
+        Convert(writer, value, static (w, h) => w.WritePropertyName(h), skipLeadingZeros: false, addQuotations: false, addHexPrefix: true);
+    }
 }
diff --git a/src/Nethermind/Nethermind.Core/ProgressLogger.cs b/src/Nethermind/Nethermind.Core/ProgressLogger.cs
index f674106dab..0129ac64d5 100644
--- a/src/Nethermind/Nethermind.Core/ProgressLogger.cs
+++ b/src/Nethermind/Nethermind.Core/ProgressLogger.cs
@@ -47,9 +47,9 @@ namespace Nethermind.Core
             Interlocked.Add(ref _skipped, skipped);
         }
 
-        public void SetMeasuringPoint()
+        public void SetMeasuringPoint(bool resetCompletion = true)
         {
-            UtcEndTime = null;
+            if (resetCompletion) UtcEndTime = null;
 
             if (UtcStartTime is not null)
             {
@@ -160,7 +160,8 @@ namespace Nethermind.Core
             _formatter = formatter;
         }
 
-        public void LogProgress()
+        // TODO: should it actually ever be true?
+        public void LogProgress(bool resetCompletion = true)
         {
             (long, long, long, long) reportState = (CurrentValue, TargetValue, CurrentQueued, _skipped);
             if (reportState != _lastReportState)
@@ -169,7 +170,7 @@ namespace Nethermind.Core
                 _lastReportState = reportState;
                 _logger.Info(reportString);
             }
-            SetMeasuringPoint();
+            SetMeasuringPoint(resetCompletion);
         }
 
         private string DefaultFormatter()
diff --git a/src/Nethermind/Nethermind.Core/Utils/ConcurrentWriteBatcher.cs b/src/Nethermind/Nethermind.Core/Utils/ConcurrentWriteBatcher.cs
index 9ee5829dc2..069bd2d7aa 100644
--- a/src/Nethermind/Nethermind.Core/Utils/ConcurrentWriteBatcher.cs
+++ b/src/Nethermind/Nethermind.Core/Utils/ConcurrentWriteBatcher.cs
@@ -40,6 +40,13 @@ public class ConcurrentWriteBatcher : IWriteBatch
         ReturnWriteBatch(currentBatch);
     }
 
+    public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+    {
+        IWriteBatch currentBatch = RentWriteBatch();
+        currentBatch.Merge(key, value, flags);
+        ReturnWriteBatch(currentBatch);
+    }
+
     public void Set(ReadOnlySpan<byte> key, byte[]? value, WriteFlags flags = WriteFlags.None)
     {
         IWriteBatch currentBatch = RentWriteBatch();
diff --git a/src/Nethermind/Nethermind.Db.Rocks/ColumnDb.cs b/src/Nethermind/Nethermind.Db.Rocks/ColumnDb.cs
index d9fe6cb551..f04f01b09e 100644
--- a/src/Nethermind/Nethermind.Db.Rocks/ColumnDb.cs
+++ b/src/Nethermind/Nethermind.Db.Rocks/ColumnDb.cs
@@ -57,6 +57,11 @@ public class ColumnDb : IDb
         _mainDb.SetWithColumnFamily(key, _columnFamily, value, writeFlags);
     }
 
+    public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags writeFlags = WriteFlags.None)
+    {
+        _mainDb.MergeWithColumnFamily(key, _columnFamily, value, writeFlags);
+    }
+
     public KeyValuePair<byte[], byte[]?>[] this[byte[][] keys] =>
         _rocksDb.MultiGet(keys, keys.Select(k => _columnFamily).ToArray());
 
@@ -115,6 +120,11 @@ public class ColumnDb : IDb
         {
             _underlyingWriteBatch.Set(key, value, _columnDb._columnFamily, flags);
         }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            _underlyingWriteBatch.Merge(key, value, _columnDb._columnFamily, flags);
+        }
     }
 
     public void Remove(ReadOnlySpan<byte> key)
@@ -130,7 +140,7 @@ public class ColumnDb : IDb
 
     public void Flush(bool onlyWal)
     {
-        _mainDb.Flush(onlyWal);
+        _mainDb.FlushWithColumnFamily(_columnFamily);
     }
 
     public void Compact()
@@ -151,4 +161,14 @@ public class ColumnDb : IDb
     {
         _mainDb.DangerousReleaseMemory(span);
     }
+
+    public IIterator GetIterator(bool isTailing = false)
+    {
+        return _mainDb.GetIterator(isTailing, _columnFamily);
+    }
+
+    public IIterator GetIterator(ref IteratorOptions options)
+    {
+        return _mainDb.GetIterator(ref options, _columnFamily);
+    }
 }
diff --git a/src/Nethermind/Nethermind.Db.Rocks/ColumnsDb.cs b/src/Nethermind/Nethermind.Db.Rocks/ColumnsDb.cs
index 02300a2d7e..30ff7e50c0 100644
--- a/src/Nethermind/Nethermind.Db.Rocks/ColumnsDb.cs
+++ b/src/Nethermind/Nethermind.Db.Rocks/ColumnsDb.cs
@@ -61,9 +61,9 @@ public class ColumnsDb<T> : DbOnTheRocks, IColumnsDb<T> where T : struct, Enum
         return keys;
     }
 
-    protected override void BuildOptions<O>(IRocksDbConfig dbConfig, Options<O> options, IntPtr? sharedCache)
+    protected override void BuildOptions<O>(IRocksDbConfig dbConfig, Options<O> options, IntPtr? sharedCache, IMergeOperator? mergeOperator)
     {
-        base.BuildOptions(dbConfig, options, sharedCache);
+        base.BuildOptions(dbConfig, options, sharedCache, mergeOperator);
         options.SetCreateMissingColumnFamilies();
     }
 
@@ -134,5 +134,10 @@ public class ColumnsDb<T> : DbOnTheRocks, IColumnsDb<T> where T : struct, Enum
         {
             _writeBatch._writeBatch.Set(key, value, _column._columnFamily, flags);
         }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            _writeBatch._writeBatch.Merge(key, value, flags);
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Db.Rocks/Config/DbConfig.cs b/src/Nethermind/Nethermind.Db.Rocks/Config/DbConfig.cs
index b2b506b1d4..0a04e3c85f 100644
--- a/src/Nethermind/Nethermind.Db.Rocks/Config/DbConfig.cs
+++ b/src/Nethermind/Nethermind.Db.Rocks/Config/DbConfig.cs
@@ -270,4 +270,14 @@ public class DbConfig : IDbConfig
     public string L1OriginDbRocksDbOptions { get; set; } = "";
 
     public string? L1OriginDbAdditionalRocksDbOptions { get; set; }
+
+    // TODO: cleanup & optimize settings
+    public string? LogIndexStorageDbRocksDbOptions { get; set; } = "";
+    public string? LogIndexStorageDbAdditionalRocksDbOptions { get; set; } = "";
+    public string? LogIndexStorageDefaultDbRocksDbOptions { get; set; } = "";
+    public string? LogIndexStorageDefaultDbAdditionalRocksDbOptions { get; set; } = "";
+    public string? LogIndexStorageAddressesDbRocksDbOptions { get; set; } = "";
+    public string? LogIndexStorageAddressesDbAdditionalRocksDbOptions { get; set; } = "";
+    public string? LogIndexStorageTopicsDbRocksDbOptions { get; set; } = "";
+    public string? LogIndexStorageTopicsDbAdditionalRocksDbOptions { get; set; } = "";
 }
diff --git a/src/Nethermind/Nethermind.Db.Rocks/Config/IDbConfig.cs b/src/Nethermind/Nethermind.Db.Rocks/Config/IDbConfig.cs
index bdd18bb3af..fbb65f5211 100644
--- a/src/Nethermind/Nethermind.Db.Rocks/Config/IDbConfig.cs
+++ b/src/Nethermind/Nethermind.Db.Rocks/Config/IDbConfig.cs
@@ -101,4 +101,13 @@ public interface IDbConfig : IConfig
 
     string L1OriginDbRocksDbOptions { get; set; }
     string? L1OriginDbAdditionalRocksDbOptions { get; set; }
+
+    string? LogIndexStorageDbRocksDbOptions { get; set; }
+    string? LogIndexStorageDbAdditionalRocksDbOptions { get; set; }
+    string? LogIndexStorageDefaultDbRocksDbOptions { get; set; }
+    string? LogIndexStorageDefaultDbAdditionalRocksDbOptions { get; set; }
+    string? LogIndexStorageAddressesDbRocksDbOptions { get; set; }
+    string? LogIndexStorageAddressesDbAdditionalRocksDbOptions { get; set; }
+    string? LogIndexStorageTopicsDbRocksDbOptions { get; set; }
+    string? LogIndexStorageTopicsDbAdditionalRocksDbOptions { get; set; }
 }
diff --git a/src/Nethermind/Nethermind.Db.Rocks/Config/PerTableDbConfig.cs b/src/Nethermind/Nethermind.Db.Rocks/Config/PerTableDbConfig.cs
index 6524901e35..3a68384593 100644
--- a/src/Nethermind/Nethermind.Db.Rocks/Config/PerTableDbConfig.cs
+++ b/src/Nethermind/Nethermind.Db.Rocks/Config/PerTableDbConfig.cs
@@ -38,7 +38,7 @@ public class PerTableDbConfig : IRocksDbConfig
         foreach (var prefix in _prefixes)
         {
             string prefixed = string.Concat(prefix, propertyName);
-            if (type.GetProperty(prefixed, BindingFlags.Public | BindingFlags.Instance) is null)
+            if (GetPrefixedConfigProperty(type, prefixed) is null)
             {
                 throw new InvalidConfigurationException($"Configuration {propertyName} not available with prefix {prefix}. Add {prefix}{propertyName} to {nameof(IDbConfig)}.", -1);
             }
@@ -90,13 +90,14 @@ public class PerTableDbConfig : IRocksDbConfig
         Type type = dbConfig.GetType();
         PropertyInfo? propertyInfo;
 
+        // TODO: clarify if this can be case-insensitive
         string val = (string)type.GetProperty(propertyName, BindingFlags.Public | BindingFlags.Instance)!.GetValue(dbConfig)!;
 
         foreach (var prefix in prefixes)
         {
             string prefixed = string.Concat(prefix, propertyName);
 
-            propertyInfo = type.GetProperty(prefixed, BindingFlags.IgnoreCase | BindingFlags.Public | BindingFlags.Instance);
+            propertyInfo = GetPrefixedConfigProperty(type, prefixed);
             if (propertyInfo is not null)
             {
                 string? valObj = (string?)propertyInfo.GetValue(dbConfig);
@@ -122,7 +123,7 @@ public class PerTableDbConfig : IRocksDbConfig
             {
                 string prefixed = string.Concat(prefix, propertyName);
 
-                propertyInfo = type.GetProperty(prefixed, BindingFlags.IgnoreCase | BindingFlags.Public | BindingFlags.Instance);
+                propertyInfo = GetPrefixedConfigProperty(type, prefixed);
                 if (propertyInfo is not null)
                 {
                     if (propertyInfo.PropertyType.CanBeAssignedNull())
@@ -147,7 +148,7 @@ public class PerTableDbConfig : IRocksDbConfig
             }
 
             // Use generic one even if its available
-            propertyInfo = type.GetProperty(propertyName, BindingFlags.IgnoreCase | BindingFlags.Public | BindingFlags.Instance);
+            propertyInfo = GetPrefixedConfigProperty(type, propertyName);
             return (T?)propertyInfo?.GetValue(dbConfig);
         }
         catch (Exception e)
@@ -156,4 +157,8 @@ public class PerTableDbConfig : IRocksDbConfig
         }
     }
 
+    private static PropertyInfo? GetPrefixedConfigProperty(Type type, string name)
+    {
+        return type.GetProperty(name, BindingFlags.IgnoreCase | BindingFlags.Public | BindingFlags.Instance);
+    }
 }
diff --git a/src/Nethermind/Nethermind.Db.Rocks/DbOnTheRocks.cs b/src/Nethermind/Nethermind.Db.Rocks/DbOnTheRocks.cs
index 1309aa0baf..a3980e82d1 100644
--- a/src/Nethermind/Nethermind.Db.Rocks/DbOnTheRocks.cs
+++ b/src/Nethermind/Nethermind.Db.Rocks/DbOnTheRocks.cs
@@ -66,6 +66,11 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
 
     private readonly DbSettings _settings;
 
+    // ReSharper disable once CollectionNeverQueried.Local
+    // Need to keep options from GC in case of merge operator applied, as they are used in callback
+    // TODO: find better way?
+    private readonly ConcurrentBag<OptionsHandle> _doNotGcOptions = [];
+
     private readonly IRocksDbConfig _perTableDbConfig;
     private ulong _maxBytesForLevelBase;
     private ulong _targetFileSizeBase;
@@ -143,7 +148,7 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
             // ReSharper disable once VirtualMemberCallInConstructor
             if (_logger.IsDebug) _logger.Debug($"Building options for {Name} DB");
             DbOptions = new DbOptions();
-            BuildOptions(_perTableDbConfig, DbOptions, sharedCache);
+            BuildOptions(_perTableDbConfig, DbOptions, sharedCache, _settings.MergeOperator);
 
             ColumnFamilies? columnFamilies = null;
             if (columnNames is not null)
@@ -155,7 +160,8 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
 
                     ColumnFamilyOptions options = new();
                     IRocksDbConfig columnConfig = _rocksDbConfigFactory.GetForDatabase(Name, columnFamily);
-                    BuildOptions(columnConfig, options, sharedCache);
+                    IMergeOperator? mergeOperator = _settings.MergeOperatorByColumn?.GetValueOrDefault(enumColumnName);
+                    BuildOptions(columnConfig, options, sharedCache, mergeOperator);
 
                     // "default" is a special column name with rocksdb, which is what previously not specifying column goes to
                     if (columnFamily == "Default") columnFamily = "default";
@@ -446,7 +452,7 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
         return asDict;
     }
 
-    protected virtual void BuildOptions<T>(IRocksDbConfig dbConfig, Options<T> options, IntPtr? sharedCache) where T : Options<T>
+    protected virtual void BuildOptions<T>(IRocksDbConfig dbConfig, Options<T> options, IntPtr? sharedCache, IMergeOperator? mergeOperator) where T : Options<T>
     {
         // This section is about the table factory.. and block cache apparently.
         // This effect the format of the SST files and usually require resync to take effect.
@@ -576,6 +582,12 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
             }
         }
 
+        if (mergeOperator is not null)
+        {
+            options.SetMergeOperator(new MergeOperatorAdapter(mergeOperator));
+            _doNotGcOptions.Add(options);
+        }
+
         #endregion
 
         #region read-write options
@@ -892,6 +904,40 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
         SetWithColumnFamily(key, null, value, writeFlags);
     }
 
+    public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+    {
+        ObjectDisposedException.ThrowIf(_isDisposing, this);
+
+        UpdateWriteMetrics();
+
+        try
+        {
+            _db.Merge(key, value, null, WriteFlagsToWriteOptions(flags));
+        }
+        catch (RocksDbSharpException e)
+        {
+            CreateMarkerIfCorrupt(e);
+            throw;
+        }
+    }
+
+    internal void MergeWithColumnFamily(ReadOnlySpan<byte> key, ColumnFamilyHandle? cf, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+    {
+        ObjectDisposedException.ThrowIf(_isDisposing, this);
+
+        UpdateWriteMetrics();
+
+        try
+        {
+            _db.Merge(key, value, cf, WriteFlagsToWriteOptions(flags));
+        }
+        catch (RocksDbSharpException e)
+        {
+            CreateMarkerIfCorrupt(e);
+            throw;
+        }
+    }
+
     public void DangerousReleaseMemory(in ReadOnlySpan<byte> span)
     {
         if (!span.IsNullOrEmpty())
@@ -981,10 +1027,21 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
         return GetAllCore(iterator);
     }
 
-    protected internal Iterator CreateIterator(bool ordered = false, ColumnFamilyHandle? ch = null)
+    protected internal Iterator CreateIterator(bool isTailing, ColumnFamilyHandle? ch = null)
+    {
+        return CreateIterator(new IteratorOptions { IsTailing = isTailing }, ch);
+    }
+
+    protected internal Iterator CreateIterator(IteratorOptions options, ColumnFamilyHandle? ch = null)
     {
         ReadOptions readOptions = new();
-        readOptions.SetTailing(!ordered);
+        readOptions.SetTailing(!options.IsTailing);
+
+        if (options.LowerBound is { } lowerBound)
+            readOptions.SetIterateLowerBound(lowerBound);
+
+        if (options.UpperBound is { } upperBound)
+            readOptions.SetIterateUpperBound(upperBound);
 
         try
         {
@@ -1275,6 +1332,21 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
             Set(key, value, null, flags);
         }
 
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            Merge(key, value, null, flags);
+        }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, ColumnFamilyHandle? cf = null, WriteFlags flags = WriteFlags.None)
+        {
+            ObjectDisposedException.ThrowIf(_isDisposed, this);
+
+            _rocksBatch.Merge(key, value, cf);
+            _writeFlags = flags;
+
+            if ((flags & WriteFlags.DisableWAL) != 0) FlushOnTooManyWrites();
+        }
+
         private void FlushOnTooManyWrites()
         {
             if (Interlocked.Increment(ref _writeCount) % MaxWritesOnNoWal != 0) return;
@@ -1301,6 +1373,13 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
         InnerFlush(onlyWal);
     }
 
+    public void FlushWithColumnFamily(ColumnFamilyHandle familyHandle)
+    {
+        ObjectDisposedException.ThrowIf(_isDisposing, this);
+
+        InnerFlush(familyHandle);
+    }
+
     public virtual void Compact()
     {
         _db.CompactRange(Keccak.Zero.BytesToArray(), Keccak.MaxValue.BytesToArray());
@@ -1323,6 +1402,18 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
         }
     }
 
+    private void InnerFlush(ColumnFamilyHandle columnFamilyHandle)
+    {
+        try
+        {
+            _rocksDbNative.rocksdb_flush_cf(_db.Handle, FlushOptions.DefaultFlushOptions.Handle, columnFamilyHandle.Handle);
+        }
+        catch (RocksDbSharpException e)
+        {
+            CreateMarkerIfCorrupt(e);
+        }
+    }
+
     public void Clear()
     {
         Dispose();
@@ -1654,6 +1745,29 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
         };
     }
 
+    public IIterator GetIterator(bool isTailing = false)
+    {
+        var iterator = CreateIterator(isTailing);
+        return new RocksDbIteratorWrapper(iterator);
+    }
+
+    public IIterator GetIterator(ref IteratorOptions options)
+    {
+        return GetIterator(ref options, null);
+    }
+
+    public IIterator GetIterator(bool isTailing, ColumnFamilyHandle familyHandle)
+    {
+        var options = new IteratorOptions { IsTailing = isTailing };
+        return GetIterator(ref options, familyHandle);
+    }
+
+    public IIterator GetIterator(ref IteratorOptions options, ColumnFamilyHandle? familyHandle)
+    {
+        var iterator = CreateIterator(options, familyHandle);
+        return new RocksDbIteratorWrapper(iterator);
+    }
+
     /// <summary>
     /// Iterators should not be kept for long as it will pin some memory block and sst file. This would show up as
     /// temporary higher disk usage or memory usage.
@@ -1771,10 +1885,10 @@ public partial class DbOnTheRocks : IDb, ITunableDb, IReadOnlyNativeKeyValueStor
             public void ClearIterators()
             {
                 if (_disposed) return;
-                if (Values is null) return;
-                foreach (IteratorHolder iterator in Values)
+                if (Values is not { } values) return;
+                foreach (IteratorHolder iterator in values)
                 {
-                    iterator.Dispose();
+                    iterator?.Dispose();
                 }
             }
 
diff --git a/src/Nethermind/Nethermind.Db.Rocks/MergeOperatorAdapter.cs b/src/Nethermind/Nethermind.Db.Rocks/MergeOperatorAdapter.cs
new file mode 100644
index 0000000000..14bf9efc98
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db.Rocks/MergeOperatorAdapter.cs
@@ -0,0 +1,80 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Runtime.CompilerServices;
+using System.Runtime.InteropServices;
+using Nethermind.Core.Collections;
+using RocksDbSharp;
+
+namespace Nethermind.Db.Rocks;
+
+// Also see RocksDbSharp.MergeOperatorImpl
+internal class MergeOperatorAdapter(IMergeOperator inner) : MergeOperator
+{
+    public string Name => inner.Name;
+
+    // TODO: fix and return array ptr instead of copying to unmanaged memory?
+    private static unsafe IntPtr GetResult(ArrayPoolList<byte>? data, out IntPtr resultLength, out IntPtr success)
+    {
+        if (data is null)
+        {
+            success = Convert.ToInt32(false);
+            resultLength = IntPtr.Zero;
+            return IntPtr.Zero;
+        }
+
+        using (data)
+        {
+            void* resultPtr = NativeMemory.Alloc((uint)data.Count);
+            var result = new Span<byte>(resultPtr, data.Count);
+            data.AsSpan().CopyTo(result);
+
+            resultLength = result.Length;
+
+            // Fixing RocksDbSharp invalid callback signature, TODO: submit an issue/PR
+            Unsafe.SkipInit(out success);
+            Unsafe.As<IntPtr, byte>(ref success) = 1;
+
+            return (IntPtr)resultPtr;
+        }
+    }
+
+    unsafe IntPtr MergeOperator.PartialMerge(
+        IntPtr keyPtr,
+        UIntPtr keyLength,
+        IntPtr operandsList,
+        IntPtr operandsListLength,
+        int numOperands,
+        out IntPtr successPtr,
+        out IntPtr resultLength)
+    {
+        var key = new Span<byte>((void*)keyPtr, (int)keyLength);
+        var enumerator = new RocksDbMergeEnumerator(new((void*)operandsList, numOperands), new((void*)operandsListLength, numOperands));
+
+        ArrayPoolList<byte>? result = inner.PartialMerge(key, enumerator);
+        return GetResult(result, out resultLength, out successPtr);
+    }
+
+    unsafe IntPtr MergeOperator.FullMerge(
+        IntPtr keyPtr,
+        UIntPtr keyLength,
+        IntPtr existingValuePtr,
+        UIntPtr existingValueLength,
+        IntPtr operandsList,
+        IntPtr operandsListLength,
+        int numOperands,
+        out IntPtr successPtr,
+        out IntPtr resultLength)
+    {
+        var key = new ReadOnlySpan<byte>((void*)keyPtr, (int)keyLength);
+        bool hasExistingValue = existingValuePtr != IntPtr.Zero;
+        Span<byte> existingValue = hasExistingValue ? new((void*)existingValuePtr, (int)existingValueLength) : Span<byte>.Empty;
+        var enumerator = new RocksDbMergeEnumerator(existingValue, hasExistingValue, new((void*)operandsList, numOperands), new((void*)operandsListLength, numOperands));
+
+        ArrayPoolList<byte>? result = inner.FullMerge(key, enumerator);
+        return GetResult(result, out resultLength, out successPtr);
+    }
+
+    unsafe void MergeOperator.DeleteValue(IntPtr value, UIntPtr valueLength) => NativeMemory.Free((void*)value);
+}
diff --git a/src/Nethermind/Nethermind.Db.Rocks/RocksDbIteratorWrapper.cs b/src/Nethermind/Nethermind.Db.Rocks/RocksDbIteratorWrapper.cs
new file mode 100644
index 0000000000..f34166b0ec
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db.Rocks/RocksDbIteratorWrapper.cs
@@ -0,0 +1,21 @@
+// SPDX-FileCopyrightText: 2024 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using RocksDbSharp;
+using System;
+
+namespace Nethermind.Db.Rocks;
+
+public class RocksDbIteratorWrapper(Iterator iterator) : IIterator
+{
+    public void SeekToFirst() => iterator.SeekToFirst();
+    public void Seek(ReadOnlySpan<byte> key) => iterator.Seek(key);
+    public void SeekForPrev(ReadOnlySpan<byte> key) => iterator.SeekForPrev(key);
+    public void Next() => iterator.Next();
+    public void Prev() => iterator.Prev();
+    public bool Valid() => iterator.Valid();
+    public ReadOnlySpan<byte> Key() => iterator.GetKeySpan();
+    public ReadOnlySpan<byte> Value() => iterator.GetValueSpan();
+    public void Dispose() => iterator.Dispose();
+}
+
diff --git a/src/Nethermind/Nethermind.Db.Rpc/RpcDb.cs b/src/Nethermind/Nethermind.Db.Rpc/RpcDb.cs
index 98279f33c4..b62adcee91 100644
--- a/src/Nethermind/Nethermind.Db.Rpc/RpcDb.cs
+++ b/src/Nethermind/Nethermind.Db.Rpc/RpcDb.cs
@@ -61,6 +61,11 @@ namespace Nethermind.Db.Rpc
 
         public KeyValuePair<byte[], byte[]>[] this[byte[][] keys] => keys.Select(k => new KeyValuePair<byte[], byte[]>(k, GetThroughRpc(k))).ToArray();
 
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            throw new InvalidOperationException("RPC DB does not support writes");
+        }
+
         public void Remove(ReadOnlySpan<byte> key)
         {
             throw new InvalidOperationException("RPC DB does not support writes");
@@ -119,5 +124,15 @@ namespace Nethermind.Db.Rpc
         public void DangerousReleaseMemory(in ReadOnlySpan<byte> span)
         {
         }
+
+        public IIterator GetIterator(bool isTailing = false)
+        {
+            throw new InvalidOperationException("RPC DB does not support iteration");
+        }
+
+        public IIterator GetIterator(ref IteratorOptions options)
+        {
+            throw new InvalidOperationException("RPC DB does not support iteration");
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Db.Test/LogIndex/LogIndexStorageComplexTests.cs b/src/Nethermind/Nethermind.Db.Test/LogIndex/LogIndexStorageComplexTests.cs
new file mode 100644
index 0000000000..66350db1bf
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db.Test/LogIndex/LogIndexStorageComplexTests.cs
@@ -0,0 +1,845 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Collections.Concurrent;
+using System.Collections.Generic;
+using System.Diagnostics;
+using System.Diagnostics.CodeAnalysis;
+using System.IO;
+using System.Linq;
+using System.Runtime.ExceptionServices;
+using System.Threading;
+using System.Threading.Tasks;
+using MathNet.Numerics.Random;
+using Nethermind.Core;
+using Nethermind.Core.Collections;
+using Nethermind.Core.Crypto;
+using Nethermind.Core.Extensions;
+using Nethermind.Core.Test;
+using Nethermind.Core.Test.Builders;
+using Nethermind.Db.Rocks;
+using Nethermind.Db.Rocks.Config;
+using Nethermind.Logging;
+using NUnit.Framework;
+using NUnit.Framework.Interfaces;
+
+namespace Nethermind.Db.Test.LogIndex
+{
+    // TODO: test for different block ranges intersection
+    // TODO: run internal state verification for all/some tests
+    // TODO: test for process crash via Thread.Abort
+    // TODO: test for reorg out-of-order
+    // TODO: test for concurrent reorg and backward sync
+    // TODO: rename to IntegrationTests?
+    [TestFixtureSource(nameof(TestCases))]
+    [Parallelizable(ParallelScope.All)]
+    [FixtureLifeCycle(LifeCycle.InstancePerTestCase)]
+    public class LogIndexStorageComplexTests(LogIndexStorageComplexTests.TestData testData)
+    {
+        private const int RaceConditionTestRepeat = 3;
+
+        public static readonly TestFixtureData[] TestCases =
+        [
+            new(new TestData(10, 100)),
+            new(new TestData(5, 200)),
+            new(new TestData(10, 100) { ExtendedGetRanges = true }) { RunState = RunState.Explicit },
+            new(new TestData(100, 100)) { RunState = RunState.Explicit },
+            new(new TestData(100, 200)) { RunState = RunState.Explicit }
+        ];
+
+        private string _dbPath = null!;
+        private IDbFactory _dbFactory = null!;
+        private readonly List<ILogIndexStorage> _createdStorages = [];
+
+        private LogIndexStorage CreateLogIndexStorage(int compactionDistance = 262_144, int ioParallelism = 16, int maxReorgDepth = 32, IDbFactory? dbFactory = null)
+        {
+            LogIndexStorage storage = new(dbFactory ?? _dbFactory, LimboLogs.Instance, ioParallelism, compactionDistance, maxReorgDepth);
+            _createdStorages.Add(storage);
+            return storage;
+        }
+
+        [SetUp]
+        public void Setup()
+        {
+            _dbPath = $"{nameof(LogIndexStorageComplexTests)}/{Guid.NewGuid():N}";
+
+            if (Directory.Exists(_dbPath))
+                Directory.Delete(_dbPath, true);
+
+            Directory.CreateDirectory(_dbPath);
+
+            var config = new DbConfig();
+            var configFactory = new RocksDbConfigFactory(new DbConfig(), new PruningConfig(), new TestHardwareInfo(0), LimboLogs.Instance);
+            _dbFactory = new RocksDbFactory(configFactory, config, new TestLogManager(), _dbPath);
+        }
+
+        [TearDown]
+        public async Task TearDown()
+        {
+            foreach (ILogIndexStorage storage in _createdStorages)
+            {
+                await using (storage)
+                    await storage.StopAsync();
+            }
+
+            if (!Directory.Exists(_dbPath))
+                return;
+
+            try
+            {
+                Directory.Delete(_dbPath, true);
+            }
+            catch
+            {
+                // ignore
+            }
+        }
+
+        [OneTimeSetUp]
+        // Causes DB error under some race condition, TODO: find & fix or remove
+        //[OneTimeTearDown]
+        public static void RemoveRootFolder()
+        {
+            if (!Directory.Exists(nameof(LogIndexStorageComplexTests)))
+                return;
+
+            try
+            {
+                Directory.Delete(nameof(LogIndexStorageComplexTests), true);
+            }
+            catch
+            {
+                // ignore
+            }
+        }
+
+        [Combinatorial]
+        public async Task Set_Get_Test(
+            [Values(100, 200, int.MaxValue)] int compactionDistance,
+            [Values(1, 8, 16)] byte ioParallelism,
+            [Values] bool isBackwardsSync,
+            [Values] bool compact
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage(compactionDistance, ioParallelism);
+
+            BlockReceipts[][] batches = isBackwardsSync ? Reverse(testData.Batches) : testData.Batches;
+            await SetReceiptsAsync(logIndexStorage, batches, isBackwardsSync);
+
+            if (compact)
+                await CompactAsync(logIndexStorage);
+
+            VerifyReceipts(logIndexStorage, testData);
+        }
+
+        [Combinatorial]
+        public async Task BackwardsSet_Set_Get_Test(
+            [Values(100, 200, int.MaxValue)] int compactionDistance
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage(compactionDistance);
+
+            var batches = testData.Batches;
+            var half = batches.Length / 2;
+
+            for (var i = 0; i < half + 1; i++)
+            {
+                if (half + i < batches.Length)
+                    await SetReceiptsAsync(logIndexStorage, [batches[half + i]], isBackwardsSync: false);
+                if (i != 0 && half - i >= 0)
+                    await SetReceiptsAsync(logIndexStorage, Reverse([batches[half - i]]), isBackwardsSync: true);
+            }
+
+            VerifyReceipts(logIndexStorage, testData);
+        }
+
+        [Combinatorial]
+        [Repeat(RaceConditionTestRepeat)]
+        [SuppressMessage("ReSharper", "AccessToDisposedClosure")]
+        public async Task Concurrent_BackwardsSet_Set_Get_Test(
+            [Values(100, int.MaxValue)] int compactionDistance
+        )
+        {
+            await using (var setStorage = CreateLogIndexStorage(compactionDistance))
+            {
+                var half = testData.Batches.Length / 2;
+                var batches = testData.Batches
+                    .Select((b, i) => i >= half ? b : b.Reverse().ToArray())
+                    .ToArray();
+
+                var forwardTask = Task.Run(async () =>
+                {
+                    for (var i = half; i < batches.Length; i++)
+                    {
+                        BlockReceipts[] batch = batches[i];
+                        await SetReceiptsAsync(setStorage, [batch], isBackwardsSync: false);
+
+                        Assert.That(setStorage.GetMinBlockNumber(), Is.LessThanOrEqualTo(batch[0].BlockNumber));
+                        Assert.That(setStorage.GetMaxBlockNumber(), Is.EqualTo(batch[^1].BlockNumber));
+                    }
+                });
+
+                var backwardTask = Task.Run(async () =>
+                {
+                    for (var i = half - 1; i >= 0; i--)
+                    {
+                        BlockReceipts[] batch = batches[i];
+                        await SetReceiptsAsync(setStorage, [batch], isBackwardsSync: true);
+
+                        Assert.That(setStorage.GetMinBlockNumber(), Is.EqualTo(batch[^1].BlockNumber));
+                        Assert.That(setStorage.GetMaxBlockNumber(), Is.GreaterThanOrEqualTo(batch[0].BlockNumber));
+                    }
+                });
+
+                await forwardTask;
+                await backwardTask;
+            }
+
+            // Create new storage to force-load everything from DB
+            await using (var testStorage = CreateLogIndexStorage(compactionDistance))
+                VerifyReceipts(testStorage, testData);
+        }
+
+        [Combinatorial]
+        public async Task Set_ReorgLast_Get_Test(
+            [Values(1, 5, 20)] int reorgDepth,
+            [Values(100, int.MaxValue)] int compactionDistance
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage(compactionDistance);
+
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            BlockReceipts[] reorgBlocks = testData.Batches.SelectMany(b => b).TakeLast(reorgDepth).ToArray();
+            foreach (BlockReceipts block in reorgBlocks)
+                await logIndexStorage.ReorgFrom(block);
+
+            VerifyReceipts(logIndexStorage, testData, excludedBlocks: reorgBlocks, maxBlock: reorgBlocks[0].BlockNumber - 1);
+        }
+
+        [Combinatorial]
+        public async Task Set_ReorgAndSetLast_Get_Test(
+            [Values(1, 5, 20)] int reorgDepth,
+            [Values(100, int.MaxValue)] int compactionDistance
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage(compactionDistance);
+
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            BlockReceipts[] reorgBlocks = testData.Batches.SelectMany(b => b).TakeLast(reorgDepth).ToArray();
+            foreach (BlockReceipts block in reorgBlocks)
+            {
+                await logIndexStorage.ReorgFrom(block);
+                await logIndexStorage.SetReceiptsAsync([block], false);
+            }
+
+            VerifyReceipts(logIndexStorage, testData);
+        }
+
+        [Combinatorial]
+        public async Task Set_ReorgLast_SetLast_Get_Test(
+            [Values(1, 5, 20)] int reorgDepth,
+            [Values(100, int.MaxValue)] int compactionDistance
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage(compactionDistance);
+
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            BlockReceipts[] reorgBlocks = testData.Batches.SelectMany(b => b).TakeLast(reorgDepth).ToArray();
+
+            foreach (BlockReceipts block in reorgBlocks)
+                await logIndexStorage.ReorgFrom(block);
+
+            await logIndexStorage.SetReceiptsAsync(reorgBlocks, false);
+
+            VerifyReceipts(logIndexStorage, testData);
+        }
+
+        [Combinatorial]
+        public async Task Set_ReorgUnexisting_Get_Test(
+            [Values(1, 5)] int reorgDepth,
+            [Values(100, int.MaxValue)] int compactionDistance
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage(compactionDistance);
+
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            var lastBlock = testData.Batches[^1][^1].BlockNumber;
+            BlockReceipts[] reorgBlocks = GenerateBlocks(new Random(4242), lastBlock - reorgDepth + 1, reorgDepth);
+            foreach (BlockReceipts block in reorgBlocks)
+                await logIndexStorage.ReorgFrom(block);
+
+            // Need custom check because Reorg updates the last block even if it's "unexisting"
+            Assert.That(logIndexStorage.GetMaxBlockNumber(), Is.EqualTo(lastBlock - reorgDepth));
+
+            VerifyReceipts(logIndexStorage, testData, excludedBlocks: reorgBlocks, validateMinMax: false);
+        }
+
+        [TestCase(1, 1)]
+        [TestCase(32, 64)]
+        [TestCase(64, 64)]
+        [TestCase(65, 64, Explicit = true)]
+        public async Task Set_Compact_ReorgLast_Get_Test(int reorgDepth, int maxReorgDepth)
+        {
+            var logIndexStorage = CreateLogIndexStorage(maxReorgDepth: maxReorgDepth);
+
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+            await CompactAsync(logIndexStorage);
+
+            BlockReceipts[] reorgBlocks = testData.Batches.SelectMany(b => b).TakeLast(reorgDepth).ToArray();
+            foreach (BlockReceipts block in reorgBlocks)
+                await logIndexStorage.ReorgFrom(block);
+
+            var lastBlock = testData.Batches[^1][^1].BlockNumber;
+            VerifyReceipts(logIndexStorage, testData, maxBlock: lastBlock - reorgDepth);
+        }
+
+        [Combinatorial]
+        public async Task Set_PeriodicReorg_Get_Test(
+            [Values(10, 70)] int reorgFrequency,
+            [Values(1, 5)] int maxReorgDepth,
+            [Values] bool compactAfter
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage();
+
+            var random = new Random(42);
+            var allReorgBlocks = new List<BlockReceipts>();
+            var allAddedBlocks = new List<BlockReceipts>();
+
+            foreach (BlockReceipts[][] batches in testData.Batches.GroupBy(b => b[0].BlockNumber / reorgFrequency).Select(g => g.ToArray()))
+            {
+                await SetReceiptsAsync(logIndexStorage, batches);
+
+                var reorgDepth = random.Next(1, maxReorgDepth);
+                BlockReceipts[] reorgBlocks = batches.SelectMany(b => b).TakeLast(reorgDepth).ToArray();
+                BlockReceipts[] addedBlocks = GenerateBlocks(random, reorgBlocks.First().BlockNumber, reorgBlocks.Length);
+
+                allReorgBlocks.AddRange(reorgBlocks);
+                allAddedBlocks.AddRange(addedBlocks);
+
+                foreach (BlockReceipts block in reorgBlocks)
+                    await logIndexStorage.ReorgFrom(block);
+
+                if (compactAfter)
+                    await CompactAsync(logIndexStorage);
+
+                await logIndexStorage.SetReceiptsAsync(addedBlocks, false);
+            }
+
+            VerifyReceipts(logIndexStorage, testData, excludedBlocks: allReorgBlocks, addedBlocks: allAddedBlocks);
+        }
+
+        [Ignore("Not supported, but is probably not needed.")]
+        [Combinatorial]
+        public async Task Set_ConsecutiveReorgsLast_Get_Test(
+            [Values(new[] { 2, 1 }, new[] { 1, 2 })] int[] reorgDepths,
+            [Values] bool compactBetween
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage();
+
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            var testBlocks = testData.Batches.SelectMany(b => b).ToArray();
+
+            foreach (var reorgDepth in reorgDepths)
+            {
+                foreach (BlockReceipts block in testBlocks.TakeLast(reorgDepth).ToArray())
+                    await logIndexStorage.ReorgFrom(block);
+
+                if (compactBetween)
+                    await CompactAsync(logIndexStorage);
+            }
+
+            VerifyReceipts(logIndexStorage, testData, maxBlock: testBlocks[^1].BlockNumber - reorgDepths.Max());
+        }
+
+        [Combinatorial]
+        public async Task SetMultiInstance_Get_Test(
+            [Values(100, int.MaxValue)] int compactionDistance,
+            [Values] bool isBackwardsSync
+        )
+        {
+            var half = testData.Batches.Length / 2;
+
+            await using (var logIndexStorage = CreateLogIndexStorage(compactionDistance))
+                await SetReceiptsAsync(logIndexStorage, testData.Batches.Take(half));
+
+            await using (var logIndexStorage = CreateLogIndexStorage(compactionDistance))
+                await SetReceiptsAsync(logIndexStorage, testData.Batches.Skip(half));
+
+            await using (var logIndexStorage = CreateLogIndexStorage(compactionDistance))
+                VerifyReceipts(logIndexStorage, testData);
+        }
+
+        [Combinatorial]
+        public async Task RepeatedSet_Get_Test(
+            [Values(100, int.MaxValue)] int compactionDistance,
+            [Values] bool isBackwardsSync
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage(compactionDistance);
+
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            VerifyReceipts(logIndexStorage, testData);
+        }
+
+        [Combinatorial]
+        public async Task RepeatedSetMultiInstance_Get_Test(
+            [Values(100, int.MaxValue)] int compactionDistance,
+            [Values] bool isBackwardsSync
+        )
+        {
+            await using (var logIndexStorage = CreateLogIndexStorage(compactionDistance))
+                await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            await using (var logIndexStorage = CreateLogIndexStorage(compactionDistance))
+                await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            await using (var logIndexStorage = CreateLogIndexStorage(compactionDistance))
+                VerifyReceipts(logIndexStorage, testData);
+        }
+
+        [Combinatorial]
+        public async Task Set_NewInstance_Get_Test(
+            [Values(1, 8)] int ioParallelism,
+            [Values(100, int.MaxValue)] int compactionDistance,
+            [Values] bool isBackwardsSync
+        )
+        {
+            await using (var logIndexStorage = CreateLogIndexStorage(compactionDistance))
+                await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            await using (var logIndexStorage = CreateLogIndexStorage(compactionDistance))
+                VerifyReceipts(logIndexStorage, testData);
+        }
+
+        [Combinatorial]
+        [Repeat(RaceConditionTestRepeat)]
+        [SuppressMessage("ReSharper", "AccessToDisposedClosure")]
+        public async Task Set_ConcurrentGet_Test(
+            [Values(100, int.MaxValue)] int compactionDistance,
+            [Values] bool isBackwardsSync
+        )
+        {
+            var logIndexStorage = CreateLogIndexStorage(compactionDistance);
+
+            using var getCancellation = new CancellationTokenSource();
+            var token = getCancellation.Token;
+
+            ConcurrentBag<Exception> exceptions = [];
+            var getThreads = new[]
+            {
+                new Thread(() => VerifyReceiptsPartialLoop(new Random(42), logIndexStorage, testData, exceptions, token)),
+                new Thread(() => VerifyReceiptsPartialLoop(new Random(4242), logIndexStorage, testData, exceptions, token)),
+                new Thread(() => VerifyReceiptsPartialLoop(new Random(424242), logIndexStorage, testData, exceptions, token)),
+            };
+            getThreads.ForEach(t => t.Start());
+
+            await SetReceiptsAsync(logIndexStorage, testData.Batches);
+
+            await getCancellation.CancelAsync();
+            getThreads.ForEach(t => t.Join());
+
+            if (exceptions.FirstOrDefault() is { } exception)
+                ExceptionDispatchInfo.Capture(exception).Throw();
+
+            VerifyReceipts(logIndexStorage, testData);
+        }
+
+        private static BlockReceipts[] GenerateBlocks(Random random, int from, int count) =>
+            new TestData(random, 1, count, startNum: from).Batches[0];
+
+        private async Task SetReceiptsAsync(ILogIndexStorage logIndexStorage, IEnumerable<BlockReceipts[]> batches, bool isBackwardsSync = false)
+        {
+            var timestamp = Stopwatch.GetTimestamp();
+            var totalStats = new LogIndexUpdateStats(logIndexStorage);
+            var (count, length) = (0, 0);
+            foreach (BlockReceipts[] batch in batches)
+            {
+                count++;
+                length = batch.Length;
+                await logIndexStorage.SetReceiptsAsync(batch, isBackwardsSync, totalStats);
+            }
+
+            // Log statistics
+            await TestContext.Out.WriteLineAsync(
+                $"""
+                 x{count} {nameof(LogIndexStorage.SetReceiptsAsync)}([{length}], {isBackwardsSync}) in {Stopwatch.GetElapsedTime(timestamp)}:
+                 {totalStats:d}
+                 {'\t'}DB size: {logIndexStorage.GetDbSize()}
+
+                 """
+            );
+        }
+
+        private static void VerifyReceipts(ILogIndexStorage logIndexStorage, TestData testData,
+            Dictionary<Address, HashSet<int>>? excludedAddresses = null,
+            Dictionary<int, Dictionary<Hash256, HashSet<int>>>? excludedTopics = null,
+            HashSet<int>? excludedBlockNums = null,
+            Dictionary<Address, HashSet<int>>? addedAddresses = null,
+            Dictionary<int, Dictionary<Hash256, HashSet<int>>>? addedTopics = null,
+            int? minBlock = null, int? maxBlock = null,
+            bool validateMinMax = true
+        )
+        {
+            minBlock ??= testData.Batches[0][0].BlockNumber;
+            maxBlock ??= testData.Batches[^1][^1].BlockNumber;
+
+            if (validateMinMax)
+            {
+                using (Assert.EnterMultipleScope())
+                {
+                    Assert.That(logIndexStorage.GetMinBlockNumber(), Is.EqualTo(minBlock));
+                    Assert.That(logIndexStorage.GetMaxBlockNumber(), Is.EqualTo(maxBlock));
+                }
+            }
+
+            foreach (var (address, nums) in testData.AddressMap)
+            {
+                IEnumerable<int> expectedNums = nums;
+
+                if (excludedAddresses != null && excludedAddresses.TryGetValue(address, out HashSet<int> addressExcludedBlocks))
+                    expectedNums = expectedNums.Except(addressExcludedBlocks);
+
+                if (excludedBlockNums != null)
+                    expectedNums = expectedNums.Except(excludedBlockNums);
+
+                if (addedAddresses != null && addedAddresses.TryGetValue(address, out HashSet<int> addressAddedBlocks))
+                    expectedNums = expectedNums.Concat(addressAddedBlocks);
+
+                expectedNums = expectedNums.Order();
+
+                if (minBlock > testData.Batches[0][0].BlockNumber)
+                    expectedNums = expectedNums.SkipWhile(b => b < minBlock);
+
+                if (maxBlock < testData.Batches[^1][^1].BlockNumber)
+                    expectedNums = expectedNums.TakeWhile(b => b <= maxBlock);
+
+                expectedNums = expectedNums.ToArray();
+
+                foreach (var (from, to) in testData.Ranges)
+                {
+                    Assert.That(
+                        logIndexStorage.GetBlockNumbersFor(address, from, to),
+                        Is.EqualTo(expectedNums.SkipWhile(i => i < from).TakeWhile(i => i <= to)),
+                        $"Address: {address}, from {from} to {to}"
+                    );
+                }
+            }
+
+            foreach (var (idx, byTopic) in testData.TopicMap)
+            {
+                foreach (var (topic, nums) in byTopic)
+                {
+                    IEnumerable<int> expectedNums = nums;
+
+                    if (excludedTopics != null && excludedTopics[idx].TryGetValue(topic, out HashSet<int> topicExcludedBlocks))
+                        expectedNums = expectedNums.Except(topicExcludedBlocks);
+
+                    if (excludedBlockNums != null)
+                        expectedNums = expectedNums.Except(excludedBlockNums);
+
+                    if (addedTopics != null && addedTopics[idx].TryGetValue(topic, out HashSet<int> topicAddedBlocks))
+                        expectedNums = expectedNums.Concat(topicAddedBlocks);
+
+                    expectedNums = expectedNums.Order();
+
+                    if (minBlock > testData.Batches[0][0].BlockNumber)
+                        expectedNums = expectedNums.SkipWhile(b => b < minBlock);
+
+                    if (maxBlock < testData.Batches[^1][^1].BlockNumber)
+                        expectedNums = expectedNums.TakeWhile(b => b <= maxBlock);
+
+                    expectedNums = expectedNums.ToArray();
+
+                    foreach (var (from, to) in testData.Ranges)
+                    {
+                        Assert.That(
+                            logIndexStorage.GetBlockNumbersFor(idx, topic, from, to),
+                            Is.EqualTo(expectedNums.SkipWhile(i => i < from).TakeWhile(i => i <= to)),
+                            $"Topic: [{idx}] {topic}, {from} - {to}"
+                        );
+                    }
+                }
+            }
+        }
+
+        private static void VerifyReceipts(ILogIndexStorage logIndexStorage, TestData testData,
+            IEnumerable<BlockReceipts>? excludedBlocks, IEnumerable<BlockReceipts>? addedBlocks = null,
+            int? minBlock = null, int? maxBlock = null,
+            bool validateMinMax = true)
+        {
+            var excludeMaps = excludedBlocks == null ? default : TestData.GenerateMaps(excludedBlocks);
+            var addMaps = addedBlocks == null ? default : TestData.GenerateMaps(addedBlocks);
+
+            VerifyReceipts(
+                logIndexStorage, testData,
+                excludedAddresses: excludeMaps.address, excludedTopics: excludeMaps.topic,
+                addedAddresses: addMaps.address, addedTopics: addMaps.topic,
+                minBlock: minBlock, maxBlock: maxBlock,
+                validateMinMax: validateMinMax
+            );
+        }
+
+        private static void VerifyReceiptsPartialLoop(Random random, ILogIndexStorage logIndexStorage, TestData testData,
+            ConcurrentBag<Exception> exceptions, CancellationToken cancellationToken)
+        {
+            try
+            {
+                var (addresses, topics) = (testData.Addresses, testData.Topics);
+
+                while (!cancellationToken.IsCancellationRequested)
+                {
+                    if (addresses.Count != 0)
+                    {
+                        var address = random.NextValue(addresses);
+                        var expectedNums = testData.AddressMap[address];
+
+                        if (logIndexStorage.GetMinBlockNumber() is not { } min || logIndexStorage.GetMaxBlockNumber() is not { } max)
+                            continue;
+
+                        Assert.That(
+                            logIndexStorage.GetBlockNumbersFor(address, min, max),
+                            Is.EqualTo(expectedNums.SkipWhile(i => i < min).TakeWhile(i => i <= max)),
+                            $"Address: {address}, available: {min} - {max}"
+                        );
+                    }
+
+                    if (topics.Count != 0)
+                    {
+                        var (idx, topic) = random.NextValue(topics);
+                        var expectedNums = testData.TopicMap[idx][topic];
+
+                        if (logIndexStorage.GetMinBlockNumber() is not { } min || logIndexStorage.GetMaxBlockNumber() is not { } max)
+                            continue;
+
+                        Assert.That(
+                            logIndexStorage.GetBlockNumbersFor(idx, topic, min, max),
+                            Is.EqualTo(expectedNums.SkipWhile(i => i < min).TakeWhile(i => i <= max)),
+                            $"Topic: [{idx}] {topic}, available: {min} - {max}"
+                        );
+                    }
+                }
+            }
+            catch (Exception ex)
+            {
+                exceptions.Add(ex);
+            }
+        }
+
+        private static BlockReceipts[][] Reverse(IEnumerable<BlockReceipts[]> batches)
+        {
+            var length = batches.Count();
+            var result = new BlockReceipts[length][];
+
+            var index = 0;
+            foreach (BlockReceipts[] batch in batches.Reverse())
+                result[index++] = batch.Reverse().ToArray();
+
+            return result;
+        }
+
+        private async Task CompactAsync(ILogIndexStorage logIndexStorage)
+        {
+            const bool flush = true;
+
+            var timestamp = Stopwatch.GetTimestamp();
+            await logIndexStorage.CompactAsync(flush);
+
+            // Log statistics
+            await TestContext.Out.WriteLineAsync(
+                $"""
+                 {nameof(LogIndexStorage.CompactAsync)}({flush}) in {Stopwatch.GetElapsedTime(timestamp)}:
+                 {'\t'}DB size: {logIndexStorage.GetDbSize()}
+
+                 """
+            );
+        }
+
+        public class TestData
+        {
+            private readonly int _batchCount;
+            private readonly int _blocksPerBatch;
+            private readonly int _startNum;
+
+            // To avoid generating all the data just to display test cases
+            private readonly Lazy<BlockReceipts[][]> _batches;
+            public BlockReceipts[][] Batches => _batches.Value;
+
+            private readonly Lazy<IEnumerable<(int from, int to)>> _ranges;
+            public IEnumerable<(int from, int to)> Ranges => _ranges.Value;
+
+            public Dictionary<Address, HashSet<int>> AddressMap { get; private set; } = new();
+            public Dictionary<int, Dictionary<Hash256, HashSet<int>>> TopicMap { get; private set; } = new();
+
+            public List<Address> Addresses { get; private set; }
+            public List<(int, Hash256)> Topics { get; private set; }
+
+            public bool ExtendedGetRanges { get; init; }
+
+            public TestData(Random random, int batchCount, int blocksPerBatch, int startNum = 0)
+            {
+                _batchCount = batchCount;
+                _blocksPerBatch = blocksPerBatch;
+                _startNum = startNum;
+
+                // Populated during GenerateBatches()
+                Addresses = null!;
+                Topics = null!;
+
+                _batches = new(() => GenerateBatches(random, batchCount, blocksPerBatch, startNum));
+                _ranges = new(() => ExtendedGetRanges ? GenerateExtendedRanges() : GenerateSimpleRanges());
+            }
+
+            public TestData(int batchCount, int blocksPerBatch, int startNum = 0) : this(new(42), batchCount, blocksPerBatch, startNum) { }
+
+            private BlockReceipts[][] GenerateBatches(Random random, int batchCount, int blocksPerBatch, int startNum = 0)
+            {
+                var batches = new BlockReceipts[batchCount][];
+                var blocksCount = batchCount * blocksPerBatch;
+
+                Address[] customAddresses =
+                [
+                    Address.Zero, Address.MaxValue,
+                    new(new byte[] { 1 }.PadLeft(Address.Size)), new(new byte[] { 1, 1 }.PadLeft(Address.Size)),
+                    new(new byte[] { 1 }.PadRight(Address.Size)), new(new byte[] { 1, 1 }.PadRight(Address.Size)),
+                    new(new byte[] { 0 }.PadLeft(Address.Size, 0xFF)), new(new byte[] { 0 }.PadRight(Address.Size, 0xFF)),
+                ];
+
+                Hash256[] customTopics =
+                [
+                    Hash256.Zero, new(Array.Empty<byte>().PadRight(Hash256.Size, 0xFF)),
+                    new(new byte[] { 0 }.PadLeft(Hash256.Size)), new(new byte[] { 1 }.PadLeft(Hash256.Size)),
+                    new(new byte[] { 0 }.PadRight(Hash256.Size)), new(new byte[] { 1 }.PadRight(Hash256.Size)),
+                    new(new byte[] { 0 }.PadLeft(Hash256.Size, 0xFF)), new(new byte[] { 0 }.PadRight(Hash256.Size, 0xFF)),
+                ];
+
+                var addresses = Enumerable.Repeat(0, Math.Max(10, blocksCount / 5) - customAddresses.Length)
+                //var addresses = Enumerable.Repeat(0, 0)
+                    .Select(_ => new Address(random.NextBytes(Address.Size)))
+                    .Concat(customAddresses)
+                    .ToArray();
+                var topics = Enumerable.Repeat(0, addresses.Length * 7 - customTopics.Length)
+                //var topics = Enumerable.Repeat(0, 0)
+                    .Select(_ => new Hash256(random.NextBytes(Hash256.Size)))
+                    .Concat(customTopics)
+                    .ToArray();
+
+                // Generate batches
+                var blockNum = startNum;
+                for (var i = 0; i < batches.Length; i++)
+                {
+                    var batch = batches[i] = new BlockReceipts[blocksPerBatch];
+
+                    for (var j = 0; j < batch.Length; j++)
+                        batch[j] = new(blockNum++, GenerateReceipts(random, addresses, topics));
+                }
+
+                var maps = GenerateMaps(batches.SelectMany(b => b));
+
+                (AddressMap, TopicMap) = (maps.address, maps.topic);
+                (Addresses, Topics) = (maps.address.Keys.ToList(), maps.topic.SelectMany(byIdx => byIdx.Value.Select(byTpc => (byIdx.Key, byTpc.Key)))
+                    .ToList());
+
+                return batches;
+            }
+
+            public static (Dictionary<Address, HashSet<int>> address, Dictionary<int, Dictionary<Hash256, HashSet<int>>> topic) GenerateMaps(
+                IEnumerable<BlockReceipts> blocks)
+            {
+                var address = new Dictionary<Address, HashSet<int>>();
+                var topic = new Dictionary<int, Dictionary<Hash256, HashSet<int>>>();
+
+                foreach (var block in blocks)
+                {
+                    foreach (var txReceipt in block.Receipts)
+                    {
+                        foreach (var log in txReceipt.Logs!)
+                        {
+                            var addressMap = address.GetOrAdd(log.Address, static _ => []);
+                            addressMap.Add(block.BlockNumber);
+
+                            for (var i = 0; i < log.Topics.Length; i++)
+                            {
+                                var topicI = topic.GetOrAdd(i, static _ => []);
+                                var topicMap = topicI.GetOrAdd(log.Topics[i], static _ => []);
+                                topicMap.Add(block.BlockNumber);
+                            }
+                        }
+                    }
+                }
+
+                return (address, topic);
+            }
+
+            private static TxReceipt[] GenerateReceipts(Random random, Address[] addresses, Hash256[] topics)
+            {
+                (int min, int max) logsPerBlock = (0, 200);
+                (int min, int max) logsPerTx = (0, 10);
+
+                LogEntry[] logs = Enumerable
+                    .Repeat(0, random.Next(logsPerBlock.min, logsPerBlock.max + 1))
+                    .Select(_ => Build.A.LogEntry
+                        .WithAddress(random.NextValue(addresses))
+                        .WithTopics(topics.Length == 0
+                            ? []
+                            : Enumerable.Repeat(0, random.Next(4)).Select(_ => random.NextValue(topics)).ToArray()
+                        ).TestObject
+                    ).ToArray();
+
+                var receipts = new List<TxReceipt>();
+                for (var i = 0; i < logs.Length;)
+                {
+                    var count = random.Next(logsPerTx.min, Math.Min(logsPerTx.max, logs.Length - i) + 1);
+                    var range = i..(i + count);
+
+                    receipts.Add(new() { Logs = logs[range] });
+                    i = range.End.Value;
+                }
+
+                return receipts.ToArray();
+            }
+
+            private static HashSet<(int from, int to)> GenerateSimpleRanges(int min, int max)
+            {
+                var quarter = (max - min) / 4;
+                return [(0, int.MaxValue), (min, max), (min + quarter, max - quarter)];
+            }
+
+            private static HashSet<(int from, int to)> GenerateExtendedRanges(int min, int max)
+            {
+                var ranges = new HashSet<(int, int)>();
+
+                var edges = new[] { min - 1, min, min + 1, max - 1, max + 1 };
+                ranges.AddRange(edges.SelectMany(_ => edges, static (x, y) => (x, y)));
+
+                const int step = 100;
+                for (var i = min; i <= max; i += step)
+                {
+                    var middles = new[] { i - step, i - 1, i, i + 1, i + step };
+                    ranges.AddRange(middles.SelectMany(_ => middles, static (x, y) => (x, y)));
+                }
+
+                return ranges;
+            }
+
+            private HashSet<(int from, int to)> GenerateSimpleRanges() => GenerateSimpleRanges(
+                _startNum, _startNum + _batchCount * _blocksPerBatch - 1
+            );
+
+            private HashSet<(int from, int to)> GenerateExtendedRanges() => GenerateExtendedRanges(
+                _startNum, _startNum + _batchCount * _blocksPerBatch - 1
+            );
+
+            public override string ToString() => $"{_batchCount} * {_blocksPerBatch} blocks (ex-ranges: {ExtendedGetRanges})";
+        }
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db.Test/Nethermind.Db.Test.csproj b/src/Nethermind/Nethermind.Db.Test/Nethermind.Db.Test.csproj
index a6ec51bc5f..033fa0b47b 100644
--- a/src/Nethermind/Nethermind.Db.Test/Nethermind.Db.Test.csproj
+++ b/src/Nethermind/Nethermind.Db.Test/Nethermind.Db.Test.csproj
@@ -4,6 +4,7 @@
   
   <PropertyGroup>
     <Nullable>annotations</Nullable>
+    <AllowUnsafeBlocks>true</AllowUnsafeBlocks>
   </PropertyGroup>
   <ItemGroup>
     <PackageReference Include="NSubstitute" />
diff --git a/src/Nethermind/Nethermind.Db.Test/TurboPForTests.cs b/src/Nethermind/Nethermind.Db.Test/TurboPForTests.cs
new file mode 100644
index 0000000000..e4eb495394
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db.Test/TurboPForTests.cs
@@ -0,0 +1,212 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Buffers.Binary;
+using System.Collections.Generic;
+using System.Linq;
+using System.Runtime.InteropServices;
+using System.Threading.Tasks;
+using NUnit.Framework;
+using NUnit.Framework.Legacy;
+
+namespace Nethermind.Db.Test;
+
+// More for documenting how the library works and comparing compression sizes
+public class TurboPForTests
+{
+    private static IEnumerable<int> Lengths()
+    {
+        yield return 1;
+        yield return 10;
+
+        for (var i = 32; i <= 1024; i <<= 1)
+        {
+            yield return i - 1;
+            yield return i;
+            yield return i + 1;
+        }
+    }
+
+    private static IEnumerable<int> Deltas()
+    {
+        yield return 10;
+        yield return 20;
+        yield return 50;
+        yield return 100;
+        yield return 1000;
+    }
+
+    [TestCaseSource(nameof(Lengths))]
+    public unsafe void p4nd1enc256v32_Increasing_Consecutive(int length)
+    {
+        var values = Enumerable.Range(0, length).ToArray();
+        var compressed = Compress(values, TurboPFor.p4nd1enc256v32);
+        var decompressed = Decompress(compressed, values.Length, TurboPFor.p4nd1dec256v32);
+
+        Assert.That(decompressed, Is.EquivalentTo(values));
+    }
+
+    [TestCaseSource(nameof(Lengths))]
+    public unsafe void p4nd1enc128v32_Increasing_Consecutive(int length)
+    {
+        var values = Enumerable.Range(0, length).ToArray();
+        var compressed = Compress(values, TurboPFor.p4nd1enc128v32);
+        var decompressed = Decompress(compressed, values.Length, TurboPFor.p4nd1dec128v32);
+
+        Assert.That(decompressed, Is.EquivalentTo(values));
+    }
+
+    [Combinatorial]
+    public unsafe void p4nd1enc256v32_Increasing_Random(
+        [ValueSource(nameof(Lengths))] int length,
+        [ValueSource(nameof(Deltas))] int maxDelta
+    )
+    {
+        var values = RandomIncreasingRange(new(42), length, maxDelta).ToArray();
+        var compressed = Compress(values, TurboPFor.p4nd1enc256v32);
+        var decompressed = Decompress(compressed, values.Length, TurboPFor.p4nd1dec256v32);
+
+        Assert.That(decompressed, Is.EquivalentTo(values));
+    }
+
+    [Combinatorial]
+    public unsafe void p4nd1enc128v32_Increasing_Random(
+        [ValueSource(nameof(Lengths))] int length,
+        [ValueSource(nameof(Deltas))] int maxDelta
+    )
+    {
+        var values = RandomIncreasingRange(new(42), length, maxDelta).ToArray();
+        var compressed = Compress(values, TurboPFor.p4nd1enc128v32);
+        var decompressed = Decompress(compressed, values.Length, TurboPFor.p4nd1dec128v32);
+
+        Assert.That(decompressed, Is.EquivalentTo(values));
+    }
+
+    [TestCaseSource(nameof(Lengths))]
+    public unsafe void p4nd1enc256v32_Increasing_Consecutive_Negative(int length)
+    {
+        var values = Enumerable.Range(0, length).Reverse().Select(x => -x).ToArray();
+        var compressed = Compress(values, TurboPFor.p4nd1enc256v32);
+        var decompressed = Decompress(compressed, values.Length, TurboPFor.p4nd1dec256v32);
+
+        Assert.That(decompressed, Is.EquivalentTo(values));
+    }
+
+    [TestCaseSource(nameof(Lengths))]
+    public unsafe void p4nd1enc256v32_Decreasing_Consecutive(int length)
+    {
+        var values = Enumerable.Range(0, length).Reverse().ToArray();
+        var compressed = Compress(values, TurboPFor.p4nd1enc256v32);
+        var decompressed = Decompress(compressed, values.Length, TurboPFor.p4nd1dec256v32);
+
+        Assert.That(decompressed, Is.EquivalentTo(values));
+    }
+
+    [Combinatorial]
+    public unsafe void p4nd1enc256v32_Increasing_Random_Negative(
+        [ValueSource(nameof(Lengths))] int length,
+        [ValueSource(nameof(Deltas))] int maxDelta
+    )
+    {
+        var values = RandomIncreasingRange(new(42), length, maxDelta).Reverse().Select(x => -x).ToArray();
+        var compressed = Compress(values, TurboPFor.p4nd1enc256v32);
+        var decompressed = Decompress(compressed, values.Length, TurboPFor.p4nd1dec256v32);
+
+        Assert.That(decompressed, Is.EquivalentTo(values));
+    }
+
+    [Combinatorial]
+    public unsafe void p4nd1enc256v32_Decreasing_Random(
+        [ValueSource(nameof(Lengths))] int length,
+        [ValueSource(nameof(Deltas))] int maxDelta
+    )
+    {
+        var values = RandomIncreasingRange(new(42), length, maxDelta).Reverse().ToArray();
+        var compressed = Compress(values, TurboPFor.p4nd1enc256v32);
+        var decompressed = Decompress(compressed, values.Length, TurboPFor.p4nd1dec256v32);
+
+        Assert.That(decompressed, Is.EquivalentTo(values));
+    }
+
+    private static IEnumerable<int> RandomIncreasingRange(Random random, int length, int maxDelta)
+    {
+        var value = 0;
+        for (var i = 0; i < length; i++)
+        {
+            value += random.Next(maxDelta);
+            yield return value;
+        }
+    }
+
+    private unsafe delegate nuint CompressFunc(int* @in, nuint n, byte* @out);
+
+    private unsafe delegate nuint DecompressFunc(byte* @in, nuint n, int* @out);
+
+    private unsafe delegate byte* CompressBlockFunc(int* @in, int n, byte* @out, int start);
+
+    private unsafe delegate byte* DecompressBlockFunc(byte* @in, int n, int* @out, int start);
+
+    private static unsafe byte[] Compress(int[] values, CompressBlockFunc compressFunc, int deltaStart = 0)
+    {
+        var buffer = new byte[values.Length * sizeof(int) + 1024];
+
+        int resultLength;
+        fixed (int* inputPtr = values)
+        fixed (byte* resultPtr = buffer)
+        {
+            var endPtr = compressFunc(inputPtr, values.Length, resultPtr, deltaStart);
+            resultLength = (int)(endPtr - (long)resultPtr);
+        }
+
+        //TestContext.Out.WriteLine($"Compressed: {resultLength} bytes");
+        return buffer[..resultLength];
+    }
+
+    private static unsafe int[] Decompress(byte[] data, int count, DecompressBlockFunc decompressFunc, int deltaStart = 0)
+    {
+        var buffer = new int[count + 1024];
+
+        fixed (byte* inputPtr = data)
+        fixed (int* resultPtr = buffer)
+        {
+            var endPtr = decompressFunc(inputPtr, count, resultPtr, deltaStart);
+        }
+
+        return buffer[..count];
+    }
+
+    private static unsafe byte[] Compress(int[] values, CompressFunc compressFunc)
+    {
+        var buffer = new byte[values.Length * sizeof(int) + 1024];
+
+        int resultLength;
+        fixed (int* inputPtr = values)
+        fixed (byte* resultPtr = buffer)
+        {
+            resultLength = (int)compressFunc(inputPtr, (nuint)values.Length, resultPtr);
+        }
+
+        //TestContext.Out.WriteLine($"Compressed: {resultLength} bytes");
+        return buffer[..resultLength];
+    }
+
+    private static unsafe int[] Decompress(byte[] data, int count, DecompressFunc decompressFunc)
+    {
+        //var buffer = new int[count];
+
+        var buffer = new int[count * 2];
+        for (var i = count; i < buffer.Length; i++)
+            buffer[i] = -1;
+
+        fixed (byte* inputPtr = data)
+        fixed (int* resultPtr = buffer)
+        {
+            _ = decompressFunc(inputPtr, (nuint)count, resultPtr);
+        }
+
+        for (var i = count; i < buffer.Length; i++) Assert.That(buffer[i], Is.EqualTo(-1));
+
+        return buffer[..count];
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/CompressingDb.cs b/src/Nethermind/Nethermind.Db/CompressingDb.cs
index 616298983d..8a6bbe3b5c 100644
--- a/src/Nethermind/Nethermind.Db/CompressingDb.cs
+++ b/src/Nethermind/Nethermind.Db/CompressingDb.cs
@@ -52,6 +52,11 @@ namespace Nethermind.Db
                     _wrapped.PutSpan(key, Compress(value, stackalloc byte[value.Length]), flags);
                 }
 
+                public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+                {
+                    throw new InvalidOperationException("EOA compressing DB does not support merging");
+                }
+
                 public bool PreferWriteByArray => _wrapped.PreferWriteByArray;
 
                 public byte[]? this[ReadOnlySpan<byte> key]
@@ -133,6 +138,11 @@ namespace Nethermind.Db
             public IEnumerable<byte[]> GetAllValues(bool ordered = false) =>
                 _wrapped.GetAllValues(ordered).Select(Decompress);
 
+            public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+            {
+                _wrapped.Merge(key, value, flags);
+            }
+
             public void Remove(ReadOnlySpan<byte> key) => _wrapped.Remove(key);
 
             public bool KeyExists(ReadOnlySpan<byte> key) => _wrapped.KeyExists(key);
@@ -169,6 +179,16 @@ namespace Nethermind.Db
                 if (_wrapped is ITunableDb tunable)
                     tunable.Tune(type);
             }
+
+            public IIterator GetIterator(bool isTailing = false)
+            {
+                return _wrapped.GetIterator(isTailing);
+            }
+
+            public IIterator GetIterator(ref IteratorOptions options)
+            {
+                return _wrapped.GetIterator(ref options);
+            }
         }
     }
 }
diff --git a/src/Nethermind/Nethermind.Db/DbNames.cs b/src/Nethermind/Nethermind.Db/DbNames.cs
index e0f96bbc64..c7e275a4ba 100644
--- a/src/Nethermind/Nethermind.Db/DbNames.cs
+++ b/src/Nethermind/Nethermind.Db/DbNames.cs
@@ -19,5 +19,6 @@ namespace Nethermind.Db
         public const string BlobTransactions = "blobTransactions";
         public const string DiscoveryNodes = "discoveryNodes";
         public const string PeersDb = "peers";
+        public const string LogIndex = "logIndex";
     }
 }
diff --git a/src/Nethermind/Nethermind.Db/FullPruning/FullPruningDb.cs b/src/Nethermind/Nethermind.Db/FullPruning/FullPruningDb.cs
index fb2c989427..14528c9969 100755
--- a/src/Nethermind/Nethermind.Db/FullPruning/FullPruningDb.cs
+++ b/src/Nethermind/Nethermind.Db/FullPruning/FullPruningDb.cs
@@ -91,6 +91,16 @@ namespace Nethermind.Db.FullPruning
             }
         }
 
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            _currentDb.Merge(key, value, flags); // we are writing to the main DB
+            IDb? cloningDb = _pruningContext?.CloningDb;
+            if (cloningDb is not null) // if pruning is in progress we are also writing to the secondary, copied DB
+            {
+                DuplicateMerge(cloningDb, key, value, flags);
+            }
+        }
+
         private void Duplicate(IWriteOnlyKeyValueStore db, ReadOnlySpan<byte> key, byte[]? value, WriteFlags flags)
         {
             db.Set(key, value, flags);
@@ -103,6 +113,12 @@ namespace Nethermind.Db.FullPruning
             _updateDuplicateWriteMetrics?.Invoke();
         }
 
+        private void DuplicateMerge(IDb db, ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags)
+        {
+            db.Merge(key, value, flags);
+            _updateDuplicateWriteMetrics?.Invoke();
+        }
+
         // we also need to duplicate writes that are in batches
         public IWriteBatch StartWriteBatch() =>
             _pruningContext is null
@@ -322,6 +338,11 @@ namespace Nethermind.Db.FullPruning
                 _writeBatch.Set(key, value, flags);
                 _db.Duplicate(_clonedWriteBatch, key, value, flags);
             }
+
+            public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+            {
+                throw new NotSupportedException("Merging is not supported by this implementation.");
+            }
         }
 
         public void Tune(ITunableDb.TuneType type)
@@ -331,5 +352,15 @@ namespace Nethermind.Db.FullPruning
                 tunableDb.Tune(type);
             }
         }
+
+        public IIterator GetIterator(bool isTailing = false)
+        {
+            return _currentDb.GetIterator(isTailing);
+        }
+
+        public IIterator GetIterator(ref IteratorOptions options)
+        {
+            return _currentDb.GetIterator(ref options);
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Db/IDb.cs b/src/Nethermind/Nethermind.Db/IDb.cs
index 73f9e4ba17..ae2ab34ceb 100644
--- a/src/Nethermind/Nethermind.Db/IDb.cs
+++ b/src/Nethermind/Nethermind.Db/IDb.cs
@@ -14,8 +14,24 @@ namespace Nethermind.Db
         IEnumerable<KeyValuePair<byte[], byte[]?>> GetAll(bool ordered = false);
         IEnumerable<byte[]> GetAllKeys(bool ordered = false);
         IEnumerable<byte[]> GetAllValues(bool ordered = false);
-
+        IIterator GetIterator(bool isTailing = false);
+        IIterator GetIterator(ref IteratorOptions options);
         public IReadOnlyDb CreateReadOnly(bool createInMemWriteStore) => new ReadOnlyDb(this, createInMemWriteStore);
+
+        // TODO: move to IWriteOnlyKeyValueStore?
+        void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None);
+    }
+
+    public ref struct IteratorOptions
+    {
+        public byte[]? LowerBound { get; init; }
+        public byte[]? UpperBound { get; init; }
+
+        /// <summary>
+        /// Whether to create a tailing operator.
+        /// </summary>
+        /// <remarks>https://github.com/facebook/rocksdb/wiki/Tailing-Iterator</remarks>
+        public bool IsTailing { get; init; }
     }
 
     // Some metadata options
diff --git a/src/Nethermind/Nethermind.Db/IIterator.cs b/src/Nethermind/Nethermind.Db/IIterator.cs
new file mode 100644
index 0000000000..fcde9e0dfe
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/IIterator.cs
@@ -0,0 +1,18 @@
+// SPDX-FileCopyrightText: 2024 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+
+namespace Nethermind.Db;
+
+public interface IIterator : IDisposable
+{
+    void SeekToFirst();
+    void Seek(ReadOnlySpan<byte> key);
+    void SeekForPrev(ReadOnlySpan<byte> key);
+    void Next();
+    void Prev();
+    bool Valid();
+    ReadOnlySpan<byte> Key();
+    ReadOnlySpan<byte> Value();
+}
diff --git a/src/Nethermind/Nethermind.Db/IMergeOperator.cs b/src/Nethermind/Nethermind.Db/IMergeOperator.cs
new file mode 100644
index 0000000000..7df2946bd7
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/IMergeOperator.cs
@@ -0,0 +1,14 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using Nethermind.Core.Collections;
+
+namespace Nethermind.Db;
+
+public interface IMergeOperator
+{
+    string Name { get; }
+    ArrayPoolList<byte>? FullMerge(ReadOnlySpan<byte> key, RocksDbMergeEnumerator enumerator);
+    ArrayPoolList<byte>? PartialMerge(ReadOnlySpan<byte> key, RocksDbMergeEnumerator enumerator);
+}
diff --git a/src/Nethermind/Nethermind.Db/InMemoryWriteBatch.cs b/src/Nethermind/Nethermind.Db/InMemoryWriteBatch.cs
index 7c43ec3903..c65d3a3142 100644
--- a/src/Nethermind/Nethermind.Db/InMemoryWriteBatch.cs
+++ b/src/Nethermind/Nethermind.Db/InMemoryWriteBatch.cs
@@ -34,5 +34,10 @@ namespace Nethermind.Db
             _currentItems[key.ToArray()] = value;
             _writeFlags = flags;
         }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            throw new NotSupportedException("Merging is not supported by this implementation.");
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/AverageStats.cs b/src/Nethermind/Nethermind.Db/LogIndex/AverageStats.cs
new file mode 100644
index 0000000000..4e6a600854
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/AverageStats.cs
@@ -0,0 +1,28 @@
+// SPDX-FileCopyrightText: 2024 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System.Threading;
+
+namespace Nethermind.Db;
+
+public class AverageStats
+{
+    private long _total;
+    private int _count;
+
+    public void Include(long value)
+    {
+        Interlocked.Add(ref _total, value);
+        Interlocked.Increment(ref _count);
+    }
+
+    public double Average => _count == 0 ? 0 : (double)_total / _count;
+
+    public override string ToString() => $"{Average:F2} ({_count:N0})";
+
+    public void Combine(AverageStats stats)
+    {
+        _total += stats._total;
+        _count += stats._count;
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/CompactingStats.cs b/src/Nethermind/Nethermind.Db/LogIndex/CompactingStats.cs
new file mode 100644
index 0000000000..0a37e4e986
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/CompactingStats.cs
@@ -0,0 +1,14 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+namespace Nethermind.Db;
+
+public class CompactingStats
+{
+    public ExecTimeStats Total { get; set; } = new();
+
+    public void Combine(CompactingStats other)
+    {
+        Total.Combine(other.Total);
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/Compactor.cs b/src/Nethermind/Nethermind.Db/LogIndex/Compactor.cs
new file mode 100644
index 0000000000..9d2d24dd3a
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/Compactor.cs
@@ -0,0 +1,144 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Diagnostics;
+using System.Threading;
+using System.Threading.Tasks;
+using Nethermind.Core.Extensions;
+using Nethermind.Logging;
+
+namespace Nethermind.Db;
+
+partial class LogIndexStorage
+{
+    private interface ICompactor
+    {
+        CompactingStats GetAndResetStats();
+        bool TryEnqueue();
+        Task StopAsync();
+        Task<CompactingStats> ForceAsync();
+    }
+
+    private class Compactor : ICompactor
+    {
+        private int? _lastAtMin;
+        private int? _lastAtMax;
+
+        private CompactingStats _stats = new();
+        private readonly LogIndexStorage _storage;
+        private readonly ILogger _logger;
+        private readonly int _compactionDistance;
+
+        // TODO: simplify concurrency handling
+        private readonly AutoResetEvent _runOnceEvent = new(false);
+        private readonly CancellationTokenSource _cancellationSource = new();
+        private readonly ManualResetEvent _compactionStartedEvent = new(false);
+        private readonly ManualResetEvent _compactionEndedEvent = new(true);
+        private readonly Task _compactionTask;
+
+        public Compactor(LogIndexStorage storage, ILogger logger, int compactionDistance)
+        {
+            _storage = storage;
+            _logger = logger;
+
+            if (compactionDistance < 1) throw new ArgumentException("Compaction distance must be a positive value.", nameof(compactionDistance));
+            _compactionDistance = compactionDistance;
+
+            _lastAtMin = storage.GetMinBlockNumber();
+            _lastAtMax = storage.GetMaxBlockNumber();
+
+            _compactionTask = DoCompactAsync();
+        }
+
+        public CompactingStats GetAndResetStats() => Interlocked.Exchange(ref _stats, new());
+
+        // Not thread-safe
+        public bool TryEnqueue()
+        {
+            _lastAtMin ??= _storage.GetMinBlockNumber();
+            _lastAtMax ??= _storage.GetMaxBlockNumber();
+
+            var uncompacted = 0;
+            if (_storage.GetMinBlockNumber() is { } storageMin && storageMin < _lastAtMin)
+                uncompacted += _lastAtMin.Value - storageMin;
+            if (_storage.GetMaxBlockNumber() is { } storageMax && storageMax > _lastAtMax)
+                uncompacted += storageMax - _lastAtMax.Value;
+
+            // TODO: cover other cases - space usage, RocksDB stats?
+            if (uncompacted < _compactionDistance)
+                return false;
+
+            if (!_runOnceEvent.Set())
+                return false;
+
+            _lastAtMin = _storage.GetMinBlockNumber();
+            _lastAtMax = _storage.GetMaxBlockNumber();
+            return true;
+        }
+
+        public async Task StopAsync()
+        {
+            await _cancellationSource.CancelAsync();
+            await _compactionEndedEvent.WaitOneAsync(CancellationToken.None);
+        }
+
+        public async Task<CompactingStats> ForceAsync()
+        {
+            // Wait for the previous one to finish
+            await _compactionEndedEvent.WaitOneAsync(_cancellationSource.Token);
+
+            _runOnceEvent.Set();
+            await _compactionStartedEvent.WaitOneAsync(100, _cancellationSource.Token);
+            await _compactionEndedEvent.WaitOneAsync(_cancellationSource.Token);
+            return _stats;
+        }
+
+        private async Task DoCompactAsync()
+        {
+            CancellationToken cancellation = _cancellationSource.Token;
+            while (!cancellation.IsCancellationRequested)
+            {
+                try
+                {
+                    await _runOnceEvent.WaitOneAsync(cancellation);
+
+                    _compactionEndedEvent.Reset();
+                    _compactionStartedEvent.Set();
+
+                    if (_logger.IsTrace)
+                        _logger.Trace("Compacting log index");
+
+                    var timestamp = Stopwatch.GetTimestamp();
+                    _storage._columnsDb.Compact();
+                    _stats.Total.Include(Stopwatch.GetElapsedTime(timestamp));
+
+                    if (_logger.IsTrace)
+                        _logger.Trace($"Compacted log index in {Stopwatch.GetElapsedTime(timestamp)}");
+                }
+                catch (TaskCanceledException ex) when (ex.CancellationToken == cancellation)
+                {
+                    return;
+                }
+                catch (Exception ex)
+                {
+                    if (_logger.IsError)
+                        _logger.Error("Failed to compact log index", ex);
+                }
+                finally
+                {
+                    _compactionStartedEvent.Reset();
+                    _compactionEndedEvent.Set();
+                }
+            }
+        }
+    }
+
+    private class NoOpCompactor : ICompactor
+    {
+        public CompactingStats GetAndResetStats() => new();
+        public bool TryEnqueue() => false;
+        public Task StopAsync() => Task.CompletedTask;
+        public Task<CompactingStats> ForceAsync() => Task.FromResult(new CompactingStats());
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/Compressor.cs b/src/Nethermind/Nethermind.Db/LogIndex/Compressor.cs
new file mode 100644
index 0000000000..4c8fbcf035
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/Compressor.cs
@@ -0,0 +1,167 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Collections.Concurrent;
+using System.Diagnostics;
+using System.Threading;
+using System.Threading.Tasks;
+using System.Threading.Tasks.Dataflow;
+using Nethermind.Core;
+using Nethermind.Core.Extensions;
+using Nethermind.Logging;
+
+namespace Nethermind.Db;
+
+partial class LogIndexStorage
+{
+    private interface ICompressor
+    {
+        PostMergeProcessingStats Stats { get; }
+        PostMergeProcessingStats GetAndResetStats();
+        bool TryEnqueue(int? topicIndex, ReadOnlySpan<byte> dbKey, ReadOnlySpan<byte> dbValue);
+        Task EnqueueAsync(int? topicIndex, byte[] dbKey);
+        void WaitUntilEmpty();
+        Task StopAsync();
+    }
+
+    private class Compressor : ICompressor
+    {
+        public const int MinLengthToCompress = 128 * BlockNumSize;
+
+        // A lot of duplicates in case of a regular Channel
+        // TODO: find a better way to guarantee uniqueness?
+        private readonly ConcurrentDictionary<byte[], bool> _compressQueue = new(Bytes.EqualityComparer);
+        private readonly LogIndexStorage _storage;
+        private readonly ILogger _logger;
+        private readonly ActionBlock<(int?, byte[])> _block;
+        private readonly ManualResetEventSlim _queueEmptyEvent = new(true); // TODO: fix event being used for both blocks
+
+        private PostMergeProcessingStats _stats = new();
+        public PostMergeProcessingStats Stats => _stats;
+
+        public PostMergeProcessingStats GetAndResetStats()
+        {
+            _stats.QueueLength = _block.InputCount;
+            return Interlocked.Exchange(ref _stats, new());
+        }
+
+        public Compressor(LogIndexStorage storage, ILogger logger, int ioParallelism)
+        {
+            _storage = storage;
+            _logger = logger;
+
+            if (ioParallelism < 1) throw new ArgumentException("IO parallelism degree must be a positive value.", nameof(ioParallelism));
+            _block = new(x => CompressValue(x.Item1, x.Item2), new() { MaxDegreeOfParallelism = ioParallelism, BoundedCapacity = 10_000 });
+        }
+
+        public bool TryEnqueue(int? topicIndex, ReadOnlySpan<byte> dbKey, ReadOnlySpan<byte> dbValue)
+        {
+            // ReSharper disable once ConditionIsAlwaysTrueOrFalse - may not initialized yet, compression can be started from the constructor
+            // TODO: add to queue, but start processing later?
+            if (_storage._columnsDb is null)
+                return false;
+
+            if (dbValue.Length < MinLengthToCompress)
+                return false;
+
+            var dbKeyArr = dbKey.ToArray();
+            if (!_compressQueue.TryAdd(dbKeyArr, true))
+                return false;
+
+            if (_block.Post((topicIndex, dbKeyArr)))
+                return true;
+
+            _compressQueue.TryRemove(dbKeyArr, out _);
+            return false;
+        }
+
+        public async Task EnqueueAsync(int? topicIndex, byte[] dbKey)
+        {
+            await _block.SendAsync((topicIndex, dbKey));
+            _queueEmptyEvent.Reset();
+        }
+
+        public void WaitUntilEmpty() => _queueEmptyEvent.Wait();
+
+        public Task StopAsync()
+        {
+            _block.Complete();
+            return _block.Completion;
+        }
+
+        // TODO: optimize allocations
+        private void CompressValue(int? topicIndex, byte[] dbKey)
+        {
+            try
+            {
+                var execTimestamp = Stopwatch.GetTimestamp();
+                IDb db = _storage.GetDb(topicIndex);
+
+                var timestamp = Stopwatch.GetTimestamp();
+                Span<byte> dbValue = db.Get(dbKey);
+                _stats.GettingValue.Include(Stopwatch.GetElapsedTime(timestamp));
+
+                // Do not compress blocks that can be reorged, as compressed data is immutable
+                if (!UseBackwardSyncFor(dbKey))
+                    dbValue = _storage.RemoveReorgableBlocks(dbValue);
+
+                if (dbValue.Length < MinLengthToCompress)
+                    return; // TODO: check back later?
+
+                var truncateBlock = GetValLastBlockNum(dbValue);
+
+                ReverseBlocksIfNeeded(dbValue);
+
+                var postfixBlock = GetValBlockNum(dbValue);
+
+                ReadOnlySpan<byte> key = ExtractKey(dbKey);
+                Span<byte> dbKeyComp = new byte[key.Length + BlockNumSize];
+                key.CopyTo(dbKeyComp);
+                SetKeyBlockNum(dbKeyComp[key.Length..], postfixBlock);
+
+                timestamp = Stopwatch.GetTimestamp();
+                dbValue = CompressDbValue(dbValue);
+                _stats.CompressingValue.Include(Stopwatch.GetElapsedTime(timestamp));
+
+                // Put compressed value at a new key and clear the uncompressed one
+                timestamp = Stopwatch.GetTimestamp();
+                using (IWriteBatch batch = db.StartWriteBatch())
+                {
+                    batch.PutSpan(dbKeyComp, dbValue);
+                    batch.Merge(dbKey, MergeOps.Create(MergeOp.TruncateOp, truncateBlock));
+                }
+
+                _stats.PuttingValues.Include(Stopwatch.GetElapsedTime(timestamp));
+
+                if (topicIndex is null)
+                    Interlocked.Increment(ref _stats.CompressedAddressKeys);
+                else
+                    Interlocked.Increment(ref _stats.CompressedTopicKeys);
+
+                _stats.Execution.Include(Stopwatch.GetElapsedTime(execTimestamp));
+            }
+            catch (Exception ex)
+            {
+                if (_logger.IsError) _logger.Error("Error during post-merge compression.", ex);
+            }
+            finally
+            {
+                _compressQueue.TryRemove(dbKey, out _);
+
+                if (_block.InputCount == 0)
+                    _queueEmptyEvent.Set();
+            }
+        }
+    }
+
+    public class NoOpCompressor : ICompressor
+    {
+        public PostMergeProcessingStats Stats { get; } = new();
+        public PostMergeProcessingStats GetAndResetStats() => Stats;
+        public bool TryEnqueue(int? topicIndex, ReadOnlySpan<byte> dbKey, ReadOnlySpan<byte> dbValue) => false;
+        public Task EnqueueAsync(int? topicIndex, byte[] dbKey) => Task.CompletedTask;
+        public void WaitUntilEmpty() { }
+        public Task StopAsync() => Task.CompletedTask;
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/ExecTimeStats.cs b/src/Nethermind/Nethermind.Db/LogIndex/ExecTimeStats.cs
new file mode 100644
index 0000000000..3fd122022b
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/ExecTimeStats.cs
@@ -0,0 +1,41 @@
+// SPDX-FileCopyrightText: 2024 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Threading;
+
+namespace Nethermind.Db;
+
+public class ExecTimeStats
+{
+    private long _totalTicks;
+    private int _count;
+
+    public void Include(TimeSpan elapsed)
+    {
+        Interlocked.Add(ref _totalTicks, elapsed.Ticks);
+        Interlocked.Increment(ref _count);
+    }
+
+    public TimeSpan Total => TimeSpan.FromTicks(_totalTicks);
+    public TimeSpan Average => _count == 0 ? TimeSpan.Zero : TimeSpan.FromTicks((long)((double)_totalTicks / _count));
+
+    private string Format(TimeSpan value) => value switch
+    {
+        { TotalDays: >= 1 } x => $"{x.TotalDays:F2}d",
+        { TotalHours: >= 1 } x => $"{x.TotalHours:F2}h",
+        { TotalMinutes: >= 1 } x => $"{x.TotalMinutes:F2}m",
+        { TotalSeconds: >= 1 } x => $"{x.TotalSeconds:F2}s",
+        { TotalMilliseconds: >= 1 } x => $"{x.TotalMilliseconds:F1}ms",
+        { TotalMicroseconds: >= 1 } x => $"{x.TotalMicroseconds:F1}μs",
+        var x => $"{x.TotalNanoseconds:F1}ns"
+    };
+
+    public override string ToString() => $"{Format(Average)} ({_count:N0}) [{Format(Total)}]";
+
+    public void Combine(ExecTimeStats stats)
+    {
+        _totalTicks += stats._totalTicks;
+        _count += stats._count;
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/ILogIndexStorage.cs b/src/Nethermind/Nethermind.Db/LogIndex/ILogIndexStorage.cs
new file mode 100644
index 0000000000..5f2b000ec5
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/ILogIndexStorage.cs
@@ -0,0 +1,59 @@
+// SPDX-FileCopyrightText: 2024 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Collections.Generic;
+using System.Linq;
+using System.Threading.Tasks;
+using Nethermind.Core;
+using Nethermind.Core.Crypto;
+using Nethermind.Core.ServiceStopper;
+
+namespace Nethermind.Db;
+
+public readonly record struct BlockReceipts(int BlockNumber, TxReceipt[] Receipts);
+
+public struct LogIndexAggregate(int firstBlockNum, int lastBlockNum)
+{
+    private Dictionary<Address, List<int>>? _address;
+    private Dictionary<Hash256, List<int>>[]? _topic;
+
+    public int FirstBlockNum { get; } = firstBlockNum;
+    public int LastBlockNum { get; } = lastBlockNum;
+
+    public Dictionary<Address, List<int>> Address => _address ??= new();
+
+    public Dictionary<Hash256, List<int>>[] Topic => _topic ??= Enumerable.Range(0, LogIndexStorage.MaxTopics)
+        .Select(static _ => new Dictionary<Hash256, List<int>>())
+        .ToArray();
+
+    public bool IsEmpty => (_address is null || _address.Count == 0) && (_topic is null || _topic[0].Count == 0);
+    public int TopicCount => _topic is { Length: > 0 } ? _topic.Sum(static t => t.Count) : 0;
+
+    public LogIndexAggregate(IReadOnlyList<BlockReceipts> batch) : this(batch[0].BlockNumber, batch[^1].BlockNumber) { }
+}
+
+// TODO: remove testing methods
+public interface ILogIndexStorage : IAsyncDisposable, IStoppableService
+{
+    int? GetMaxBlockNumber();
+    int? GetMinBlockNumber();
+
+    List<int> GetBlockNumbersFor(Address address, int from, int to);
+    List<int> GetBlockNumbersFor(int index, Hash256 topic, int from, int to);
+
+    Dictionary<byte[], int[]> GetKeysFor(Address address, int from, int to, bool includeValues = false);
+    Dictionary<byte[], int[]> GetKeysFor(int index, Hash256 topic, int from, int to, bool includeValues = false);
+
+    string GetDbSize();
+
+    Task FirstBlockAdded { get; }
+    Task CheckMigratedData();
+
+    LogIndexAggregate Aggregate(IReadOnlyList<BlockReceipts> batch, bool isBackwardSync, LogIndexUpdateStats? stats = null);
+    Task SetReceiptsAsync(IReadOnlyList<BlockReceipts> batch, bool isBackwardSync, LogIndexUpdateStats? stats = null);
+    Task SetReceiptsAsync(LogIndexAggregate aggregate, bool isBackwardSync, LogIndexUpdateStats? stats = null);
+    Task ReorgFrom(BlockReceipts block);
+    Task CompactAsync(bool flush, LogIndexUpdateStats? stats = null);
+    Task RecompactAsync(int maxUncompressedLength = -1, LogIndexUpdateStats? stats = null);
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/LogIndexColumns.cs b/src/Nethermind/Nethermind.Db/LogIndex/LogIndexColumns.cs
new file mode 100644
index 0000000000..62fdfba580
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/LogIndexColumns.cs
@@ -0,0 +1,13 @@
+// SPDX-FileCopyrightText: 2024 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+namespace Nethermind.Db;
+
+public enum LogIndexColumns
+{
+    Addresses,
+    Topics0,
+    Topics1,
+    Topics2,
+    Topics3
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/LogIndexStorage.cs b/src/Nethermind/Nethermind.Db/LogIndex/LogIndexStorage.cs
new file mode 100644
index 0000000000..6b17ff7e82
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/LogIndexStorage.cs
@@ -0,0 +1,1027 @@
+using System;
+using System.Buffers;
+using System.Buffers.Binary;
+using System.Collections.Generic;
+using System.ComponentModel.DataAnnotations;
+using System.Diagnostics;
+using System.Diagnostics.CodeAnalysis;
+using System.Linq;
+using System.Runtime.InteropServices;
+using System.Threading;
+using System.Threading.Tasks;
+using Nethermind.Core;
+using Nethermind.Core.Collections;
+using Nethermind.Core.Crypto;
+using Nethermind.Core.Extensions;
+using Nethermind.Logging;
+
+namespace Nethermind.Db
+{
+    // TODO: get rid of InvalidOperationExceptions - these are for state validation
+    // TODO: verify all MemoryMarshal usages - needs to be CPU-cross-compatible
+    [SuppressMessage("ReSharper", "PrivateFieldCanBeConvertedToLocalVariable")] // TODO: get rid of unused fields
+    public sealed partial class LogIndexStorage : ILogIndexStorage
+    {
+        private static class SpecialKey
+        {
+            // Use values that we won't encounter during iterator Seek or SeekForPrev
+            public static readonly byte[] MinBlockNum = Enumerable.Repeat(byte.MaxValue, MaxDbKeyLength)
+                .Concat(new byte[] { 1 }).ToArray();
+
+            // Use values that we won't encounter during iterator Seek or SeekForPrev
+            public static readonly byte[] MaxBlockNum = Enumerable.Repeat(byte.MaxValue, MaxDbKeyLength)
+                .Concat(new byte[] { 2 }).ToArray();
+        }
+
+        private static class SpecialPostfix
+        {
+            // Any ordered prefix seeking will start on it
+            public static readonly byte[] BackwardMerge = Enumerable.Repeat((byte)0, BlockNumSize).ToArray();
+
+            // Any ordered prefix seeking will end on it.
+            public static readonly byte[] ForwardMerge = Enumerable.Repeat(byte.MaxValue, BlockNumSize).ToArray();
+        }
+
+        private static class Defaults
+        {
+            public const int IOParallelism = 1;
+            public const int MaxReorgDepth = 64;
+        }
+
+        public const int MaxTopics = 4;
+
+        private const int MaxKeyLength = Hash256.Size + 1; // Math.Max(Address.Size, Hash256.Size)
+        private const int MaxDbKeyLength = MaxKeyLength + BlockNumSize;
+
+        // TODO: consider using ArrayPoolList just for `using` syntax
+        private static readonly ArrayPool<byte> _arrayPool = ArrayPool<byte>.Shared;
+
+        private readonly IColumnsDb<LogIndexColumns> _columnsDb;
+        private readonly IDb _addressDb;
+        private readonly IDb[] _topicsDbs;
+        private readonly ILogger _logger;
+
+        private const int BlockNumSize = sizeof(int);
+
+        private readonly int _maxReorgDepth;
+
+        private readonly Dictionary<LogIndexColumns, MergeOperator> _mergeOperators;
+        private readonly ICompressor _compressor;
+        private readonly ICompactor _compactor;
+
+        private readonly Lock _rangeInitLock = new(); // May not be needed, but added for safety
+        private int? _addressMaxBlock;
+        private int? _addressMinBlock;
+        private int?[] _topicMinBlocks;
+        private int?[] _topicMaxBlocks;
+
+        /// <summary>
+        /// Whether a first batch was already added.
+        /// </summary>
+        private bool WasInitialized => _addressMinBlock is not null; // TODO: check other metadata values?
+
+        private readonly TaskCompletionSource _firstBlockAddedSource = new();
+        public Task FirstBlockAdded => _firstBlockAddedSource.Task;
+
+        /// <summary>
+        /// Guarantees initialization won't be run concurrently
+        /// </summary>
+        private readonly SemaphoreSlim _initSemaphore = new(1, 1);
+
+        /// <summary>
+        /// Maps a syncing direction to semaphore.
+        /// Used for blocking concurrent executions and
+        /// ensuring the current iteration is completed before stopping/disposing.
+        /// </summary>
+        private readonly Dictionary<bool, SemaphoreSlim> _setReceiptsSemaphores = new()
+        {
+            { false, new(1, 1) },
+            { true, new(1, 1) }
+        };
+
+        // Not thread safe
+        private bool _stopped;
+        private bool _disposed;
+
+        public LogIndexStorage(IDbFactory dbFactory, ILogManager logManager,
+            int? ioParallelism = null, int? compactionDistance = null, int? maxReorgDepth = null)
+        {
+            if (maxReorgDepth < 0) throw new ArgumentException("Compaction distance must be a positive value.", nameof(compactionDistance));
+            _maxReorgDepth = maxReorgDepth ?? Defaults.MaxReorgDepth;
+
+            _logger = logManager.GetClassLogger<LogIndexStorage>();
+            _compressor = new Compressor(this, _logger, ioParallelism ?? Defaults.IOParallelism);
+            _compactor = compactionDistance.HasValue ? new Compactor(this, _logger, compactionDistance.Value) : new NoOpCompactor();
+
+            _mergeOperators = new()
+            {
+                { LogIndexColumns.Addresses, new(this, _compressor, topicIndex: null) },
+                { LogIndexColumns.Topics0, new(this, _compressor, topicIndex: 0) },
+                { LogIndexColumns.Topics1, new(this, _compressor, topicIndex: 1) },
+                { LogIndexColumns.Topics2, new(this, _compressor, topicIndex: 2) },
+                { LogIndexColumns.Topics3, new(this, _compressor, topicIndex: 3) }
+            };
+
+            _columnsDb = dbFactory.CreateColumnsDb<LogIndexColumns>(new("logIndexStorage", DbNames.LogIndex)
+            {
+                MergeOperatorByColumn = _mergeOperators.ToDictionary(x => $"{x.Key}", x => (IMergeOperator)x.Value)
+            });
+            _addressDb = _columnsDb.GetColumnDb(LogIndexColumns.Addresses);
+            _topicsDbs = _mergeOperators.Keys.Where(cl => $"{cl}".Contains("Topic")).Select(cl => _columnsDb.GetColumnDb(cl)).ToArray();
+
+            _addressMaxBlock = LoadRangeBound(_addressDb, SpecialKey.MaxBlockNum);
+            _addressMinBlock = LoadRangeBound(_addressDb, SpecialKey.MinBlockNum);
+            _topicMaxBlocks = _topicsDbs.Select(static db => LoadRangeBound(db, SpecialKey.MaxBlockNum)).ToArray();
+            _topicMinBlocks = _topicsDbs.Select(static db => LoadRangeBound(db, SpecialKey.MinBlockNum)).ToArray();
+
+            if (WasInitialized)
+                _firstBlockAddedSource.SetResult();
+        }
+
+        // TODO: remove if unused
+        private static IEnumerable<(byte[] key, byte[] value)> Enumerate(IIterator iterator)
+        {
+            iterator.SeekToFirst();
+            while (iterator.Valid())
+            {
+                yield return (iterator.Key().ToArray(), iterator.Value().ToArray());
+                iterator.Next();
+            }
+        }
+
+        public async Task StopAsync()
+        {
+            if (_stopped)
+                return;
+
+            await _setReceiptsSemaphores[false].WaitAsync();
+            await _setReceiptsSemaphores[true].WaitAsync();
+
+            try
+            {
+                if (_stopped)
+                    return;
+
+                await _compactor.StopAsync(); // Need to wait, as releasing RocksDB during compaction will cause 0xC0000005
+                await _compressor.StopAsync(); // TODO: consider not waiting for compression queue to finish
+
+                // TODO: check if needed
+                _addressDb.Flush();
+                _topicsDbs.ForEach(static db => db.Flush());
+
+                if (_logger.IsInfo) _logger.Info("Log index storage stopped");
+            }
+            finally
+            {
+                _stopped = true;
+
+                _setReceiptsSemaphores[false].Release();
+                _setReceiptsSemaphores[true].Release();
+            }
+        }
+
+        async ValueTask IAsyncDisposable.DisposeAsync()
+        {
+            if (_disposed)
+                return;
+
+            await StopAsync();
+
+            _setReceiptsSemaphores[false].Dispose();
+            _setReceiptsSemaphores[true].Dispose();
+            _columnsDb.Dispose();
+            _addressDb.Dispose();
+            _topicsDbs.ForEach(static db => db.Dispose());
+
+            _disposed = true;
+        }
+
+        private static int? LoadRangeBound(IDb db, byte[] key)
+        {
+            var value = db.Get(key);
+            return value is { Length: > 1 } ? GetValBlockNum(value) : null;
+        }
+
+        private void UpdateRange((int min, int max) addressRange, (int?[] min, int?[] max) topicRanges, bool isBackwardSync)
+        {
+            if (!WasInitialized)
+            {
+                using Lock.Scope _ = _rangeInitLock.EnterScope();
+                (_addressMinBlock, _addressMaxBlock) = addressRange;
+                (_topicMinBlocks, _topicMaxBlocks) = topicRanges;
+                return;
+            }
+
+            if (isBackwardSync)
+                (_addressMinBlock, _topicMinBlocks) = (addressRange.min, topicRanges.min);
+            else
+                (_addressMaxBlock, _topicMaxBlocks) = (addressRange.max, topicRanges.max);
+        }
+
+        private static int SaveRangeBound(IWriteOnlyKeyValueStore dbBatch, byte[] key, int value)
+        {
+            var bufferArr = _arrayPool.Rent(BlockNumSize);
+            Span<byte> buffer = bufferArr.AsSpan(BlockNumSize);
+
+            try
+            {
+                SetValBlockNum(buffer, value);
+                dbBatch.PutSpan(key, buffer);
+                return value;
+            }
+            finally
+            {
+                _arrayPool.Return(bufferArr);
+            }
+        }
+
+        private static (int min, int max) SaveRange(IWriteOnlyKeyValueStore dbBatch, int batchFirst, int batchLast,
+            int? lastMin, int? lastMax, bool isBackwardSync, bool isReorg)
+        {
+            var batchMin = Math.Min(batchFirst, batchLast);
+            var batchMax = Math.Max(batchFirst, batchLast);
+
+            var min = lastMin ?? SaveRangeBound(dbBatch, SpecialKey.MinBlockNum, batchMin);
+            var max = lastMax ??= SaveRangeBound(dbBatch, SpecialKey.MaxBlockNum, batchMax);
+
+            if (!isBackwardSync)
+            {
+                if ((isReorg && batchMax < lastMax) || (!isReorg && batchMax > lastMax))
+                    max = SaveRangeBound(dbBatch, SpecialKey.MaxBlockNum, batchMax);
+            }
+            else
+            {
+                if (isReorg)
+                    throw ValidationException("Backwards sync does not support reorgs.");
+                if (batchMin < lastMin)
+                    min = SaveRangeBound(dbBatch, SpecialKey.MinBlockNum, batchMin);
+            }
+
+            return (min, max);
+        }
+
+        private (int min, int max) SaveAddressRange(IWriteBatch dbBatch, LogIndexAggregate aggregate, bool isBackwardSync, bool isReorg = false) =>
+            SaveRange(dbBatch, aggregate.FirstBlockNum, aggregate.LastBlockNum, _addressMinBlock, _addressMaxBlock, isBackwardSync, isReorg);
+
+        private (int min, int max) SaveAddressRange(IWriteBatch dbBatch, int block, bool isBackwardSync, bool isReorg = false) =>
+            SaveRange(dbBatch, block, block, _addressMinBlock, _addressMaxBlock, isBackwardSync, isReorg);
+
+        private (int min, int max) SaveTopicBlockNumbers(int topicIndex, IWriteBatch dbBatch, LogIndexAggregate aggregate, bool isBackwardSync, bool isReorg = false) =>
+            SaveRange(dbBatch, aggregate.FirstBlockNum, aggregate.LastBlockNum, _topicMinBlocks[topicIndex], _topicMaxBlocks[topicIndex], isBackwardSync, isReorg);
+
+        private (int min, int max) SaveTopicBlockNumbers(int topicIndex, IWriteBatch dbBatch, int block, bool isBackwardSync, bool isReorg = false) =>
+            SaveRange(dbBatch, block, block, _topicMinBlocks[topicIndex], _topicMaxBlocks[topicIndex], isBackwardSync, isReorg);
+
+        private int GetLastReorgableBlockNumber() => Math.Min(_addressMaxBlock ?? 0, _topicMaxBlocks.Min() ?? 0) - _maxReorgDepth;
+
+        private static bool IsBlockNewer(int next, int? lastMin, int? lastMax, bool isBackwardSync) => isBackwardSync
+            ? lastMin is null || next < lastMin
+            : lastMax is null || next > lastMax;
+
+        private bool IsAddressBlockNewer(int next, bool isBackwardSync) => IsBlockNewer(next, _addressMinBlock, _addressMaxBlock, isBackwardSync);
+        private bool IsTopicBlockNewer(int topicIndex, int next, bool isBackwardSync) => IsBlockNewer(next, _topicMinBlocks[topicIndex], _topicMaxBlocks[topicIndex], isBackwardSync);
+
+        private bool IsBlockNewer(int next, bool isBackwardSync) =>
+            IsAddressBlockNewer(next, isBackwardSync) ||
+            IsTopicBlockNewer(0, next, isBackwardSync) ||
+            IsTopicBlockNewer(1, next, isBackwardSync) ||
+            IsTopicBlockNewer(2, next, isBackwardSync) ||
+            IsTopicBlockNewer(3, next, isBackwardSync);
+
+        public int? GetMaxBlockNumber() => _addressMaxBlock is { } addressMaxBlock && _topicMaxBlocks.Min() is { } topicMaxBlock
+            ? Math.Min(addressMaxBlock, topicMaxBlock)
+            : null;
+
+        public int? GetMinBlockNumber() => _addressMinBlock is { } addressMinBlock && _topicMinBlocks.Max() is { } topicMinBlock
+            ? Math.Max(addressMinBlock, topicMinBlock)
+            : null;
+
+        public string GetDbSize()
+        {
+            return FormatSize(_columnsDb.GatherMetric().Size);
+        }
+
+        public Dictionary<byte[], int[]> GetKeysFor(Address address, int from, int to, bool includeValues = false) =>
+            GetKeysFor(null, address.Bytes, from, to, includeValues);
+
+        public Dictionary<byte[], int[]> GetKeysFor(int index, Hash256 topic, int from, int to, bool includeValues = false) =>
+            GetKeysFor(index, topic.Bytes.ToArray(), from, to, includeValues);
+
+        [SuppressMessage("ReSharper", "AccessToDisposedClosure")]
+        private Dictionary<byte[], int[]> GetKeysFor(int? topicIndex, byte[] key, int from, int to, bool includeValues = false)
+        {
+            var result = new Dictionary<byte[], int[]>(Bytes.EqualityComparer);
+            using var buffer = new ArrayPoolList<int>(includeValues ? 128 : 0);
+
+            IterateBlockNumbersFor(topicIndex, key, from, to, iterator =>
+            {
+                var iteratorKey = iterator.Key().ToArray();
+                var value = iterator.Value().ToArray();
+                foreach (var block in EnumerateBlockNumbers(value, from))
+                {
+                    if (block > to)
+                    {
+                        result.Add(iteratorKey, buffer.AsSpan().ToArray());
+                        return false;
+                    }
+
+                    if (includeValues)
+                        buffer.Add(block);
+                }
+
+                result.Add(iteratorKey, buffer.AsSpan().ToArray());
+                buffer.Clear();
+
+                return true;
+            });
+
+            return result;
+        }
+
+        public List<int> GetBlockNumbersFor(Address address, int from, int to)
+        {
+            return GetBlockNumbersFor(null, address.Bytes, from, to);
+        }
+
+        public List<int> GetBlockNumbersFor(int index, Hash256 topic, int from, int to)
+        {
+            return GetBlockNumbersFor(index, topic.Bytes.ToArray(), from, to);
+        }
+
+        private List<int> GetBlockNumbersFor(int? topicIndex, byte[] key, int from, int to)
+        {
+            // TODO: use ArrayPoolList?
+            var result = new List<int>(128);
+
+            IterateBlockNumbersFor(topicIndex, key, from, to, iterator =>
+            {
+                var value = iterator.Value().ToArray();
+                foreach (var block in EnumerateBlockNumbers(value, from))
+                {
+                    if (block > to)
+                        return false;
+
+                    result.Add(block);
+                }
+
+                return true;
+            });
+
+            return result;
+        }
+
+        private void IterateBlockNumbersFor(
+            int? topicIndex, byte[] key, int from, int to,
+            Func<IIterator, bool> callback
+        )
+        {
+            var timestamp = Stopwatch.GetTimestamp();
+            byte[] dbKeyBuffer = null;
+
+            try
+            {
+                // Adjust parameters to avoid composing invalid lookup keys
+                if (from < 0) from = 0;
+                if (to < from) return;
+
+                dbKeyBuffer = _arrayPool.Rent(MaxDbKeyLength);
+                ReadOnlySpan<byte> dbKey = CreateDbKey(key, from, dbKeyBuffer);
+                ReadOnlySpan<byte> normalizedKey = ExtractKey(dbKey);
+
+                IDb? db = GetDb(topicIndex);
+                using IIterator iterator = db.GetIterator(true); // TODO: specify lower/upper bounds?
+
+                // Find the last index for the given key, starting at or before `from`
+                iterator.SeekForPrev(dbKey);
+
+                // Otherwise, find the first index for the given key
+                // TODO: achieve in a single seek?
+                if (!IsInKeyBounds(iterator, normalizedKey))
+                {
+                    iterator.SeekToFirst();
+                    iterator.Seek(key);
+                }
+
+                while (IsInKeyBounds(iterator, normalizedKey))
+                {
+                    if (!callback(iterator))
+                        return;
+
+                    iterator.Next();
+                }
+            }
+            finally
+            {
+                if (dbKeyBuffer != null) _arrayPool.Return(dbKeyBuffer);
+
+                if (_logger.IsTrace) _logger.Trace($"{nameof(IterateBlockNumbersFor)}({Convert.ToHexString(key)}, {from}, {to}) in {Stopwatch.GetElapsedTime(timestamp)}");
+            }
+        }
+
+        private static bool IsInKeyBounds(IIterator iterator, ReadOnlySpan<byte> key)
+        {
+            return iterator.Valid() && ExtractKey(iterator.Key()).SequenceEqual(key);
+        }
+
+        private static IEnumerable<int> EnumerateBlockNumbers(byte[] data, int from)
+        {
+            if (data.Length == 0)
+                yield break;
+
+            var blockNums = data.Length == 0 || !IsCompressed(data, out _)
+                ? ReadBlockNums(data)
+                : DecompressDbValue(data);
+
+            ReverseBlocksIfNeeded(blockNums);
+
+            int startIndex = BinarySearch(blockNums, from);
+            if (startIndex < 0)
+            {
+                startIndex = ~startIndex;
+            }
+
+            for (int i = startIndex; i < blockNums.Length; i++)
+                yield return blockNums[i];
+        }
+
+        // TODO: optimize allocations
+        public LogIndexAggregate Aggregate(IReadOnlyList<BlockReceipts> batch, bool isBackwardSync, LogIndexUpdateStats? stats)
+        {
+            if (!IsBlockNewer(batch[^1].BlockNumber, isBackwardSync))
+                return new(batch);
+
+            var timestamp = Stopwatch.GetTimestamp();
+
+            var aggregate = new LogIndexAggregate(batch);
+            foreach ((var blockNumber, TxReceipt[] receipts) in batch)
+            {
+                if (!IsBlockNewer(blockNumber, isBackwardSync))
+                    continue;
+
+                stats?.IncrementBlocks();
+
+                foreach (TxReceipt receipt in receipts)
+                {
+                    stats?.IncrementTx();
+
+                    if (receipt.Logs == null)
+                        continue;
+
+                    foreach (LogEntry log in receipt.Logs)
+                    {
+                        stats?.IncrementLogs();
+
+                        if (IsAddressBlockNewer(blockNumber, isBackwardSync))
+                        {
+                            List<int> addressNums = aggregate.Address.GetOrAdd(log.Address, static _ => new(1));
+
+                            if (addressNums.Count == 0 || addressNums[^1] != blockNumber)
+                                addressNums.Add(blockNumber);
+                        }
+
+                        for (byte topicIndex = 0; topicIndex < log.Topics.Length; topicIndex++)
+                        {
+                            if (IsTopicBlockNewer(topicIndex, blockNumber, isBackwardSync))
+                            {
+
+                                stats?.IncrementTopics();
+
+                                var topicNums = aggregate.Topic[topicIndex].GetOrAdd(log.Topics[topicIndex], static _ => new(1));
+
+                                if (topicNums.Count == 0 || topicNums[^1] != blockNumber)
+                                    topicNums.Add(blockNumber);
+                            }
+                        }
+                    }
+                }
+            }
+
+            stats?.KeysCount.Include(aggregate.Address.Count + aggregate.TopicCount);
+            stats?.Aggregating.Include(Stopwatch.GetElapsedTime(timestamp));
+
+            return aggregate;
+        }
+
+        public Task CheckMigratedData()
+        {
+            // using IIterator<byte[], byte[]> addressIterator = _addressDb.GetIterator();
+            // using IIterator<byte[], byte[]> topicIterator = _topicsDb.GetIterator();
+            //
+            // // Total: 9244, finalized - 31
+            // (Address, IndexInfo)[] addressData = Enumerate(addressIterator).Select(x => (new Address(SplitDbKey(x.key).key), IndexInfo.Deserialize(x.key, x.value))).ToArray();
+            //
+            // // Total: 5_654_366
+            // // From first 200_000: 1 - 134_083 (0.670415), 2 - 10_486, 3 - 33_551, 4 - 4872, 5 - 4227, 6 - 4764, 7 - 6792, 8 - 609, 9 - 67, 10 - 55
+            // // From first 300_000: 1 - 228_553 (0.761843333)
+            // // From first 1_000_000: 1 - 875_216 (0.875216)
+            // //var topicData = Enumerate(topicIterator).Select(x => (new Hash256(SplitDbKey(x.key).key), DeserializeIndexInfo(x.key, x.value))).ToArray();
+            // var topicData = Enumerate(topicIterator).Take(200_000).Select(x => (topic: new Hash256(SplitDbKey(x.key).key), Index: IndexInfo.Deserialize(x.key, x.value))).GroupBy(x => x.Index.TotalValuesCount).ToDictionary(g => g.Key, g => g.Count());
+            //
+            // GC.KeepAlive(addressData);
+            // GC.KeepAlive(topicData);
+
+            return Task.CompletedTask;
+        }
+
+        private static async ValueTask LockRunAsync(SemaphoreSlim semaphore)
+        {
+            if (!await semaphore.WaitAsync(TimeSpan.Zero, CancellationToken.None))
+                throw new InvalidOperationException($"{nameof(LogIndexStorage)} does not support concurrent invocations in the same direction.");
+        }
+
+        public async Task ReorgFrom(BlockReceipts block)
+        {
+            if (!WasInitialized)
+                return;
+
+            const bool isBackwardSync = false;
+
+            SemaphoreSlim semaphore = _setReceiptsSemaphores[isBackwardSync];
+            await LockRunAsync(semaphore);
+
+            byte[]? keyArray = null, valueArray = null;
+
+            try
+            {
+                keyArray = _arrayPool.Rent(MaxDbKeyLength);
+                valueArray = _arrayPool.Rent(BlockNumSize + 1);
+
+                IWriteBatch addressBatch = _addressDb.StartWriteBatch();
+                IWriteBatch[] topicBatches = _topicsDbs.Select(static db => db.StartWriteBatch()).ToArray();
+
+                Span<byte> dbValue = MergeOps.Create(MergeOp.ReorgOp, block.BlockNumber, valueArray);
+
+                foreach (TxReceipt receipt in block.Receipts)
+                {
+                    foreach (LogEntry log in receipt.Logs ?? [])
+                    {
+                        ReadOnlySpan<byte> addressKey = CreateMergeDbKey(log.Address.Bytes, keyArray, isBackwardSync: false);
+                        addressBatch.Merge(addressKey, dbValue);
+
+                        for (var topicIndex = 0; topicIndex < log.Topics.Length; topicIndex++)
+                        {
+                            Hash256 topic = log.Topics[topicIndex];
+                            ReadOnlySpan<byte> topicKey = CreateMergeDbKey(topic.Bytes, keyArray, isBackwardSync: false);
+                            topicBatches[topicIndex].Merge(topicKey, dbValue);
+                        }
+                    }
+                }
+
+                // Need to update last block number, so that new-receipts comparison won't fail when rewriting it
+                // TODO: figure out if this can be improved, maybe don't use comparison checks at all
+                var blockNum = block.BlockNumber - 1;
+
+                (int min, int max) addressRange =
+                    SaveAddressRange(addressBatch, blockNum, isBackwardSync, isReorg: true);
+
+                (int?[] min, int?[] max) topicRanges = (min: _topicMinBlocks.ToArray(), max: _topicMaxBlocks.ToArray());
+                for (var topicIndex = 0; topicIndex < topicBatches.Length; topicIndex++)
+                {
+                    IWriteBatch topicBatch = topicBatches[topicIndex];
+
+                    (topicRanges.min[topicIndex], topicRanges.max[topicIndex]) =
+                        SaveTopicBlockNumbers(topicIndex, topicBatch, blockNum, isBackwardSync: false, isReorg: true);
+                }
+
+                addressBatch.Dispose();
+                topicBatches.ForEach(static b => b.Dispose());
+
+                UpdateRange(addressRange, topicRanges, isBackwardSync);
+            }
+            finally
+            {
+                semaphore.Release();
+
+                if (keyArray is not null) _arrayPool.Return(keyArray);
+                if (valueArray is not null) _arrayPool.Return(valueArray);
+            }
+        }
+
+        public async Task CompactAsync(bool flush, LogIndexUpdateStats? stats = null)
+        {
+            // TODO: include time to stats
+            if (flush)
+            {
+                _addressDb.Flush();
+                _topicsDbs.ForEach(static db => db.Flush());
+            }
+
+            CompactingStats compactStats = await _compactor.ForceAsync();
+            stats?.Compacting.Combine(compactStats);
+        }
+
+        public async Task RecompactAsync(int minLengthToCompress = -1, LogIndexUpdateStats? stats = null)
+        {
+            if (minLengthToCompress < 0)
+                minLengthToCompress = Compressor.MinLengthToCompress;
+
+            await CompactAsync(flush: true, stats);
+
+            var timestamp = Stopwatch.GetTimestamp();
+            var addressCount = await QueueLargeKeysCompression(topicIndex: null, minLengthToCompress);
+            stats?.QueueingAddressCompression.Include(Stopwatch.GetElapsedTime(timestamp));
+
+            timestamp = Stopwatch.GetTimestamp();
+            var topicCount = 0;
+            for (var topicIndex = 0; topicIndex < _topicsDbs.Length; topicIndex++)
+                topicCount += await QueueLargeKeysCompression(topicIndex, minLengthToCompress);
+            stats?.QueueingTopicCompression.Include(Stopwatch.GetElapsedTime(timestamp));
+
+            _logger.Info($"Queued keys for compaction: {addressCount:N0} address, {topicCount:N0} topic");
+
+            _compressor.WaitUntilEmpty();
+            await CompactAsync(flush: true, stats);
+        }
+
+        private async Task<int> QueueLargeKeysCompression(int? topicIndex, int minLengthToCompress)
+        {
+            var counter = 0;
+
+            IDb db = GetDb(topicIndex);
+            using var addressIterator = db.GetIterator();
+            foreach (var (key, value) in Enumerate(addressIterator))
+            {
+                if (IsCompressed(value) || value.Length < minLengthToCompress)
+                    continue;
+
+                await _compressor.EnqueueAsync(topicIndex, key);
+
+                counter++;
+            }
+
+            return counter;
+        }
+
+        public async Task SetReceiptsAsync(LogIndexAggregate aggregate, bool isBackwardSync, LogIndexUpdateStats? stats = null)
+        {
+            long totalTimestamp = Stopwatch.GetTimestamp();
+
+            SemaphoreSlim semaphore = _setReceiptsSemaphores[isBackwardSync];
+            await LockRunAsync(semaphore);
+
+            var wasInitialized = WasInitialized;
+            if (!wasInitialized)
+                await _initSemaphore.WaitAsync();
+
+            try
+            {
+                IWriteBatch addressBatch = _addressDb.StartWriteBatch();
+                IWriteBatch[] topicBatches = _topicsDbs.Select(static db => db.StartWriteBatch()).ToArray();
+
+                // Add values to batches
+                long timestamp;
+                if (!aggregate.IsEmpty)
+                {
+                    timestamp = Stopwatch.GetTimestamp();
+
+                    // Add addresses
+                    foreach (var (address, blockNums) in aggregate.Address)
+                    {
+                        SaveBlockNumbersByKey(addressBatch, address.Bytes, blockNums, isBackwardSync, stats);
+                    }
+
+                    // Add topics
+                    for (var topicIndex = 0; topicIndex < aggregate.Topic.Length; topicIndex++)
+                    {
+                        var topics = aggregate.Topic[topicIndex];
+
+                        foreach (var (topic, blockNums) in topics)
+                            SaveBlockNumbersByKey(topicBatches[topicIndex], topic.Bytes, blockNums, isBackwardSync, stats);
+                    }
+
+                    stats?.Processing.Include(Stopwatch.GetElapsedTime(timestamp));
+                }
+
+                timestamp = Stopwatch.GetTimestamp();
+
+                // Update ranges in DB
+                (int min, int max) addressRange = SaveAddressRange(addressBatch, aggregate, isBackwardSync);
+
+                (int?[] min, int?[] max) topicRanges = (min: _topicMinBlocks.ToArray(), max: _topicMaxBlocks.ToArray());
+                for (var topicIndex = 0; topicIndex < topicBatches.Length; topicIndex++)
+                {
+                    IWriteBatch topicBatch = topicBatches[topicIndex];
+
+                    (topicRanges.min[topicIndex], topicRanges.max[topicIndex]) =
+                        SaveTopicBlockNumbers(topicIndex, topicBatch, aggregate, isBackwardSync);
+                }
+
+                stats?.UpdatingMeta.Include(Stopwatch.GetElapsedTime(timestamp));
+
+                // Notify we have the first block
+                _firstBlockAddedSource.TrySetResult();
+
+                // Submit batches
+                // TODO: return batches in case of an error without writing anything
+                timestamp = Stopwatch.GetTimestamp();
+                addressBatch.Dispose();
+                topicBatches.ForEach(static b => b.Dispose());
+                stats?.WaitingBatch.Include(Stopwatch.GetElapsedTime(timestamp));
+
+                // Update ranges in memory
+                UpdateRange(addressRange, topicRanges, isBackwardSync);
+
+                // Enqueue compaction if needed
+                _compactor.TryEnqueue();
+            }
+            finally
+            {
+                if (!wasInitialized)
+                    _initSemaphore.Release();
+
+                semaphore.Release();
+            }
+
+            foreach (MergeOperator mergeOperator in _mergeOperators.Values)
+                stats?.Combine(mergeOperator.GetAndResetStats());
+            stats?.PostMergeProcessing.Combine(_compressor.GetAndResetStats());
+            stats?.Compacting.Combine(_compactor.GetAndResetStats());
+            stats?.SetReceipts.Include(Stopwatch.GetElapsedTime(totalTimestamp));
+        }
+
+        // batch is expected to be sorted, TODO: validate this is the case
+        public Task SetReceiptsAsync(IReadOnlyList<BlockReceipts> batch, bool isBackwardSync, LogIndexUpdateStats? stats = null)
+        {
+            LogIndexAggregate aggregate = Aggregate(batch, isBackwardSync, stats);
+            return SetReceiptsAsync(aggregate, isBackwardSync, stats);
+        }
+
+        // TODO: optimize allocations
+        private static void SaveBlockNumbersByKey(
+            IWriteBatch dbBatch, ReadOnlySpan<byte> key, IReadOnlyList<int> blockNums,
+            bool isBackwardSync, LogIndexUpdateStats? stats
+        )
+        {
+            var dbKeyArray = _arrayPool.Rent(MaxDbKeyLength);
+
+            try
+            {
+                ReadOnlySpan<byte> dbKey = CreateMergeDbKey(key, dbKeyArray, isBackwardSync);
+
+                // TODO: handle writing already processed blocks
+                // if (blockNums[^1] <= lastSavedNum)
+                //     return;
+
+                var newValue = CreateDbValue(blockNums);
+
+                var timestamp = Stopwatch.GetTimestamp();
+
+                if (newValue is null or [])
+                    throw ValidationException($"No block numbers to save for {Convert.ToHexString(key)}.");
+
+                dbBatch.Merge(dbKey, newValue);
+                stats?.CallingMerge.Include(Stopwatch.GetElapsedTime(timestamp));
+            }
+            finally
+            {
+                _arrayPool.Return(dbKeyArray);
+            }
+        }
+
+        private static ReadOnlySpan<byte> WriteKey(ReadOnlySpan<byte> key, Span<byte> buffer)
+        {
+            //ReadOnlySpan<byte> normalized = key.WithoutLeadingZeros();
+            //normalized = normalized.Length > 0 ? normalized : ZeroArray;
+
+            key.CopyTo(buffer);
+            return buffer[..key.Length];
+        }
+
+        private static ReadOnlySpan<byte> ExtractKey(ReadOnlySpan<byte> dbKey) => dbKey[..^BlockNumSize];
+
+        /// <summary>
+        /// Generates a key consisting of the <c>key || block-number</c> byte array.
+        /// </summary>
+        private static ReadOnlySpan<byte> CreateDbKey(ReadOnlySpan<byte> key, int blockNumber, Span<byte> buffer)
+        {
+            key = WriteKey(key, buffer);
+            SetKeyBlockNum(buffer[key.Length..], blockNumber);
+
+            var length = key.Length + BlockNumSize;
+            return buffer[..length];
+        }
+
+        private static ReadOnlySpan<byte> CreateMergeDbKey(ReadOnlySpan<byte> key, Span<byte> buffer, bool isBackwardSync)
+        {
+            key = WriteKey(key, buffer);
+            var postfix = isBackwardSync ? SpecialPostfix.BackwardMerge : SpecialPostfix.ForwardMerge;
+            postfix.CopyTo(buffer[key.Length..]);
+
+            var length = key.Length + postfix.Length;
+            return buffer[..length];
+        }
+
+        // RocksDB uses big-endian (lexicographic) ordering
+        // +1 is needed as 0 is used for the backward-merge key
+        private static int GetKeyBlockNum(ReadOnlySpan<byte> dbKey) => BinaryPrimitives.ReadInt32BigEndian(dbKey[^BlockNumSize..]) - 1;
+        private static void SetKeyBlockNum(Span<byte> dbKeyEnd, int blockNumber) => BinaryPrimitives.WriteInt32BigEndian(dbKeyEnd, blockNumber + 1);
+
+        private static bool UseBackwardSyncFor(ReadOnlySpan<byte> dbKey) => dbKey.EndsWith(SpecialPostfix.BackwardMerge);
+
+        private static int BinarySearch(ReadOnlySpan<int> blocks, int from)
+        {
+            int index = blocks.BinarySearch(from);
+            return index < 0 ? ~index : index;
+        }
+
+        private static unsafe ReadOnlySpan<int> Decompress(ReadOnlySpan<byte> data, ReadOnlySpan<int> decompressedBlockNumbers)
+        {
+            fixed (byte* dataPtr = data)
+            fixed (int* decompressedPtr = decompressedBlockNumbers)
+                _ = TurboPFor.p4nd1dec256v32(dataPtr, (nuint)decompressedBlockNumbers.Length, decompressedPtr);
+
+            return decompressedBlockNumbers;
+        }
+
+        // TODO: test on big-endian system?
+        private static unsafe ReadOnlySpan<byte> Compress(Span<byte> data, Span<byte> buffer)
+        {
+            int length;
+            ReadOnlySpan<int> blockNumbers = MemoryMarshal.Cast<byte, int>(data);
+
+            fixed (int* blockNumbersPtr = blockNumbers)
+            fixed (byte* compressedPtr = buffer)
+            {
+                // TODO: test different deltas and block sizes
+                length = (int)TurboPFor.p4nd1enc256v32(blockNumbersPtr, (nuint)blockNumbers.Length, compressedPtr);
+            }
+
+            return buffer[..length];
+        }
+
+        // used for data validation, TODO: introduce custom exception type
+        // TODO: include key value when available
+        private static Exception ValidationException(string message) => new InvalidOperationException(message);
+
+        public static int ReadCompressionMarker(ReadOnlySpan<byte> source) => -BinaryPrimitives.ReadInt32LittleEndian(source);
+        public static void WriteCompressionMarker(Span<byte> source, int len) => BinaryPrimitives.WriteInt32LittleEndian(source, -len);
+
+        public static bool IsCompressed(ReadOnlySpan<byte> source) => IsCompressed(source, out _);
+        public static bool IsCompressed(ReadOnlySpan<byte> source, out int len)
+        {
+            len = ReadCompressionMarker(source);
+            return len > 0;
+        }
+
+        public static void SetValBlockNum(Span<byte> destination, int blockNum) => BinaryPrimitives.WriteInt32LittleEndian(destination, blockNum);
+        public static int GetValBlockNum(ReadOnlySpan<byte> source) => BinaryPrimitives.ReadInt32LittleEndian(source);
+        public static int GetValLastBlockNum(ReadOnlySpan<byte> source) => GetValBlockNum(source[^BlockNumSize..]);
+
+        public static void SetValBlockNums(Span<byte> destination, IEnumerable<int> blockNums)
+        {
+            var shift = 0;
+            foreach (var blockNum in blockNums)
+            {
+                SetValBlockNum(destination[shift..], blockNum);
+                shift += BlockNumSize;
+            }
+        }
+
+        public static int[] ReadBlockNums(ReadOnlySpan<byte> source)
+        {
+            if (source.Length % 4 != 0)
+                throw ValidationException("Invalid length for array of block numbers.");
+
+            var result = new int[source.Length / BlockNumSize];
+            for (var i = 0; i < source.Length; i += BlockNumSize)
+                result[i / BlockNumSize] = GetValBlockNum(source[i..]);
+
+            return result;
+        }
+
+        private static byte[] CreateDbValue(IReadOnlyList<int> blockNums)
+        {
+            var value = new byte[blockNums.Count * BlockNumSize];
+            SetValBlockNums(value, blockNums);
+            return value;
+        }
+
+        private IDb GetDb(int? topicIndex) => topicIndex.HasValue ? _topicsDbs[topicIndex.Value] : _addressDb;
+
+        private static byte[] CompressDbValue(Span<byte> data)
+        {
+            if (IsCompressed(data, out _))
+                throw ValidationException("Data is already compressed.");
+            if (data.Length % BlockNumSize != 0)
+                throw ValidationException("Invalid data length.");
+
+            // TODO: use same array as destination if possible
+            Span<byte> buffer = new byte[data.Length + BlockNumSize];
+            WriteCompressionMarker(buffer, data.Length / BlockNumSize);
+            ReadOnlySpan<byte> compressed = Compress(data, buffer[BlockNumSize..]);
+
+            compressed = buffer[..(BlockNumSize + compressed.Length)];
+            return compressed.ToArray();
+        }
+
+        private static int[] DecompressDbValue(ReadOnlySpan<byte> data)
+        {
+            if (!IsCompressed(data, out int len))
+                throw new ValidationException("Data is not compressed");
+
+            // TODO: reuse buffer
+            ReadOnlySpan<int> buffer = new int[len + 1]; // +1 fixes TurboPFor reading outside of array bounds
+            buffer = buffer[..^1];
+
+            var result = Decompress(data[BlockNumSize..], buffer);
+            return result.ToArray();
+        }
+
+        private static void ReverseBlocksIfNeeded(Span<byte> data)
+        {
+            if (data.Length != 0 && GetValBlockNum(data) > GetValLastBlockNum(data))
+                MemoryMarshal.Cast<byte, int>(data).Reverse();
+        }
+
+        private static void ReverseBlocksIfNeeded(Span<int> blocks)
+        {
+            if (blocks.Length != 0 && blocks[0] > blocks[^1])
+                blocks.Reverse();
+        }
+
+        private Span<byte> RemoveReorgableBlocks(Span<byte> data)
+        {
+            var lastCompressBlock = GetLastReorgableBlockNumber();
+            var lastCompressIndex = LastBlockSearch(data, lastCompressBlock, false);
+
+            if (lastCompressIndex < 0) lastCompressIndex = 0;
+            if (lastCompressIndex > data.Length) lastCompressIndex = data.Length;
+
+            return data[..lastCompressIndex];
+        }
+
+        private static int LastBlockSearch(ReadOnlySpan<byte> operand, int block, bool isBackward)
+        {
+            if (operand.IsEmpty)
+                return 0;
+
+            var i = operand.Length - BlockNumSize;
+            for (; i >= 0; i -= BlockNumSize)
+            {
+                var currentBlock = GetValBlockNum(operand[i..]);
+                if (currentBlock == block)
+                    return i;
+
+                if (isBackward)
+                {
+                    if (currentBlock > block)
+                        return i + BlockNumSize;
+                }
+                else
+                {
+                    if (currentBlock < block)
+                        return i + BlockNumSize;
+                }
+            }
+
+            return i;
+        }
+
+        // TODO: check if MemoryExtensions.BinarySearch<int> can be used and will be faster
+        private static int BinaryBlockSearch(ReadOnlySpan<byte> data, int target)
+        {
+            if (data.Length == 0)
+                return 0;
+
+            int count = data.Length / sizeof(int);
+            int left = 0, right = count - 1;
+
+            // Short circuits in some cases
+            if (GetValLastBlockNum(data) == target)
+                return right * BlockNumSize;
+            if (GetValBlockNum(data) == target)
+                return 0;
+
+            while (left <= right)
+            {
+                int mid = left + (right - left) / 2;
+                int offset = mid * 4;
+
+                int value = GetValBlockNum(data[offset..]);
+
+                if (value == target)
+                    return offset;
+                if (value < target)
+                    left = mid + 1;
+                else
+                    right = mid - 1;
+            }
+
+            return ~(left * BlockNumSize);
+        }
+
+        private static readonly string[] SizeSuffixes = ["B", "KB", "MB", "GB", "TB", "PB"];
+
+        private static string FormatSize(double size)
+        {
+            int index = 0;
+            while (size >= 1024 && index < SizeSuffixes.Length - 1)
+            {
+                size /= 1024;
+                index++;
+            }
+
+            return $"{size:0.##} {SizeSuffixes[index]}";
+        }
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/LogIndexUpdateStats.cs b/src/Nethermind/Nethermind.Db/LogIndex/LogIndexUpdateStats.cs
new file mode 100644
index 0000000000..2264367967
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/LogIndexUpdateStats.cs
@@ -0,0 +1,112 @@
+// SPDX-FileCopyrightText: 2024 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Threading;
+
+namespace Nethermind.Db;
+
+// TODO: separate stats for forward and backwards sync - should help with debugging
+public class LogIndexUpdateStats(ILogIndexStorage storage) : IFormattable
+{
+    private long _blocksAdded;
+    private long _txAdded;
+    private long _logsAdded;
+    private long _topicsAdded;
+
+    public long BlocksAdded => _blocksAdded;
+    public long TxAdded => _txAdded;
+    public long LogsAdded => _logsAdded;
+    public long TopicsAdded => _topicsAdded;
+
+    public long? MaxBlockNumber => storage.GetMaxBlockNumber();
+    public long? MinBlockNumber => storage.GetMinBlockNumber();
+
+    public ExecTimeStats SetReceipts { get; } = new();
+    public ExecTimeStats Aggregating { get; } = new();
+    public ExecTimeStats Processing { get; } = new();
+
+    public ExecTimeStats CallingMerge { get; } = new();
+    public ExecTimeStats UpdatingMeta { get; } = new();
+    public ExecTimeStats WaitingBatch { get; } = new();
+    public ExecTimeStats InMemoryMerging { get; } = new();
+
+    public AverageStats KeysCount { get; } = new();
+
+    public ExecTimeStats QueueingAddressCompression { get; } = new();
+    public ExecTimeStats QueueingTopicCompression { get; } = new();
+
+    public PostMergeProcessingStats PostMergeProcessing { get; } = new();
+    public CompactingStats Compacting { get; } = new();
+
+    public ExecTimeStats LoadingReceipts { get; } = new();
+
+    public void Combine(LogIndexUpdateStats other)
+    {
+        _blocksAdded += other._blocksAdded;
+        _txAdded += other._txAdded;
+        _logsAdded += other._logsAdded;
+        _topicsAdded += other._topicsAdded;
+
+        SetReceipts.Combine(other.SetReceipts);
+        Aggregating.Combine(other.Aggregating);
+        Processing.Combine(other.Processing);
+        UpdatingMeta.Combine(other.UpdatingMeta);
+        CallingMerge.Combine(other.CallingMerge);
+        WaitingBatch.Combine(other.WaitingBatch);
+        InMemoryMerging.Combine(other.InMemoryMerging);
+        KeysCount.Combine(other.KeysCount);
+
+        QueueingAddressCompression.Combine(other.QueueingAddressCompression);
+        QueueingTopicCompression.Combine(other.QueueingTopicCompression);
+
+        PostMergeProcessing.Combine(other.PostMergeProcessing);
+        Compacting.Combine(other.Compacting);
+
+        LoadingReceipts.Combine(other.LoadingReceipts);
+    }
+
+    public void IncrementBlocks() => Interlocked.Increment(ref _blocksAdded);
+    public void IncrementTx() => Interlocked.Increment(ref _txAdded);
+    public void IncrementLogs() => Interlocked.Increment(ref _logsAdded);
+    public void IncrementTopics() => Interlocked.Increment(ref _topicsAdded);
+
+    public string ToString(string? format, IFormatProvider? formatProvider)
+    {
+        const string tab = "\t";
+
+        return !string.Equals(format, "D", StringComparison.OrdinalIgnoreCase)
+            ? $"{MinBlockNumber:N0} - {MaxBlockNumber:N0} (blocks: +{_blocksAdded:N0} | txs: +{_txAdded:N0} | logs: +{_logsAdded:N0} | topics: +{_topicsAdded:N0})"
+            : $"""
+
+               {tab}Blocks: {MinBlockNumber:N0} - {MaxBlockNumber:N0} (+{_blocksAdded:N0})
+
+               {tab}Txs: +{_txAdded:N0}
+               {tab}Logs: +{_logsAdded:N0}
+               {tab}Topics: +{_topicsAdded:N0}
+
+               {tab}Loading receipts: {LoadingReceipts}
+
+               {tab}Keys per batch: {KeysCount:N0}
+               {tab}SetReceipts: {SetReceipts}
+               {tab}Aggregating: {Aggregating}
+               {tab}Processing: {Processing}
+
+               {tab}Merge call: {CallingMerge}
+               {tab}Updating metadata: {UpdatingMeta}
+               {tab}Waiting batch: {WaitingBatch}
+               {tab}In-memory merging: {InMemoryMerging}
+
+               {tab}Post-merge processing: {PostMergeProcessing.Execution}
+               {tab}{tab}DB getting: {PostMergeProcessing.GettingValue}
+               {tab}{tab}Compressing: {PostMergeProcessing.CompressingValue}
+               {tab}{tab}Putting: {PostMergeProcessing.PuttingValues}
+               {tab}{tab}Compressed keys: {PostMergeProcessing.CompressedAddressKeys:N0} address, {PostMergeProcessing.CompressedTopicKeys:N0} topic
+               {tab}{tab}In queue: {PostMergeProcessing.QueueLength:N0}
+
+               {tab}Compacting: {Compacting.Total}
+               """;
+    }
+
+    public override string ToString() => ToString(null, null);
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/MergeOperator.cs b/src/Nethermind/Nethermind.Db/LogIndex/MergeOperator.cs
new file mode 100644
index 0000000000..b6e3cd0d90
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/MergeOperator.cs
@@ -0,0 +1,145 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Diagnostics;
+using System.Threading;
+using Nethermind.Core.Collections;
+
+namespace Nethermind.Db;
+
+partial class LogIndexStorage
+{
+    // TODO: check if success=false + paranoid_checks=true is better than throwing exception
+    // TODO: tests for MergeOperator specifically?
+    private class MergeOperator(ILogIndexStorage logIndexStorage, ICompressor compressor, int? topicIndex) : IMergeOperator
+    {
+        private LogIndexUpdateStats _stats = new(logIndexStorage);
+        public LogIndexUpdateStats GetAndResetStats() => Interlocked.Exchange(ref _stats, new(logIndexStorage));
+
+        public string Name => $"{nameof(LogIndexStorage)}.{nameof(MergeOperator)}";
+
+        public ArrayPoolList<byte>? FullMerge(ReadOnlySpan<byte> key, RocksDbMergeEnumerator enumerator) =>
+            Merge(key, enumerator, isPartial: false);
+
+        public ArrayPoolList<byte>? PartialMerge(ReadOnlySpan<byte> key, RocksDbMergeEnumerator enumerator) =>
+            Merge(key, enumerator, isPartial: true);
+
+        private static bool IsBlockNewer(int next, int? last, bool isBackwardSync) =>
+            LogIndexStorage.IsBlockNewer(next, last, last, isBackwardSync);
+
+        // Validate we are merging non-intersecting segments - to prevent data corruption
+        // TODO: remove as it's just a time-consuming validation?
+        private static void AddEnsureSorted(ArrayPoolList<byte> result, ReadOnlySpan<byte> value, bool isBackwards)
+        {
+            if (value.Length == 0)
+                return;
+
+            var nextBlock = GetValBlockNum(value);
+            var lastBlock = result.Count > 0 ? GetValLastBlockNum(result.AsSpan()) : (int?)null;
+
+            if (!IsBlockNewer(next: nextBlock, last: lastBlock, isBackwards))
+                throw ValidationException($"Invalid order during merge: {lastBlock} -> {nextBlock} (backwards: {isBackwards}).");
+
+            result.AddRange(value);
+        }
+
+        // TODO: avoid array copying in case of a single value?
+        private ArrayPoolList<byte>? Merge(ReadOnlySpan<byte> key, RocksDbMergeEnumerator enumerator, bool isPartial)
+        {
+            var success = false;
+            ArrayPoolList<byte>? result = null;
+            var timestamp = Stopwatch.GetTimestamp();
+
+            try
+            {
+                // Fast return in case of a single operand
+                if (!enumerator.HasExistingValue && enumerator.OperandsCount == 1 && !MergeOps.IsAny(enumerator.GetOperand(0)))
+                    return new(enumerator.GetOperand(0));
+
+                bool isBackwards = UseBackwardSyncFor(key);
+
+                // Calculate total length
+                var resultLength = enumerator.GetExistingValue().Length;
+                for (var i = 0; i < enumerator.OperandsCount; i++)
+                {
+                    ReadOnlySpan<byte> operand = enumerator.GetOperand(i);
+
+                    if (MergeOps.IsAny(operand))
+                    {
+                        if (isPartial)
+                            return null; // Notify RocksDB that we can't partially merge custom ops
+
+                        continue;
+                    }
+
+                    resultLength += operand.Length;
+                }
+
+                result = new(resultLength);
+
+                // For truncate - just use max/min for all operands
+                var truncateAggregate = Aggregate(MergeOp.TruncateOp, enumerator, isBackwards);
+
+                var iReorg = 0;
+                for (var i = 0; i < enumerator.TotalCount; i++)
+                {
+                    Span<byte> operand = enumerator.Get(i);
+
+                    if (MergeOps.IsAny(operand))
+                        continue;
+
+                    // For reorg - order matters, so we need to always traverse from the current position
+                    iReorg = Math.Max(iReorg, i + 1);
+                    if (FindNext(MergeOp.ReorgOp, enumerator, ref iReorg) is { } reorgBlock)
+                        operand = MergeOps.ApplyTo(operand, MergeOp.ReorgOp, reorgBlock, isBackwards);
+
+                    if (truncateAggregate is { } truncateBlock)
+                        operand = MergeOps.ApplyTo(operand, MergeOp.TruncateOp, truncateBlock, isBackwards);
+
+                    AddEnsureSorted(result, operand, isBackwards);
+                }
+
+                if (result.Count % BlockNumSize != 0)
+                    throw ValidationException("Invalid data length post-merge.");
+
+                compressor.TryEnqueue(topicIndex, key, result.AsSpan());
+
+                success = true;
+                return result;
+            }
+            finally
+            {
+                if (!success) result?.Dispose();
+
+                _stats.InMemoryMerging.Include(Stopwatch.GetElapsedTime(timestamp));
+            }
+        }
+
+        private static int? FindNext(MergeOp op, RocksDbMergeEnumerator enumerator, ref int i)
+        {
+            while (i < enumerator.TotalCount && !MergeOps.Is(op, enumerator.Get(i)))
+                i++;
+
+            if (i < enumerator.TotalCount && MergeOps.Is(op, enumerator.Get(i), out int block))
+                return block;
+
+            return null;
+        }
+
+        private static int? Aggregate(MergeOp op, RocksDbMergeEnumerator enumerator, bool isBackwardSync)
+        {
+            int? result = null;
+            for (var i = 0; i < enumerator.OperandsCount; i++)
+            {
+                if (!MergeOps.Is(op, enumerator.GetOperand(i), out var next))
+                    continue;
+
+                if (result is null || (isBackwardSync && next < result) || (!isBackwardSync && next > result))
+                    result = next;
+            }
+
+            return result;
+        }
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/MergeOps.cs b/src/Nethermind/Nethermind.Db/LogIndex/MergeOps.cs
new file mode 100644
index 0000000000..fcfcdbef1a
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/MergeOps.cs
@@ -0,0 +1,85 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+
+namespace Nethermind.Db;
+
+partial class LogIndexStorage
+{
+    private enum MergeOp : byte
+    {
+        /// <summary>
+        /// Reorgs from the provided block number,
+        /// removing any numbers starting from it.
+        /// </summary>
+        ReorgOp = 1,
+
+        /// <summary>
+        /// Truncates data up to the provided block number,
+        /// removing it and anything coming before.
+        /// </summary>
+        TruncateOp = 2
+    }
+
+    private static class MergeOps
+    {
+        public static bool Is(MergeOp op, ReadOnlySpan<byte> operand)
+        {
+            return operand.Length == BlockNumSize + 1 && operand[0] == (byte)op;
+        }
+
+        public static bool Is(MergeOp op, ReadOnlySpan<byte> operand, out int fromBlock)
+        {
+            if (operand.Length == BlockNumSize + 1 && operand[0] == (byte)op)
+            {
+                fromBlock = GetValLastBlockNum(operand);
+                return true;
+            }
+
+            fromBlock = 0;
+            return false;
+        }
+
+        public static bool IsAny(ReadOnlySpan<byte> operand) =>
+            Is(MergeOp.ReorgOp, operand, out _) ||
+            Is(MergeOp.TruncateOp, operand, out _);
+
+        public static Span<byte> Create(MergeOp op, int fromBlock, Span<byte> buffer)
+        {
+            Span<byte> dbValue = buffer[..(BlockNumSize + 1)];
+            dbValue[0] = (byte)op;
+            SetValBlockNum(dbValue[1..], fromBlock);
+            return dbValue;
+        }
+
+        // TODO: use ArrayPool?
+        public static Span<byte> Create(MergeOp op, int fromBlock)
+        {
+            var buffer = new byte[BlockNumSize + 1];
+            return Create(op, fromBlock, buffer);
+        }
+
+        public static Span<byte> ApplyTo(Span<byte> operand, MergeOp op, int block, bool isBackward)
+        {
+            // In most cases the searched block will be near or at the end of the operand, if present there
+            var i = LastBlockSearch(operand, block, isBackward);
+
+            if (op is MergeOp.ReorgOp)
+            {
+                if (i < 0) return Span<byte>.Empty;
+                if (i >= operand.Length) return operand;
+                return operand[..i];
+            }
+
+            if (op is MergeOp.TruncateOp)
+            {
+                if (i < 0) return operand;
+                if (i >= operand.Length) return Span<byte>.Empty;
+                return operand[(i + BlockNumSize)..];
+            }
+
+            throw new ArgumentOutOfRangeException(nameof(op), op, "Unsupported merge operation.");
+        }
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/LogIndex/PostMergeProcessingStats.cs b/src/Nethermind/Nethermind.Db/LogIndex/PostMergeProcessingStats.cs
new file mode 100644
index 0000000000..aeb29b560c
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/LogIndex/PostMergeProcessingStats.cs
@@ -0,0 +1,29 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+namespace Nethermind.Db;
+
+public class PostMergeProcessingStats
+{
+    public ExecTimeStats GettingValue { get; set; } = new();
+    public ExecTimeStats CompressingValue { get; set; } = new();
+    public ExecTimeStats PuttingValues { get; set; } = new();
+    public int QueueLength { get; set; }
+
+    public long CompressedAddressKeys;
+    public long CompressedTopicKeys;
+
+    public ExecTimeStats Execution { get; } = new();
+
+    public void Combine(PostMergeProcessingStats other)
+    {
+        GettingValue.Combine(other.GettingValue);
+        CompressingValue.Combine(other.CompressingValue);
+        PuttingValues.Combine(other.PuttingValues);
+
+        CompressedAddressKeys += other.CompressedAddressKeys;
+        CompressedTopicKeys += other.CompressedTopicKeys;
+
+        Execution.Combine(other.Execution);
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/MemDb.cs b/src/Nethermind/Nethermind.Db/MemDb.cs
index af6f4313e0..fc674e2276 100644
--- a/src/Nethermind/Nethermind.Db/MemDb.cs
+++ b/src/Nethermind/Nethermind.Db/MemDb.cs
@@ -168,6 +168,21 @@ namespace Nethermind.Db
             };
         }
 
+        public IIterator GetIterator(bool isTailing = false)
+        {
+            throw new NotSupportedException();
+        }
+
+        public IIterator GetIterator(ref IteratorOptions options)
+        {
+            throw new NotSupportedException();
+        }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            throw new NotSupportedException();
+        }
+
         private IEnumerable<KeyValuePair<byte[], byte[]?>> OrderedDb => _db.OrderBy(kvp => kvp.Key, Bytes.Comparer);
     }
 }
diff --git a/src/Nethermind/Nethermind.Db/Nethermind.Db.csproj b/src/Nethermind/Nethermind.Db/Nethermind.Db.csproj
index 04ad5b8b91..2bc743df97 100644
--- a/src/Nethermind/Nethermind.Db/Nethermind.Db.csproj
+++ b/src/Nethermind/Nethermind.Db/Nethermind.Db.csproj
@@ -12,4 +12,20 @@
   <ItemGroup>
     <PackageReference Include="NonBlocking" />
   </ItemGroup>
+
+  <ItemGroup>
+    <!-- TODO: put natives into a separate Nuget package -->
+    <None Update="runtimes\win-x64\native\ic.dll">
+      <CopyToOutputDirectory>Always</CopyToOutputDirectory>
+    </None>
+    <None Update="runtimes\osx-x64\native\libic.dylib">
+      <CopyToOutputDirectory>Always</CopyToOutputDirectory>
+    </None>
+    <None Update="runtimes\osx-arm64\native\libic.dylib">
+      <CopyToOutputDirectory>Always</CopyToOutputDirectory>
+    </None>
+    <None Update="runtimes\linux-x64\native\libic.so">
+      <CopyToOutputDirectory>Always</CopyToOutputDirectory>
+    </None>
+  </ItemGroup>
 </Project>
diff --git a/src/Nethermind/Nethermind.Db/NullDb.cs b/src/Nethermind/Nethermind.Db/NullDb.cs
index 1bb3948fda..4856d2cb7b 100644
--- a/src/Nethermind/Nethermind.Db/NullDb.cs
+++ b/src/Nethermind/Nethermind.Db/NullDb.cs
@@ -60,5 +60,20 @@ namespace Nethermind.Db
         public void Dispose()
         {
         }
+
+        public IIterator GetIterator(bool isTailing = false)
+        {
+            throw new NotSupportedException();
+        }
+
+        public IIterator GetIterator(ref IteratorOptions options)
+        {
+            throw new NotSupportedException();
+        }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            throw new NotSupportedException();
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Db/ReadOnlyDb.cs b/src/Nethermind/Nethermind.Db/ReadOnlyDb.cs
index ff73f260a3..83a394adce 100644
--- a/src/Nethermind/Nethermind.Db/ReadOnlyDb.cs
+++ b/src/Nethermind/Nethermind.Db/ReadOnlyDb.cs
@@ -87,5 +87,20 @@ namespace Nethermind.Db
         public void DangerousReleaseMemory(in ReadOnlySpan<byte> span) { }
 
         public bool PreferWriteByArray => true; // Because of memdb buffer
+
+        public IIterator GetIterator(bool isTailing = false)
+        {
+            return _memDb.GetIterator(isTailing);
+        }
+
+        public IIterator GetIterator(ref IteratorOptions options)
+        {
+            return _memDb.GetIterator(ref options);
+        }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            throw new NotSupportedException();
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Db/RocksDbMergeEnumerator.cs b/src/Nethermind/Nethermind.Db/RocksDbMergeEnumerator.cs
new file mode 100644
index 0000000000..fa2da2060b
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/RocksDbMergeEnumerator.cs
@@ -0,0 +1,51 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+
+namespace Nethermind.Db;
+
+/// <summary>
+/// RocksDB enumerator for values of merge operation.
+/// </summary>
+// Interface was not used because of ref struct limitations.
+public readonly ref struct RocksDbMergeEnumerator(ReadOnlySpan<IntPtr> operandsList, ReadOnlySpan<long> operandsListLength)
+{
+    private readonly ReadOnlySpan<IntPtr> _operandsList = operandsList;
+    private readonly ReadOnlySpan<long> _operandsListLength = operandsListLength;
+
+    public Span<byte> ExistingValue { get; }
+    public bool HasExistingValue { get; }
+    public int OperandsCount => _operandsList.Length;
+    public int TotalCount => OperandsCount + (HasExistingValue ? 1 : 0);
+
+    public RocksDbMergeEnumerator(
+        Span<byte> existingValue, bool hasExistingValue,
+        ReadOnlySpan<IntPtr> operandsList, ReadOnlySpan<long> operandsListLength
+    ) : this(operandsList, operandsListLength)
+    {
+        ExistingValue = existingValue;
+        HasExistingValue = hasExistingValue;
+    }
+
+    public Span<byte> GetExistingValue()
+    {
+        return HasExistingValue ? ExistingValue : default;
+    }
+
+    public unsafe Span<byte> GetOperand(int index)
+    {
+        return new((void*)_operandsList[index], (int)_operandsListLength[index]);
+    }
+
+    public Span<byte> Get(int index)
+    {
+        if (index == 0 && HasExistingValue)
+            return ExistingValue;
+
+        if (HasExistingValue)
+            index -= 1;
+
+        return GetOperand(index);
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/RocksDbSettings.cs b/src/Nethermind/Nethermind.Db/RocksDbSettings.cs
index f3d9eebd4e..5f49d109fc 100644
--- a/src/Nethermind/Nethermind.Db/RocksDbSettings.cs
+++ b/src/Nethermind/Nethermind.Db/RocksDbSettings.cs
@@ -1,6 +1,9 @@
 // SPDX-FileCopyrightText: 2022 Demerzel Solutions Limited
 // SPDX-License-Identifier: LGPL-3.0-only
 
+using System;
+using System.Collections.Generic;
+
 namespace Nethermind.Db
 {
     public class DbSettings
@@ -17,6 +20,9 @@ namespace Nethermind.Db
         public bool DeleteOnStart { get; set; }
         public bool CanDeleteFolder { get; set; } = true;
 
+        public IMergeOperator? MergeOperator { get; set; }
+        public Dictionary<string, IMergeOperator>? MergeOperatorByColumn { get; set; }
+
         public DbSettings Clone(string name, string path)
         {
             DbSettings settings = (DbSettings)MemberwiseClone();
diff --git a/src/Nethermind/Nethermind.Db/SimpleFilePublicKeyDb.cs b/src/Nethermind/Nethermind.Db/SimpleFilePublicKeyDb.cs
index 979ff10526..55389ce2ab 100644
--- a/src/Nethermind/Nethermind.Db/SimpleFilePublicKeyDb.cs
+++ b/src/Nethermind/Nethermind.Db/SimpleFilePublicKeyDb.cs
@@ -325,5 +325,20 @@ namespace Nethermind.Db
         public void Dispose()
         {
         }
+
+        public IIterator GetIterator(bool isTailing = false)
+        {
+            throw new NotSupportedException();
+        }
+
+        public IIterator GetIterator(ref IteratorOptions options)
+        {
+            throw new NotSupportedException();
+        }
+
+        public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+        {
+            throw new NotSupportedException();
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Db/TurboPFor.cs b/src/Nethermind/Nethermind.Db/TurboPFor.cs
new file mode 100644
index 0000000000..48f2d356fe
--- /dev/null
+++ b/src/Nethermind/Nethermind.Db/TurboPFor.cs
@@ -0,0 +1,258 @@
+using System.Runtime.InteropServices;
+using System.Reflection;
+using System.Runtime.Loader;
+using System.IO;
+using System;
+
+namespace Nethermind.Db
+{
+    // https://github.com/brettwooldridge/TurboPFor#function-syntax
+    // https://github.com/brettwooldridge/TurboPFor/blob/master/java/jic.java
+    // https://github.com/brettwooldridge/TurboPFor/blob/master/vp4.h
+    // TODO: move to separate Nuget package
+    // TODO: fix bindings using incorrect types
+    public static class TurboPFor
+    {
+        private const string LibraryName = "ic";
+        private static string? _libraryFallbackPath;
+
+        static TurboPFor() => AssemblyLoadContext.Default.ResolvingUnmanagedDll += OnResolvingUnmanagedDll;
+
+        [DllImport(LibraryName)]
+        public static extern unsafe int vbenc32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public static extern unsafe int vbdec32(byte[] @in, int n, int[] @out);
+
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int vbdenc32(int[] @in, int n, byte[] @out, int start);
+        [DllImport(LibraryName)]
+        public extern unsafe static int vbddec32(byte[] @in, int n, int[] @out, int start);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int vbd1enc32(int[] @in, int n, byte[] @out, int start);
+        [DllImport(LibraryName)]
+        public extern unsafe static int vbd1dec32(byte[] @in, int n, int[] @out, int start);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int vbzenc32(int[] @in, int n, byte[] @out, int start);
+        [DllImport(LibraryName)]
+        public extern unsafe static int vbzdec32(byte[] @in, int n, int[] @out, int start);
+
+        // variable simple
+        [DllImport(LibraryName)]
+        public extern unsafe static int vsenc32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int vsdec32(byte[] @in, int n, int[] @out);
+
+        //************ TurboPFor: PFor/PForDelta
+        // High level API: n unlimited
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nenc32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ndec32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nenc128v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ndec128v32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nenc256v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ndec256v32(byte[] @in, int n, int[] @out);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ndenc32(int[] @in, int n, byte[] @out); // delta 0: increasing integer list (sorted)
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nddec32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ndenc128v32(int* @in, int n, byte* @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nddec128v32(byte* @in, int n, int* @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ndenc256v32(int* @in, int n, byte* @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nddec256v32(byte* @in, int n, int* @out);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nd1enc32(int[] @in, int n, byte[] @out); // delta 1: strictly increasing integer list (sorted)
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nd1dec32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static nuint p4nd1enc128v32(int* @in, nuint n, byte* @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static nuint p4nd1dec128v32(byte* @in, nuint n, int* @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static nuint p4nd1enc256v32(int* @in, nuint n, byte* @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static nuint p4nd1dec256v32(byte* @in, nuint n, int* @out);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nzenc32(int[] @in, int n, byte[] @out); // zigzag: unsorted integer list
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nzdec32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nzenc128v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nzdec128v32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nzenc256v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4nzdec256v32(byte[] @in, int n, int[] @out);
+
+        // Single block: n limited to 128/256
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4enc32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4dec32(byte[] @in, int n, int[] @out);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4enc128v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4dec128v32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4enc256v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4dec256v32(byte[] @in, int n, int[] @out);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ddec32(byte[] @in, int n, int[] @out, int start);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ddec128v32(byte[] @in, int n, int[] @out, int start);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4ddec256v32(byte[] @in, int n, int[] @out, int start);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4d1dec32(byte[] @in, int n, int[] @out, int start);
+        [DllImport(LibraryName)]
+        public extern unsafe static int p4d1dec128v32(byte[] @in, int n, int[] @out, int start);
+        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
+        public extern unsafe static byte* p4d1enc256v32(int* @in, int n, byte* @out, int start);
+        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
+        public extern unsafe static byte* p4d1dec256v32(byte* @in, int n, int* @out, int start);
+
+        //********** bitpack scalar
+        // High level API
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnpack32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnunpack32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnpack128v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnunpack128v32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnpack256v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnunpack256v32(byte[] @in, int n, int[] @out);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitndpack32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitndunpack32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitndpack128v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitndunpack128v32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitndpack256v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitndunpack256v32(byte[] @in, int n, int[] @out);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnd1pack32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnd1unpack32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnd1pack128v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnd1unpack128v32(byte[] @in, int n, int[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnd1pack256v32(int[] @in, int n, byte[] @out);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitnd1unpack256v32(byte[] @in, int n, int[] @out);
+
+        // Low level API - single block limited to 128/256 integers
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitpack32(int[] @in, int n, byte[] @out, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitunpack32(byte[] @in, int n, int[] @out, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitpack128v32(int[] @in, int n, byte[] @out, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitunpack128v32(byte[] @in, int n, int[] @out, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitpack256v32(int[] @in, int n, byte[] @out, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitunpack256v32(byte[] @in, int n, int[] @out, int b);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitdpack32(int[] @in, int n, byte[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitdunpack32(byte[] @in, int n, int[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitdpack128v32(int[] @in, int n, byte[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitdunpack128v32(byte[] @in, int n, int[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitdpack256v32(int[] @in, int n, byte[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitdunpack256v32(byte[] @in, int n, int[] @out, int start, int b);
+
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitd1pack32(int[] @in, int n, byte[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitd1unpack32(byte[] @in, int n, int[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitd1pack128v32(int[] @in, int n, byte[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitd1unpack128v32(byte[] @in, int n, int[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitd1pack256v32(int[] @in, int n, byte[] @out, int start, int b);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitd1unpack256v32(byte[] @in, int n, int[] @out, int start, int b);
+
+        // bitutil
+        [DllImport(LibraryName)]
+        public extern unsafe static int bit32(int[] @in, int n);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitd32(int[] @in, int n, int start);
+        [DllImport(LibraryName)]
+        public extern unsafe static int bitd132(int[] @in, int n, int start);
+
+
+        private static IntPtr OnResolvingUnmanagedDll(Assembly context, string name)
+        {
+            if (!LibraryName.Equals(name, StringComparison.Ordinal))
+                return nint.Zero;
+
+            if (_libraryFallbackPath is null)
+            {
+                string platform;
+
+                if (RuntimeInformation.IsOSPlatform(OSPlatform.Linux))
+                {
+                    name = $"lib{name}.so";
+                    platform = "linux";
+                }
+                else if (RuntimeInformation.IsOSPlatform(OSPlatform.OSX))
+                {
+                    name = $"lib{name}.dylib";
+                    platform = "osx";
+                }
+                else if (RuntimeInformation.IsOSPlatform(OSPlatform.Windows))
+                {
+                    name = $"{name}.dll";
+                    platform = "win";
+                }
+                else
+                    throw new PlatformNotSupportedException();
+
+                var arch = RuntimeInformation.ProcessArchitecture.ToString().ToLowerInvariant();
+
+                _libraryFallbackPath = Path.Combine("runtimes", $"{platform}-{arch}", "native", name);
+            }
+
+            return NativeLibrary.Load(_libraryFallbackPath, context, default);
+        }
+    }
+}
diff --git a/src/Nethermind/Nethermind.Db/runtimes/linux-x64/native/libic.so b/src/Nethermind/Nethermind.Db/runtimes/linux-x64/native/libic.so
new file mode 100644
index 0000000000..2728afa32c
Binary files /dev/null and b/src/Nethermind/Nethermind.Db/runtimes/linux-x64/native/libic.so differ
diff --git a/src/Nethermind/Nethermind.Db/runtimes/osx-arm64/native/libic.dylib b/src/Nethermind/Nethermind.Db/runtimes/osx-arm64/native/libic.dylib
new file mode 100644
index 0000000000..fcfa9d2318
Binary files /dev/null and b/src/Nethermind/Nethermind.Db/runtimes/osx-arm64/native/libic.dylib differ
diff --git a/src/Nethermind/Nethermind.Db/runtimes/osx-x64/native/libic.dylib b/src/Nethermind/Nethermind.Db/runtimes/osx-x64/native/libic.dylib
new file mode 100644
index 0000000000..fcfa9d2318
Binary files /dev/null and b/src/Nethermind/Nethermind.Db/runtimes/osx-x64/native/libic.dylib differ
diff --git a/src/Nethermind/Nethermind.Db/runtimes/win-x64/native/ic.dll b/src/Nethermind/Nethermind.Db/runtimes/win-x64/native/ic.dll
new file mode 100644
index 0000000000..e1f03b25c2
Binary files /dev/null and b/src/Nethermind/Nethermind.Db/runtimes/win-x64/native/ic.dll differ
diff --git a/src/Nethermind/Nethermind.Facade/Filters/LogFilter.cs b/src/Nethermind/Nethermind.Facade/Filters/LogFilter.cs
index fc17885d64..6fdc832c3b 100644
--- a/src/Nethermind/Nethermind.Facade/Filters/LogFilter.cs
+++ b/src/Nethermind/Nethermind.Facade/Filters/LogFilter.cs
@@ -19,6 +19,12 @@ namespace Nethermind.Blockchain.Filters
         public TopicsFilter TopicsFilter { get; } = topicsFilter;
         public BlockParameter FromBlock { get; } = fromBlock;
         public BlockParameter ToBlock { get; } = toBlock;
+        public bool UseIndex { get; set; } = true; // TODO: remove after testing
+
+        public bool AcceptsAnyBlock =>
+            AddressFilter.Address == null &&
+            (AddressFilter.Addresses?.Count ?? 0) == 0 &&
+            TopicsFilter.AcceptsAnyBlock;
 
         public bool Accepts(LogEntry logEntry) => AddressFilter.Accepts(logEntry.Address) && TopicsFilter.Accepts(logEntry);
 
diff --git a/src/Nethermind/Nethermind.Facade/Filters/Topics/AnyTopic.cs b/src/Nethermind/Nethermind.Facade/Filters/Topics/AnyTopic.cs
index f8278d7ff6..05384907f6 100644
--- a/src/Nethermind/Nethermind.Facade/Filters/Topics/AnyTopic.cs
+++ b/src/Nethermind/Nethermind.Facade/Filters/Topics/AnyTopic.cs
@@ -1,6 +1,8 @@
 // SPDX-FileCopyrightText: 2022 Demerzel Solutions Limited
 // SPDX-License-Identifier: LGPL-3.0-only
 
+using System.Collections.Generic;
+using System.Linq;
 using Nethermind.Core;
 using Nethermind.Core.Crypto;
 
@@ -18,6 +20,12 @@ namespace Nethermind.Blockchain.Filters.Topics
         public override bool Matches(Bloom bloom) => true;
         public override bool Matches(ref BloomStructRef bloom) => true;
 
+        public override bool AcceptsAnyBlock => true;
+
+        public override IEnumerable<Hash256> Topics => [];
+
+        public override List<int>? FilterBlockNumbers(IDictionary<Hash256, List<int>> byTopic) => null;
+
         public override string ToString() => "null";
     }
 }
diff --git a/src/Nethermind/Nethermind.Facade/Filters/Topics/AnyTopicsFilter.cs b/src/Nethermind/Nethermind.Facade/Filters/Topics/AnyTopicsFilter.cs
index 60ce110682..b46483190a 100644
--- a/src/Nethermind/Nethermind.Facade/Filters/Topics/AnyTopicsFilter.cs
+++ b/src/Nethermind/Nethermind.Facade/Filters/Topics/AnyTopicsFilter.cs
@@ -2,6 +2,8 @@
 // SPDX-License-Identifier: LGPL-3.0-only
 
 using System;
+using System.Collections.Generic;
+using System.Linq;
 using Nethermind.Blockchain.Receipts;
 using Nethermind.Core;
 using Nethermind.Core.Crypto;
@@ -10,9 +12,10 @@ namespace Nethermind.Blockchain.Filters.Topics
 {
     public class AnyTopicsFilter : TopicsFilter
     {
-
         private readonly TopicExpression[] _expressions;
 
+        public override IReadOnlyList<TopicExpression> Expressions => _expressions.AsReadOnly();
+
         public AnyTopicsFilter(params TopicExpression[] expressions)
         {
             _expressions = expressions;
@@ -94,5 +97,25 @@ namespace Nethermind.Blockchain.Filters.Topics
 
             return result;
         }
+
+        public override bool AcceptsAnyBlock => _expressions.Length == 0 || _expressions.Any(e => e.AcceptsAnyBlock);
+
+        public override IEnumerable<Hash256> Topics => _expressions.SelectMany(e => e.Topics);
+
+        public override List<int> FilterBlockNumbers(IDictionary<Hash256, List<int>>[] byTopic)
+        {
+            List<int>? result = null;
+            for (var i = 0; i < _expressions.Length; i++)
+            {
+                TopicExpression expression = _expressions[i];
+
+                if (result == null)
+                    result = expression.FilterBlockNumbers(byTopic[i]);
+                else if (expression.FilterBlockNumbers(byTopic[i]) is { } next)
+                    result = AscListHelper.Union(result, next);
+            }
+
+            return result ?? [];
+        }
     }
 }
diff --git a/src/Nethermind/Nethermind.Facade/Filters/Topics/OrExpression.cs b/src/Nethermind/Nethermind.Facade/Filters/Topics/OrExpression.cs
index 5b8e3a4fb2..856ed7e9e9 100644
--- a/src/Nethermind/Nethermind.Facade/Filters/Topics/OrExpression.cs
+++ b/src/Nethermind/Nethermind.Facade/Filters/Topics/OrExpression.cs
@@ -2,6 +2,7 @@
 // SPDX-License-Identifier: LGPL-3.0-only
 
 using System;
+using System.Collections.Generic;
 using System.Linq;
 using Nethermind.Core;
 using Nethermind.Core.Crypto;
@@ -69,6 +70,24 @@ namespace Nethermind.Blockchain.Filters.Topics
             return false;
         }
 
+        public override bool AcceptsAnyBlock => _subexpressions.Any(e => e.AcceptsAnyBlock);
+
+        public override IEnumerable<Hash256> Topics => _subexpressions.SelectMany(e => e.Topics);
+
+        public override List<int>? FilterBlockNumbers(IDictionary<Hash256, List<int>> byTopic)
+        {
+            List<int>? result = null;
+            foreach (TopicExpression subexpression in _subexpressions)
+            {
+                if (result == null)
+                    result = subexpression.FilterBlockNumbers(byTopic);
+                else if (subexpression.FilterBlockNumbers(byTopic) is { } next)
+                    result = AscListHelper.Union(result, next);
+            }
+
+            return result ?? [];
+        }
+
         public override bool Equals(object? obj)
         {
             if (obj is null) return false;
diff --git a/src/Nethermind/Nethermind.Facade/Filters/Topics/SequenceTopicsFilter.cs b/src/Nethermind/Nethermind.Facade/Filters/Topics/SequenceTopicsFilter.cs
index 7386afe49a..2cc4f6816e 100644
--- a/src/Nethermind/Nethermind.Facade/Filters/Topics/SequenceTopicsFilter.cs
+++ b/src/Nethermind/Nethermind.Facade/Filters/Topics/SequenceTopicsFilter.cs
@@ -2,6 +2,7 @@
 // SPDX-License-Identifier: LGPL-3.0-only
 
 using System;
+using System.Collections.Generic;
 using System.Linq;
 using Nethermind.Blockchain.Receipts;
 using Nethermind.Core;
@@ -15,6 +16,8 @@ namespace Nethermind.Blockchain.Filters.Topics
 
         private readonly TopicExpression[] _expressions;
 
+        public override IReadOnlyList<TopicExpression> Expressions => _expressions.AsReadOnly();
+
         public SequenceTopicsFilter(params TopicExpression[] expressions)
         {
             _expressions = expressions;
@@ -99,6 +102,26 @@ namespace Nethermind.Blockchain.Filters.Topics
             return result;
         }
 
+        public override bool AcceptsAnyBlock => _expressions.Length == 0 || _expressions.All(e => e.AcceptsAnyBlock);
+
+        public override IEnumerable<Hash256> Topics => _expressions.SelectMany(e => e.Topics);
+
+        public override List<int> FilterBlockNumbers(IDictionary<Hash256, List<int>>[] byTopic)
+        {
+            List<int>? result = null;
+            for (var i = 0; i < _expressions.Length; i++)
+            {
+                TopicExpression expression = _expressions[i];
+
+                if (result == null)
+                    result = expression.FilterBlockNumbers(byTopic[i]);
+                else if (expression.FilterBlockNumbers(byTopic[i]) is { } next)
+                    result = AscListHelper.Intersect(result, next);
+            }
+
+            return result ?? [];
+        }
+
         public bool Equals(SequenceTopicsFilter other) => _expressions.SequenceEqual(other._expressions);
 
         public override bool Equals(object? obj)
diff --git a/src/Nethermind/Nethermind.Facade/Filters/Topics/SpecificTopic.cs b/src/Nethermind/Nethermind.Facade/Filters/Topics/SpecificTopic.cs
index a9dee55bc8..99aac4872d 100644
--- a/src/Nethermind/Nethermind.Facade/Filters/Topics/SpecificTopic.cs
+++ b/src/Nethermind/Nethermind.Facade/Filters/Topics/SpecificTopic.cs
@@ -1,6 +1,7 @@
 // SPDX-FileCopyrightText: 2022 Demerzel Solutions Limited
 // SPDX-License-Identifier: LGPL-3.0-only
 
+using System.Collections.Generic;
 using Nethermind.Core;
 using Nethermind.Core.Crypto;
 
@@ -37,6 +38,18 @@ namespace Nethermind.Blockchain.Filters.Topics
 
         public override bool Matches(ref BloomStructRef bloom) => bloom.Matches(BloomExtract);
 
+        public override bool AcceptsAnyBlock => false;
+
+        public override IEnumerable<Hash256> Topics
+        {
+            get { yield return _topic; }
+        }
+
+        public override List<int>? FilterBlockNumbers(IDictionary<Hash256, List<int>> byTopic)
+        {
+            return byTopic.TryGetValue(_topic, out List<int>? result) ? result : [];
+        }
+
         private bool Equals(SpecificTopic other) => _topic.Equals(other._topic);
 
         public override bool Equals(object? obj)
diff --git a/src/Nethermind/Nethermind.Facade/Filters/Topics/TopicExpression.cs b/src/Nethermind/Nethermind.Facade/Filters/Topics/TopicExpression.cs
index 5193e0bfba..055de92dd3 100644
--- a/src/Nethermind/Nethermind.Facade/Filters/Topics/TopicExpression.cs
+++ b/src/Nethermind/Nethermind.Facade/Filters/Topics/TopicExpression.cs
@@ -1,6 +1,7 @@
 // SPDX-FileCopyrightText: 2022 Demerzel Solutions Limited
 // SPDX-License-Identifier: LGPL-3.0-only
 
+using System.Collections.Generic;
 using Nethermind.Core;
 using Nethermind.Core.Crypto;
 
@@ -15,5 +16,11 @@ namespace Nethermind.Blockchain.Filters.Topics
         public abstract bool Matches(Bloom bloom);
 
         public abstract bool Matches(ref BloomStructRef bloom);
+
+        public abstract bool AcceptsAnyBlock { get; }
+
+        public abstract IEnumerable<Hash256> Topics { get; }
+
+        public abstract List<int>? FilterBlockNumbers(IDictionary<Hash256, List<int>> byTopic);
     }
 }
diff --git a/src/Nethermind/Nethermind.Facade/Filters/Topics/TopicsFilter.cs b/src/Nethermind/Nethermind.Facade/Filters/Topics/TopicsFilter.cs
index 0cdc7ece38..4d8fb30d3f 100644
--- a/src/Nethermind/Nethermind.Facade/Filters/Topics/TopicsFilter.cs
+++ b/src/Nethermind/Nethermind.Facade/Filters/Topics/TopicsFilter.cs
@@ -1,7 +1,9 @@
 // SPDX-FileCopyrightText: 2022 Demerzel Solutions Limited
 // SPDX-License-Identifier: LGPL-3.0-only
 
+using System.Collections.Generic;
 using Nethermind.Core;
+using Nethermind.Core.Crypto;
 
 namespace Nethermind.Blockchain.Filters.Topics
 {
@@ -14,5 +16,13 @@ namespace Nethermind.Blockchain.Filters.Topics
         public abstract bool Matches(Bloom bloom);
 
         public abstract bool Matches(ref BloomStructRef bloom);
+
+        public abstract bool AcceptsAnyBlock { get; }
+
+        public abstract IEnumerable<Hash256> Topics { get; }
+
+        public abstract IReadOnlyList<TopicExpression> Expressions { get; }
+
+        public abstract List<int> FilterBlockNumbers(IDictionary<Hash256, List<int>>[] byTopic);
     }
 }
diff --git a/src/Nethermind/Nethermind.Facade/Find/ILogIndexService.cs b/src/Nethermind/Nethermind.Facade/Find/ILogIndexService.cs
new file mode 100644
index 0000000000..898e012921
--- /dev/null
+++ b/src/Nethermind/Nethermind.Facade/Find/ILogIndexService.cs
@@ -0,0 +1,13 @@
+using System;
+using System.Threading.Tasks;
+using Nethermind.Core.ServiceStopper;
+
+namespace Nethermind.Facade.Find
+{
+    public interface ILogIndexService : IAsyncDisposable, IStoppableService
+    {
+        Task StartAsync();
+        int GetMaxTargetBlockNumber();
+        int GetMinTargetBlockNumber();
+    }
+}
diff --git a/src/Nethermind/Nethermind.Facade/Find/LogFinder.cs b/src/Nethermind/Nethermind.Facade/Find/LogFinder.cs
index 8e6e8b2d2d..8328752996 100644
--- a/src/Nethermind/Nethermind.Facade/Find/LogFinder.cs
+++ b/src/Nethermind/Nethermind.Facade/Find/LogFinder.cs
@@ -11,6 +11,7 @@ using Nethermind.Blockchain.Find;
 using Nethermind.Blockchain.Receipts;
 using Nethermind.Core;
 using Nethermind.Core.Crypto;
+using Nethermind.Db;
 using Nethermind.Db.Blooms;
 using Nethermind.Facade.Filters;
 using Nethermind.Logging;
@@ -31,6 +32,7 @@ namespace Nethermind.Facade.Find
         private readonly int _rpcConfigGetLogsThreads;
         private readonly IBlockFinder _blockFinder;
         private readonly ILogger _logger;
+        private readonly ILogIndexStorage? _logIndexStorage;
 
         public LogFinder(IBlockFinder? blockFinder,
             IReceiptFinder? receiptFinder,
@@ -38,6 +40,7 @@ namespace Nethermind.Facade.Find
             IBloomStorage? bloomStorage,
             ILogManager? logManager,
             IReceiptsRecovery? receiptsRecovery,
+            ILogIndexStorage? logIndexStorage,
             int maxBlockDepth = 1000)
         {
             _blockFinder = blockFinder ?? throw new ArgumentNullException(nameof(blockFinder));
@@ -45,6 +48,7 @@ namespace Nethermind.Facade.Find
             _receiptStorage = receiptStorage ?? throw new ArgumentNullException(nameof(receiptStorage)); ;
             _bloomStorage = bloomStorage ?? throw new ArgumentNullException(nameof(bloomStorage));
             _receiptsRecovery = receiptsRecovery ?? throw new ArgumentNullException(nameof(receiptsRecovery));
+            _logIndexStorage = logIndexStorage;
             _logger = logManager?.GetClassLogger<LogFinder>() ?? throw new ArgumentNullException(nameof(logManager));
             _maxBlockDepth = maxBlockDepth;
             _rpcConfigGetLogsThreads = Math.Max(1, Environment.ProcessorCount / 4);
@@ -52,19 +56,21 @@ namespace Nethermind.Facade.Find
 
         public IEnumerable<FilterLog> FindLogs(LogFilter filter, CancellationToken cancellationToken = default)
         {
-            BlockHeader FindHeader(BlockParameter blockParameter, string name, bool headLimit) =>
-                _blockFinder.FindHeader(blockParameter, headLimit) ?? throw new ResourceNotFoundException($"Block not found: {name} {blockParameter}");
-
             cancellationToken.ThrowIfCancellationRequested();
-            BlockHeader toBlock = FindHeader(filter.ToBlock, nameof(filter.ToBlock), false);
+            BlockHeader toBlock = FindHeader(filter.ToBlock, nameof(filter.ToBlock));
             cancellationToken.ThrowIfCancellationRequested();
             BlockHeader fromBlock = filter.ToBlock == filter.FromBlock ?
                 toBlock :
-                FindHeader(filter.FromBlock, nameof(filter.FromBlock), false);
+                FindHeader(filter.FromBlock, nameof(filter.FromBlock));
 
             return FindLogs(filter, fromBlock, toBlock, cancellationToken);
         }
 
+        private BlockHeader FindHeader(BlockParameter blockParameter, string name) =>
+            _blockFinder.FindHeader(blockParameter) ?? throw new ResourceNotFoundException($"Block not found: {name} {blockParameter}");
+
+        private BlockHeader FindHeader(int number) => FindHeader(new(number), $"{number}");
+
         public IEnumerable<FilterLog> FindLogs(LogFilter filter, BlockHeader fromBlock, BlockHeader toBlock, CancellationToken cancellationToken = default)
         {
             cancellationToken.ThrowIfCancellationRequested();
@@ -87,6 +93,30 @@ namespace Nethermind.Facade.Find
             }
             cancellationToken.ThrowIfCancellationRequested();
 
+            if (CanUseLogIndex(filter, fromBlock, toBlock) is not { } indexRange)
+                return FilterLogsWithoutIndex(filter, fromBlock, toBlock, cancellationToken);
+
+            // Combine results from regular scanning and index
+            IEnumerable<FilterLog>? result = [];
+
+            if (indexRange.from > fromBlock.Number)
+                result = result.Concat(FilterLogsWithoutIndex(filter, fromBlock, FindHeader(indexRange.from - 1), cancellationToken));
+
+            cancellationToken.ThrowIfCancellationRequested();
+
+            result = result.Concat(FilterLogsUsingIndex(filter, FindHeader(indexRange.from), FindHeader(indexRange.to), cancellationToken));
+
+            cancellationToken.ThrowIfCancellationRequested();
+
+            if (indexRange.to < toBlock.Number)
+                result = result.Concat(FilterLogsWithoutIndex(filter, FindHeader(indexRange.to + 1), toBlock, cancellationToken));
+
+            return result;
+        }
+
+        private IEnumerable<FilterLog> FilterLogsWithoutIndex(LogFilter filter, BlockHeader fromBlock, BlockHeader toBlock,
+            CancellationToken cancellationToken)
+        {
             bool shouldUseBloom = ShouldUseBloomDatabase(fromBlock, toBlock);
             bool canUseBloom = CanUseBloomDatabase(toBlock, fromBlock);
             bool useBloom = shouldUseBloom && canUseBloom;
@@ -103,7 +133,7 @@ namespace Nethermind.Facade.Find
 
         private IEnumerable<FilterLog> FilterLogsWithBloomsIndex(LogFilter filter, BlockHeader fromBlock, BlockHeader toBlock, CancellationToken cancellationToken)
         {
-            Hash256 FindBlockHash(long blockNumber, CancellationToken token)
+            Hash256? FindBlockHash(long blockNumber, CancellationToken token)
             {
                 token.ThrowIfCancellationRequested();
                 var blockHash = _blockFinder.FindBlockHash(blockNumber);
@@ -163,6 +193,21 @@ namespace Nethermind.Facade.Find
                 .SelectMany(blockNumber => FindLogsInBlock(filter, FindBlockHash(blockNumber, cancellationToken), blockNumber, cancellationToken));
         }
 
+        // TODO: Do not use for single block after testing - scanning it will be usually faster
+        private (int from, int to)? CanUseLogIndex(LogFilter filter, BlockHeader fromBlock, BlockHeader toBlock)
+        {
+            if (!filter.UseIndex || _logIndexStorage == null || filter.AcceptsAnyBlock)
+                return null;
+
+            if (_logIndexStorage.GetMinBlockNumber() is not { } indexFrom || _logIndexStorage.GetMaxBlockNumber() is not { } indexTo)
+                return null;
+
+            return (
+                Math.Max((int)fromBlock.Number, indexFrom),
+                Math.Min((int)toBlock.Number, indexTo)
+            );
+        }
+
         private bool CanUseBloomDatabase(BlockHeader toBlock, BlockHeader fromBlock)
         {
             // method is designed for convenient debugging
@@ -188,6 +233,74 @@ namespace Nethermind.Facade.Find
             return true;
         }
 
+        private IEnumerable<FilterLog> FilterLogsUsingIndex(LogFilter filter, BlockHeader fromBlock, BlockHeader toBlock,
+            CancellationToken cancellationToken)
+        {
+            // TODO: merge common code with FilterLogsWithBloomsIndex
+
+            Hash256? FindBlockHash(long blockNumber, CancellationToken token)
+            {
+                token.ThrowIfCancellationRequested();
+                var blockHash = _blockFinder.FindBlockHash(blockNumber);
+                if (blockHash is null)
+                {
+                    if (_logger.IsError)
+                        _logger.Error($"Could not find block {blockNumber} in database. eth_getLogs will return incomplete results.");
+                }
+
+                return blockHash;
+            }
+
+            IEnumerable<long> FilterBlocks(LogFilter f, long from, long to, bool runParallel, CancellationToken token)
+            {
+                try
+                {
+                    foreach (var blockNumber in _logIndexStorage!.GetBlockNumbersFor(f, from, to, token))
+                    {
+                        token.ThrowIfCancellationRequested();
+
+                        yield return blockNumber;
+
+                        // TODO: is this needed?
+                        // if (++count >= _maxBlockDepth)
+                        //     break;
+                    }
+                }
+                finally
+                {
+                    if (runParallel)
+                    {
+                        Interlocked.CompareExchange(ref ParallelLock, 0, 1);
+                    }
+
+                    Interlocked.Decrement(ref ParallelExecutions);
+                }
+            }
+
+            // we want to support one parallel eth_getLogs call for maximum performance
+            // we don't want support more than one eth_getLogs call so we don't starve CPU and threads
+            int parallelLock = Interlocked.CompareExchange(ref ParallelLock, 1, 0);
+            int parallelExecutions = Interlocked.Increment(ref ParallelExecutions) - 1;
+            bool canRunParallel = parallelLock == 0;
+
+            IEnumerable<long> filterBlocks = FilterBlocks(filter, fromBlock.Number, toBlock.Number, canRunParallel, cancellationToken);
+
+            if (canRunParallel)
+            {
+                if (_logger.IsTrace) _logger.Trace($"Allowing parallel eth_getLogs, already parallel executions: {parallelExecutions}.");
+                filterBlocks = filterBlocks.AsParallel() // can yield big performance improvements
+                    .AsOrdered() // we want to keep block order
+                    .WithDegreeOfParallelism(_rpcConfigGetLogsThreads); // explicitly provide number of threads
+            }
+            else
+            {
+                if (_logger.IsTrace) _logger.Trace($"Not allowing parallel eth_getLogs, already parallel executions: {parallelExecutions}.");
+            }
+
+            return filterBlocks
+                .SelectMany(blockNumber => FindLogsInBlock(filter, FindBlockHash(blockNumber, cancellationToken), blockNumber, cancellationToken));
+        }
+
         private IEnumerable<FilterLog> FilterLogsIteratively(LogFilter filter, BlockHeader fromBlock, BlockHeader toBlock, CancellationToken cancellationToken)
         {
             int count = 0;
@@ -210,7 +323,7 @@ namespace Nethermind.Facade.Find
                 ? FindLogsInBlock(filter, block.Hash, block.Number, cancellationToken)
                 : [];
 
-        private IEnumerable<FilterLog> FindLogsInBlock(LogFilter filter, Hash256 blockHash, long blockNumber, CancellationToken cancellationToken)
+        private IEnumerable<FilterLog> FindLogsInBlock(LogFilter filter, Hash256? blockHash, long blockNumber, CancellationToken cancellationToken)
         {
             if (blockHash is not null)
             {
diff --git a/src/Nethermind/Nethermind.Facade/Find/LogIndexService.cs b/src/Nethermind/Nethermind.Facade/Find/LogIndexService.cs
new file mode 100644
index 0000000000..6e07638cc3
--- /dev/null
+++ b/src/Nethermind/Nethermind.Facade/Find/LogIndexService.cs
@@ -0,0 +1,494 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System;
+using System.Collections.Concurrent;
+using System.Collections.Generic;
+using System.Diagnostics;
+using System.Diagnostics.CodeAnalysis;
+using System.Threading;
+using System.Threading.Channels;
+using System.Threading.Tasks;
+using System.Threading.Tasks.Dataflow;
+using Nethermind.Blockchain;
+using Nethermind.Blockchain.Receipts;
+using Nethermind.Blockchain.Synchronization;
+using Nethermind.Core;
+using Nethermind.Db;
+using Nethermind.Logging;
+using Timer = System.Timers.Timer;
+
+namespace Nethermind.Facade.Find;
+
+// TODO: move to correct namespace
+// TODO: reduce periodic logging
+public sealed class LogIndexService : ILogIndexService
+{
+    private sealed class ProcessingQueue
+    {
+        private readonly TransformBlock<IReadOnlyList<BlockReceipts>, LogIndexAggregate> _aggregateBlock;
+        private readonly ActionBlock<LogIndexAggregate> _setReceiptsBlock;
+
+        public int QueueCount => _aggregateBlock.InputCount + _setReceiptsBlock.InputCount;
+        public Task WriteAsync(IReadOnlyList<BlockReceipts> batch, CancellationToken cancellation) => _aggregateBlock.SendAsync(batch, cancellation);
+        public Task Completion => Task.WhenAll(_aggregateBlock.Completion, _setReceiptsBlock.Completion);
+
+        public ProcessingQueue(
+            TransformBlock<IReadOnlyList<BlockReceipts>, LogIndexAggregate> aggregateBlock,
+            ActionBlock<LogIndexAggregate> setReceiptsBlock)
+        {
+            _aggregateBlock = aggregateBlock;
+            _setReceiptsBlock = setReceiptsBlock;
+        }
+    }
+
+    private readonly IBlockTree _blockTree;
+    private readonly ISyncConfig _syncConfig;
+    private readonly ILogger _logger;
+
+    private readonly CancellationTokenSource _cancellationSource = new();
+    private CancellationToken CancellationToken => _cancellationSource.Token;
+
+    // TODO: take some/all values from chain config
+    private const int MaxReorgDepth = 8;
+    private const int BatchSize = 256;
+    private const int MaxBatchQueueSize = 4096;
+    private const int MaxAggregateQueueSize = 512;
+    private static readonly int IOParallelism = 16;
+    private static readonly int AggregateParallelism = Math.Max(Environment.ProcessorCount / 2, 1);
+    private static readonly TimeSpan NewBlockWaitTimeout = TimeSpan.FromSeconds(5);
+
+    private readonly ILogIndexStorage _logIndexStorage;
+    private readonly IReceiptFinder _receiptFinder;
+    private readonly IReceiptStorage _receiptStorage;
+    private readonly ProgressLogger _forwardProgressLogger;
+    private readonly ProgressLogger _backwardProgressLogger;
+    private Timer? _progressLoggerTimer;
+
+    // TODO: handle risk or potential reorg loss on restart
+    private readonly Channel<BlockReceipts> _reorgChannel = Channel.CreateUnbounded<BlockReceipts>();
+
+    private readonly TaskCompletionSource<int> _pivotSource = new(TaskCreationOptions.RunContinuationsAsynchronously);
+    private readonly Task<int> _pivotTask;
+
+    [SuppressMessage("ReSharper", "CollectionNeverQueried.Local")]
+    private readonly CompositeDisposable _disposables = new();
+    private readonly List<Task> _tasks = new();
+
+    private Dictionary<bool, ProcessingQueue>? _processingQueues;
+    private ConcurrentDictionary<bool, LogIndexUpdateStats>? _stats;
+
+    public string Description => "log index service";
+
+    public LogIndexService(ILogIndexStorage logIndexStorage, IBlockTree blockTree, ISyncConfig syncConfig,
+        IReceiptFinder receiptFinder, IReceiptStorage receiptStorage, ILogManager logManager)
+    {
+        ArgumentNullException.ThrowIfNull(logIndexStorage);
+        ArgumentNullException.ThrowIfNull(blockTree);
+        ArgumentNullException.ThrowIfNull(receiptFinder);
+        ArgumentNullException.ThrowIfNull(receiptStorage);
+        ArgumentNullException.ThrowIfNull(logManager);
+        ArgumentNullException.ThrowIfNull(syncConfig);
+
+        _logIndexStorage = logIndexStorage;
+        _blockTree = blockTree;
+        _syncConfig = syncConfig;
+        _receiptFinder = receiptFinder;
+        _receiptStorage = receiptStorage;
+        _logger = logManager.GetClassLogger<LogIndexService>();
+
+        _forwardProgressLogger = new(GetLogPrefix(isForward: true), logManager);
+        _backwardProgressLogger = new(GetLogPrefix(isForward: false), logManager);
+
+        _pivotTask = _pivotSource.Task;
+        if (_logIndexStorage.GetMaxBlockNumber() is { } maxNumber)
+            _pivotSource.TrySetResult(maxNumber);
+    }
+
+    public async Task StartAsync()
+    {
+        try
+        {
+            _receiptStorage.AnyReceiptsInserted += OnReceiptsInserted;
+
+            if (!_pivotTask.IsCompleted && _logger.IsInfo)
+                _logger.Info($"{GetLogPrefix()}: waiting for the first block...");
+
+            await _pivotTask;
+
+            _stats = new()
+            {
+                [true] = new(_logIndexStorage),
+                [false] = new(_logIndexStorage)
+            };
+
+            _processingQueues = new()
+            {
+                [true] = BuildQueue(isForward: true),
+                [false] = BuildQueue(isForward: false)
+            };
+
+            _tasks.AddRange(
+                Task.Run(() => DoQueueBlocks(isForward: true), CancellationToken),
+                Task.Run(() => DoQueueBlocks(isForward: false), CancellationToken), // TODO: don't start if old receipts download is disabled
+                _processingQueues[true].Completion,
+                _processingQueues[false].Completion
+            );
+
+            UpdateProgress();
+            LogProgress();
+
+            _disposables.Add(_progressLoggerTimer = new(TimeSpan.FromSeconds(30)));
+            _progressLoggerTimer.AutoReset = true;
+            _progressLoggerTimer.Elapsed += (_, _) => LogProgress();
+            _progressLoggerTimer.Start();
+        }
+        catch (Exception ex)
+        {
+            HandleException(ex);
+        }
+    }
+
+    public async Task StopAsync()
+    {
+        await _cancellationSource.CancelAsync();
+
+        _pivotSource.TrySetCanceled(CancellationToken);
+        _progressLoggerTimer?.Stop();
+
+        foreach (Task task in _tasks)
+        {
+            try
+            {
+                await task;
+            }
+            catch (Exception ex)
+            {
+                HandleException(ex);
+            }
+        }
+
+        await _logIndexStorage.StopAsync();
+    }
+
+    public async ValueTask DisposeAsync()
+    {
+        _disposables.Dispose();
+        await _logIndexStorage.DisposeAsync();
+    }
+
+    private void LogStats(bool isForward)
+    {
+        LogIndexUpdateStats stats = _stats?[isForward];
+
+        if (stats is not { BlocksAdded: > 0 })
+            return;
+
+        _stats[isForward] = new(_logIndexStorage);
+
+        if (_logger.IsInfo) // TODO: log at debug/trace
+            _logger.Info($"{GetLogPrefix(isForward)}: {stats:d}");
+    }
+
+    private void LogProgress()
+    {
+        _forwardProgressLogger.LogProgress(false);
+        LogStats(isForward: true);
+
+        _backwardProgressLogger.LogProgress(false);
+        LogStats(isForward: false);
+    }
+
+    // TODO: add receipts to cache
+    private void OnReceiptsInserted(object? sender, ReceiptsEventArgs args)
+    {
+        // if (args.WasRemoved)
+        // {
+        //     _reorgChannel.Writer.TryWrite(new((int)args.BlockHeader.Number, args.TxReceipts));
+        //     return;
+        // }
+
+        //_logger.Info($"[TRACE] {nameof(OnReceiptsInserted)}: {args.BlockHeader.ToString(BlockHeader.Format.FullHashAndNumber)} [{args.TxReceipts.Length}]");
+
+        var next = (int)args.BlockHeader.Number;
+
+        if (next != 0 && !_pivotTask.IsCompleted && _pivotSource.TrySetResult(next) && _logger.IsInfo)
+            _logger.Info($"{GetLogPrefix()}: using block {next} as pivot.");
+
+        // var (min, max) = (_logIndexStorage.GetMinBlockNumber(), _logIndexStorage.GetMaxBlockNumber());
+        //
+        // if (min is null || next < min)
+        //     _newBackwardBlockEvent.Set();
+        //
+        // if (max is null || next > max)
+        //     _newForwardBlockEvent.Set();
+    }
+
+    public int GetMaxTargetBlockNumber()
+    {
+        return (int)Math.Max(_blockTree.BestKnownNumber - MaxReorgDepth, 0);
+    }
+
+    public int GetMinTargetBlockNumber()
+    {
+        // Block 0 should always be present
+        return (int)(_syncConfig.AncientReceiptsBarrierCalc <= 1 ? 0 : _syncConfig.AncientReceiptsBarrierCalc);
+    }
+
+    private ProcessingQueue BuildQueue(bool isForward)
+    {
+        var aggregateBlock = new TransformBlock<IReadOnlyList<BlockReceipts>, LogIndexAggregate>(
+            batch => Aggregate(batch, isForward),
+            new() {
+                BoundedCapacity = MaxBatchQueueSize / BatchSize, MaxDegreeOfParallelism = AggregateParallelism,
+                CancellationToken = CancellationToken, SingleProducerConstrained = true
+            }
+        );
+
+        var setReceiptsBlock = new ActionBlock<LogIndexAggregate>(
+            aggr => SetReceiptsAsync(aggr, isForward),
+            new()
+            {
+                BoundedCapacity = MaxAggregateQueueSize, MaxDegreeOfParallelism = 1,
+                CancellationToken = CancellationToken, SingleProducerConstrained = true
+            }
+        );
+
+        aggregateBlock.Completion.ContinueWith(t => HandleException(t.Exception), TaskContinuationOptions.OnlyOnFaulted);
+        setReceiptsBlock.Completion.ContinueWith(t => HandleException(t.Exception), TaskContinuationOptions.OnlyOnFaulted);
+
+        aggregateBlock.LinkTo(setReceiptsBlock, new() { PropagateCompletion = true });
+        return new(aggregateBlock, setReceiptsBlock);
+    }
+
+    private void HandleException(Exception? exception)
+    {
+        if (exception is null)
+            return;
+
+        if (exception is OperationCanceledException oc && oc.CancellationToken == CancellationToken)
+            return; // Cancelled
+
+        if (exception is AggregateException a)
+            exception = a.InnerException;
+
+        if (_logger.IsError)
+            _logger.Error($"{GetLogPrefix()} syncing failed. Please restart the client.", exception);
+
+        _cancellationSource.Cancel();
+    }
+
+    private LogIndexAggregate Aggregate(IReadOnlyList<BlockReceipts> batch, bool isForward)
+    {
+        // TODO: remove ordering check to save time?
+        if ((isForward && !IsSeqAsc(batch)) || (!isForward && !IsSeqDesc(batch)))
+            throw new($"{GetLogPrefix(isForward)}: non-ordered batch in queue: ({batch[0]} -> {batch[^1]}).");
+
+        return _logIndexStorage.Aggregate(batch, !isForward, _stats?[isForward]);
+    }
+
+    private async Task SetReceiptsAsync(LogIndexAggregate aggregate, bool isForward)
+    {
+        if (GetNextBlockNumber(_logIndexStorage, isForward) is { } next && next != aggregate.FirstBlockNum)
+            throw new($"{GetLogPrefix(isForward)}: non sequential batches: ({aggregate.FirstBlockNum} instead of {next}).");
+
+        await _logIndexStorage.SetReceiptsAsync(aggregate, !isForward, _stats?[isForward]);
+
+        if (aggregate.LastBlockNum == 0)
+            _receiptStorage.AnyReceiptsInserted -= OnReceiptsInserted;
+
+        UpdateProgress();
+    }
+
+    private async Task DoQueueBlocks(bool isForward)
+    {
+        try
+        {
+            var pivotNumber = await _pivotTask;
+
+            ProcessingQueue queue = _processingQueues![isForward];
+
+            var next = GetNextBlockNumber(isForward);
+            if (next is not { } start)
+            {
+                if (isForward)
+                {
+                    start = pivotNumber;
+                }
+                else
+                {
+                    start = pivotNumber - 1;
+                }
+            }
+
+            var buffer = new BlockReceipts[BatchSize];
+            while (!CancellationToken.IsCancellationRequested)
+            {
+                if (!isForward && start < GetMinTargetBlockNumber())
+                {
+                    if (_logger.IsTrace)
+                        _logger.Trace($"{GetLogPrefix(isForward)}: queued last block");
+
+                    return;
+                }
+
+                var end = isForward ? start + BatchSize - 1 : Math.Max(0, start - BatchSize + 1);
+
+                // from - inclusive, to - exclusive
+                var (from, to) = isForward
+                    ? (start, Math.Min(end, GetMaxTargetBlockNumber()) + 1)
+                    : (end, Math.Max(start, GetMinTargetBlockNumber()) + 1);
+
+                var timestamp = Stopwatch.GetTimestamp();
+                Array.Clear(buffer);
+                ReadOnlySpan<BlockReceipts> batch = GetNextBatch(from, to, buffer, isForward, CancellationToken);
+
+                if (batch.Length == 0)
+                {
+                    next = isForward ? from : to - 1;
+
+                    var block = _blockTree.FindBlock((long)next);
+                    var status = new
+                    {
+                        Block = block,
+                        HasTransactions = block?.Header.HasTransactions,
+                        HasBlock = block == null ? (bool?)null : _receiptStorage.HasBlock(block.Number, block.Hash!),
+                        ReceiptsLength = block == null ? null : _receiptStorage.Get(block)?.Length,
+                        BestKnownNumber = _blockTree.BestKnownNumber
+                    };
+                    _logger.Info($"[TRACE] {GetLogPrefix(isForward)}: waiting for receipts of block {next}: {status}");
+
+                    await Task.Delay(NewBlockWaitTimeout, CancellationToken);
+                    continue;
+                }
+
+                _stats?[isForward].LoadingReceipts.Include(Stopwatch.GetElapsedTime(timestamp));
+
+                start = GetNextBlockNumber(batch[^1].BlockNumber, isForward);
+                await queue.WriteAsync(batch.ToArray(), CancellationToken);
+            }
+        }
+        catch (Exception ex)
+        {
+            HandleException(ex);
+        }
+
+        if (_logger.IsInfo)
+            _logger.Info($"{GetLogPrefix(isForward)}: queueing completed.");
+    }
+
+    private void UpdateProgress()
+    {
+        if (!_pivotTask.IsCompletedSuccessfully) return;
+        var pivotNumber = _pivotTask.Result;
+
+        if (_processingQueues is null) return;
+
+        if (!_forwardProgressLogger.HasEnded)
+        {
+            _forwardProgressLogger.TargetValue = Math.Max(0, _blockTree.BestKnownNumber - MaxReorgDepth - pivotNumber + 1);
+            _forwardProgressLogger.Update(_logIndexStorage.GetMaxBlockNumber() is { } max ? max - pivotNumber + 1 : 0);
+            _forwardProgressLogger.CurrentQueued = _processingQueues[true].QueueCount;
+
+            // if (_forwardProgressLogger.CurrentValue == _forwardProgressLogger.TargetValue)
+            //     _forwardProgressLogger.MarkEnd();
+        }
+
+        if (!_backwardProgressLogger.HasEnded)
+        {
+            _backwardProgressLogger.TargetValue = pivotNumber - GetMinTargetBlockNumber();
+            _backwardProgressLogger.Update(_logIndexStorage.GetMinBlockNumber() is { } min ? pivotNumber - min : 0);
+            _backwardProgressLogger.CurrentQueued = _processingQueues[false].QueueCount;
+
+            if (_backwardProgressLogger.CurrentValue >= _backwardProgressLogger.TargetValue)
+            {
+                _backwardProgressLogger.MarkEnd();
+
+                if (_logger.IsInfo)
+                    _logger.Info($"{GetLogPrefix(isForward: false)}: completed.");
+            }
+        }
+    }
+
+    private static int? GetNextBlockNumber(ILogIndexStorage storage, bool isForward)
+    {
+        return isForward ? storage.GetMaxBlockNumber() + 1 : storage.GetMinBlockNumber() - 1;
+    }
+
+    private int? GetNextBlockNumber(bool isForward) => GetNextBlockNumber(_logIndexStorage, isForward);
+
+    private static int GetNextBlockNumber(int last, bool isForward)
+    {
+        return isForward ? last + 1 : last - 1;
+    }
+
+    private ReadOnlySpan<BlockReceipts> GetNextBatch(int from, int to, BlockReceipts[] buffer, bool isForward, CancellationToken token)
+    {
+        if (to <= from)
+            return ReadOnlySpan<BlockReceipts>.Empty;
+
+        if (to - from > buffer.Length)
+            throw new InvalidOperationException($"{GetLogPrefix()}: buffer size is too small: {buffer.Length} / {to - from}");
+
+        // Check the immediate next block first
+        var nextIndex = isForward ? from : to - 1;
+        buffer[0] = GetBlockReceipts(nextIndex);
+
+        if (buffer[0] == default)
+            return ReadOnlySpan<BlockReceipts>.Empty;
+
+        Parallel.For(from, to, new()
+        {
+            CancellationToken = token,
+            MaxDegreeOfParallelism = IOParallelism
+        }, i =>
+        {
+            var bufferIndex = isForward ? i - from : to - 1 - i;
+            if (buffer[bufferIndex] == default)
+                buffer[bufferIndex] = GetBlockReceipts(i);
+        });
+
+        var endIndex = Array.IndexOf(buffer, default);
+        return endIndex < 0 ? buffer : buffer.AsSpan(..endIndex);
+    }
+
+    // TODO: move to IReceiptStorage as `TryGet`?
+    private BlockReceipts GetBlockReceipts(int i)
+    {
+        if (_blockTree.FindBlock(i, BlockTreeLookupOptions.ExcludeTxHashes) is not { Hash: not null } block)
+            return default;
+
+        if (!block.Header.HasTransactions)
+            return new(i, []);
+
+        TxReceipt[] receipts = _receiptStorage.Get(block) ?? [];
+
+        if (receipts.Length == 0)
+            return default;
+
+        return new(i, receipts);
+    }
+
+    private static bool IsSeqAsc(IReadOnlyList<BlockReceipts> blocks)
+    {
+        int j = blocks.Count - 1;
+        int i = 1, d = blocks[0].BlockNumber;
+        while (i <= j && blocks[i].BlockNumber - i == d) i++;
+        return i > j;
+    }
+
+    private static bool IsSeqDesc(IReadOnlyList<BlockReceipts> blocks)
+    {
+        int j = blocks.Count - 1;
+        int i = 1, d = blocks[0].BlockNumber;
+        while (i <= j && blocks[i].BlockNumber + i == d) i++;
+        return i > j;
+    }
+
+    private static string GetLogPrefix(bool? isForward = null) => isForward switch
+    {
+        true => "Log index sync (Forward)",
+        false => "Log index sync (Backward)",
+        _ => "Log index sync"
+    };
+}
diff --git a/src/Nethermind/Nethermind.Facade/Find/LogIndexStorageRpcExtensions.cs b/src/Nethermind/Nethermind.Facade/Find/LogIndexStorageRpcExtensions.cs
new file mode 100644
index 0000000000..617ce6f2d4
--- /dev/null
+++ b/src/Nethermind/Nethermind.Facade/Find/LogIndexStorageRpcExtensions.cs
@@ -0,0 +1,60 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System.Collections.Generic;
+using System.Linq;
+using System.Threading;
+using Nethermind.Blockchain.Filters;
+using Nethermind.Blockchain.Filters.Topics;
+using Nethermind.Core;
+using Nethermind.Core.Collections;
+using Nethermind.Core.Crypto;
+using Nethermind.Db;
+
+namespace Nethermind.Facade.Find;
+
+public static class LogIndexStorageRpcExtensions
+{
+    // Done sequentially, as with a single address/topic fetching averaging at 0.01s,
+    // using parallelization here introduces more problems than it solves.
+    public static List<int> GetBlockNumbersFor(this ILogIndexStorage storage,
+        LogFilter filter, long fromBlock, long toBlock,
+        CancellationToken cancellationToken = default)
+    {
+        (int from, int to) = ((int)fromBlock, (int)toBlock);
+
+        List<int>? addressNumbers = null;
+        if (filter.AddressFilter.Address is { } address)
+            addressNumbers = storage.GetBlockNumbersFor(address, from, to);
+        else if (filter.AddressFilter.Addresses is { Count: > 0 } addresses)
+            addressNumbers = AscListHelper.UnionAll(addresses.Select(a => storage.GetBlockNumbersFor(a, from, to)));
+
+        // TODO: consider passing storage directly to keep abstractions
+        var topicIndex = 0;
+        Dictionary<Hash256, List<int>>[]? byTopic = null;
+        foreach (TopicExpression expression in filter.TopicsFilter.Expressions)
+        {
+            byTopic ??= new Dictionary<Hash256, List<int>>[LogIndexStorage.MaxTopics];
+            byTopic[topicIndex] = new();
+
+            foreach (Hash256 topic in expression.Topics)
+            {
+                var i = topicIndex;
+                byTopic[topicIndex].GetOrAdd(topic, _ => storage.GetBlockNumbersFor(i, topic, from, to));
+            }
+
+            topicIndex++;
+        }
+
+        if (byTopic is null)
+            return addressNumbers ?? [];
+
+        // ReSharper disable once CoVariantArrayConversion
+        List<int> topicNumbers = filter.TopicsFilter.FilterBlockNumbers(byTopic);
+
+        if (addressNumbers is null)
+            return topicNumbers;
+
+        return AscListHelper.Intersect(addressNumbers, topicNumbers);
+    }
+}
diff --git a/src/Nethermind/Nethermind.Init/Modules/BuiltInStepsModule.cs b/src/Nethermind/Nethermind.Init/Modules/BuiltInStepsModule.cs
index 8c75493148..fb7ce63ec1 100644
--- a/src/Nethermind/Nethermind.Init/Modules/BuiltInStepsModule.cs
+++ b/src/Nethermind/Nethermind.Init/Modules/BuiltInStepsModule.cs
@@ -33,6 +33,7 @@ public class BuiltInStepsModule : Module
         typeof(StartBlockProcessor),
         typeof(StartBlockProducer),
         typeof(StartMonitoring),
+        typeof(StartLogIndexService)
     ];
 
     protected override void Load(ContainerBuilder builder)
diff --git a/src/Nethermind/Nethermind.Init/Modules/RpcModules.cs b/src/Nethermind/Nethermind.Init/Modules/RpcModules.cs
index 2e954df724..c7812c2e14 100644
--- a/src/Nethermind/Nethermind.Init/Modules/RpcModules.cs
+++ b/src/Nethermind/Nethermind.Init/Modules/RpcModules.cs
@@ -13,6 +13,7 @@ using Nethermind.Core;
 using Nethermind.Core.Timers;
 using Nethermind.Facade;
 using Nethermind.Facade.Eth;
+using Nethermind.Facade.Find;
 using Nethermind.Facade.Simulate;
 using Nethermind.Init.Steps.Migrations;
 using Nethermind.JsonRpc;
@@ -21,6 +22,7 @@ using Nethermind.JsonRpc.Modules.Admin;
 using Nethermind.JsonRpc.Modules.DebugModule;
 using Nethermind.JsonRpc.Modules.Eth;
 using Nethermind.JsonRpc.Modules.Eth.FeeHistory;
+using Nethermind.JsonRpc.Modules.LogIndex;
 using Nethermind.JsonRpc.Modules.Net;
 using Nethermind.JsonRpc.Modules.Parity;
 using Nethermind.JsonRpc.Modules.Personal;
@@ -59,6 +61,8 @@ public class RpcModules(IJsonRpcConfig jsonRpcConfig) : Module
             .RegisterSingletonJsonRpcModule<IWeb3RpcModule, Web3RpcModule>()
             .RegisterSingletonJsonRpcModule<IPersonalRpcModule, PersonalRpcModule>()
             .RegisterSingletonJsonRpcModule<IRpcRpcModule, RpcRpcModule>()
+            .RegisterSingletonJsonRpcModule<ILogIndexRpcModule, LogIndexRpcModule>()
+            .AddSingleton<ILogIndexService, LogIndexService>() // TODO: relocate registration?
 
             // Txpool rpc
             .RegisterSingletonJsonRpcModule<ITxPoolRpcModule, TxPoolRpcModule>()
diff --git a/src/Nethermind/Nethermind.Init/Steps/DatabaseMigrations.cs b/src/Nethermind/Nethermind.Init/Steps/DatabaseMigrations.cs
index d5fc14e560..54b175f9aa 100644
--- a/src/Nethermind/Nethermind.Init/Steps/DatabaseMigrations.cs
+++ b/src/Nethermind/Nethermind.Init/Steps/DatabaseMigrations.cs
@@ -29,6 +29,7 @@ namespace Nethermind.Init.Steps
             yield return new BloomMigration(_api);
             yield return new ReceiptMigration(_api);
             yield return new ReceiptFixMigration(_api);
+            //yield return new LogIndexMigration(_api);
             yield return new TotalDifficultyFixMigration(_api.ChainLevelInfoRepository, _api.BlockTree, _api.Config<ISyncConfig>(), _api.LogManager);
         }
     }
diff --git a/src/Nethermind/Nethermind.Init/Steps/InitializeBlockTree.cs b/src/Nethermind/Nethermind.Init/Steps/InitializeBlockTree.cs
index 504799c621..1ad16690e4 100644
--- a/src/Nethermind/Nethermind.Init/Steps/InitializeBlockTree.cs
+++ b/src/Nethermind/Nethermind.Init/Steps/InitializeBlockTree.cs
@@ -4,6 +4,7 @@
 using System.IO;
 using System.Threading;
 using System.Threading.Tasks;
+using Autofac;
 using Nethermind.Api;
 using Nethermind.Api.Steps;
 using Nethermind.Blockchain;
@@ -13,6 +14,7 @@ using Nethermind.Blockchain.Receipts;
 using Nethermind.Blockchain.Synchronization;
 using Nethermind.Consensus;
 using Nethermind.Core;
+using Nethermind.Core.ServiceStopper;
 using Nethermind.Db;
 using Nethermind.Db.Blooms;
 using Nethermind.Facade.Find;
@@ -24,11 +26,13 @@ namespace Nethermind.Init.Steps
     [RunnerStepDependencies(typeof(InitTxTypesAndRlp), typeof(InitDatabase), typeof(MigrateConfigs), typeof(SetupKeyStore))]
     public class InitializeBlockTree : IStep
     {
+        private readonly IServiceStopper _stopper;
         private readonly IBasicApi _get;
         private readonly IApiWithStores _set;
 
-        public InitializeBlockTree(INethermindApi api)
+        public InitializeBlockTree(INethermindApi api, IServiceStopper stopper)
         {
+            _stopper = stopper;
             (_get, _set) = api.ForInit;
         }
 
@@ -94,6 +98,14 @@ namespace Nethermind.Init.Steps
 
             IReceiptFinder receiptFinder = _set.ReceiptFinder;
 
+            ILogIndexStorage logIndexStorage = _set.LogIndexStorage = new LogIndexStorage(
+                _get.Context.Resolve<IDbFactory>(),
+                _get.LogManager,
+                receiptConfig.LogIndexIOParallelism,
+                receiptConfig.LogIndexCompactionDistance
+            );
+            _get.DisposeStack.Push(logIndexStorage);
+
             LogFinder logFinder = new(
                 blockTree,
                 receiptFinder,
@@ -101,6 +113,7 @@ namespace Nethermind.Init.Steps
                 bloomStorage,
                 _get.LogManager,
                 new ReceiptsRecovery(_get.EthereumEcdsa, _get.SpecProvider),
+                logIndexStorage,
                 receiptConfig.MaxBlockDepth);
 
             _set.LogFinder = logFinder;
diff --git a/src/Nethermind/Nethermind.Init/Steps/Migrations/ReceiptMigration.cs b/src/Nethermind/Nethermind.Init/Steps/Migrations/ReceiptMigration.cs
index 169c2f2daa..c69bcc2cba 100644
--- a/src/Nethermind/Nethermind.Init/Steps/Migrations/ReceiptMigration.cs
+++ b/src/Nethermind/Nethermind.Init/Steps/Migrations/ReceiptMigration.cs
@@ -165,7 +165,7 @@ namespace Nethermind.Init.Steps.Migrations
 
             try
             {
-                int parallelism = _receiptConfig.ReceiptsMigrationDegreeOfParallelism;
+                int parallelism = _receiptConfig.LogIndexIOParallelism;
                 if (parallelism == 0)
                 {
                     parallelism = Environment.ProcessorCount;
diff --git a/src/Nethermind/Nethermind.Init/Steps/StartLogIndexService.cs b/src/Nethermind/Nethermind.Init/Steps/StartLogIndexService.cs
new file mode 100644
index 0000000000..4b69bb21a4
--- /dev/null
+++ b/src/Nethermind/Nethermind.Init/Steps/StartLogIndexService.cs
@@ -0,0 +1,27 @@
+// SPDX-FileCopyrightText: 2022 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System.Threading;
+using System.Threading.Tasks;
+using Nethermind.Api;
+using Nethermind.Api.Steps;
+using Nethermind.Core.ServiceStopper;
+using Nethermind.Facade.Find;
+
+namespace Nethermind.Init.Steps
+{
+    [RunnerStepDependencies(typeof(StartBlockProcessor))]
+    public class StartLogIndexService(IBasicApi api, IServiceStopper serviceStopper, ILogIndexService logIndexService) : IStep
+    {
+        public Task Execute(CancellationToken cancellationToken)
+        {
+            _ = logIndexService.StartAsync();
+            api.DisposeStack.Push(logIndexService);
+            serviceStopper.AddStoppable(logIndexService);
+
+            return Task.CompletedTask;
+        }
+
+        public bool MustInitialize => false;
+    }
+}
diff --git a/src/Nethermind/Nethermind.JsonRpc/Modules/Eth/EthRpcModule.cs b/src/Nethermind/Nethermind.JsonRpc/Modules/Eth/EthRpcModule.cs
index 21594a0e9c..a78f6dea39 100644
--- a/src/Nethermind/Nethermind.JsonRpc/Modules/Eth/EthRpcModule.cs
+++ b/src/Nethermind/Nethermind.JsonRpc/Modules/Eth/EthRpcModule.cs
@@ -628,6 +628,10 @@ public partial class EthRpcModule(
         {
             LogFilter logFilter = _blockchainBridge.GetFilter(fromBlock, toBlock, filter.Address, filter.Topics);
 
+            // ReSharper disable once ConditionIsAlwaysTrueOrFalse - can be null in tests
+            if (logFilter is not null && filter is not null)
+                logFilter.UseIndex = filter.UseIndex;
+
             IEnumerable<FilterLog> filterLogs = _blockchainBridge.GetLogs(logFilter, fromBlockHeader, toBlockHeader, cancellationToken);
 
             ArrayPoolList<FilterLog> logs = new(_rpcConfig.MaxLogsPerResponse);
diff --git a/src/Nethermind/Nethermind.JsonRpc/Modules/Eth/Filter.cs b/src/Nethermind/Nethermind.JsonRpc/Modules/Eth/Filter.cs
index 0b3a0284bd..01603c319b 100644
--- a/src/Nethermind/Nethermind.JsonRpc/Modules/Eth/Filter.cs
+++ b/src/Nethermind/Nethermind.JsonRpc/Modules/Eth/Filter.cs
@@ -19,6 +19,8 @@ public class Filter : IJsonRpcParam
 
     public IEnumerable<object?>? Topics { get; set; }
 
+    public bool UseIndex { get; set; } = true; // TODO: remove after testing
+
     public void ReadJson(JsonElement filter, JsonSerializerOptions options)
     {
         JsonDocument doc = null;
@@ -59,6 +61,16 @@ public class Filter : IJsonRpcParam
             {
                 Topics = null;
             }
+
+            if (filter.TryGetProperty("useIndex"u8, out JsonElement useIndex))
+            {
+                UseIndex = useIndex.ValueKind switch
+                {
+                    JsonValueKind.False => false,
+                    JsonValueKind.True => true,
+                    _ => UseIndex
+                };
+            }
         }
         finally
         {
diff --git a/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/ILogIndexRpcModule.cs b/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/ILogIndexRpcModule.cs
new file mode 100644
index 0000000000..d90fddcba2
--- /dev/null
+++ b/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/ILogIndexRpcModule.cs
@@ -0,0 +1,24 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System.Collections.Generic;
+using Nethermind.JsonRpc.Modules.Eth;
+
+namespace Nethermind.JsonRpc.Modules.LogIndex;
+
+[RpcModule(ModuleType.LogIndex)]
+public interface ILogIndexRpcModule : IRpcModule
+{
+    [JsonRpcMethod(Description = "Retrieves log index keys (and optionally values) for the given address/topic.", IsImplemented = true, IsSharable = true)]
+    ResultWrapper<Dictionary<byte[], int[]>> logIndex_keys(
+        [JsonRpcParameter(ExampleValue = "{\"key\":\"0xC02aaA39b223FE8D0A0e5C4F27eAD9083C756Cc2\"}")] LogIndexKeysRequest request
+    );
+
+    [JsonRpcMethod(Description = "Retrieves log index block number for the given filter.", IsImplemented = true, IsSharable = true)]
+    ResultWrapper<int[]> logIndex_blockNumbers(
+        [JsonRpcParameter] Filter filter
+    );
+
+    [JsonRpcMethod(Description = "Retrieves log index status.", IsImplemented = true, IsSharable = true)]
+    ResultWrapper<LogIndexStatus> logIndex_status();
+}
diff --git a/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexKeysRequest.cs b/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexKeysRequest.cs
new file mode 100644
index 0000000000..3d76b0ae65
--- /dev/null
+++ b/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexKeysRequest.cs
@@ -0,0 +1,45 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System.Text.Json;
+using Nethermind.Blockchain.Find;
+using Nethermind.Core;
+using Nethermind.Core.Crypto;
+using Nethermind.Core.Extensions;
+using Nethermind.JsonRpc.Data;
+
+namespace Nethermind.JsonRpc.Modules.LogIndex;
+
+public class LogIndexKeysRequest : IJsonRpcParam
+{
+    public Address? Address { get; set; }
+    public Hash256? Topic { get; set; }
+    public int? TopicIndex { get; set; }
+
+    public BlockParameter FromBlock { get; set; } = BlockParameter.Earliest;
+
+    public BlockParameter ToBlock { get; set; } = BlockParameter.Latest;
+
+    public bool IncludeValues { get; set; } = false;
+
+    public void ReadJson(JsonElement json, JsonSerializerOptions options)
+    {
+        if (json.TryGetProperty("fromBlock"u8, out JsonElement fromBlockElement))
+            FromBlock = BlockParameterConverter.GetBlockParameter(fromBlockElement.ToString());
+
+        if (json.TryGetProperty("toBlock"u8, out JsonElement toBlockElement))
+            ToBlock = BlockParameterConverter.GetBlockParameter(toBlockElement.ToString());
+
+        if (json.TryGetProperty("address"u8, out JsonElement addressElement) && addressElement.ValueKind == JsonValueKind.String)
+            Address = new(Bytes.FromHexString(addressElement.ToString()));
+
+        if (json.TryGetProperty("topic"u8, out JsonElement topicElement) && topicElement.ValueKind == JsonValueKind.String)
+            Topic = new(Bytes.FromHexString(topicElement.ToString()));
+
+        if (json.TryGetProperty("topicIndex"u8, out JsonElement topicIndexElement) && topicIndexElement.ValueKind == JsonValueKind.Number)
+            TopicIndex = topicIndexElement.GetInt32();
+
+        if (json.TryGetProperty("includeValues"u8, out JsonElement valuesElement))
+            IncludeValues = valuesElement.GetBoolean();
+    }
+}
diff --git a/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexRpcModule.cs b/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexRpcModule.cs
new file mode 100644
index 0000000000..ba8175d20e
--- /dev/null
+++ b/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexRpcModule.cs
@@ -0,0 +1,69 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+using System.Collections.Generic;
+using Nethermind.Blockchain.Filters;
+using Nethermind.Blockchain.Find;
+using Nethermind.Db;
+using Nethermind.Facade;
+using Nethermind.Facade.Find;
+using Nethermind.JsonRpc.Modules.Eth;
+
+namespace Nethermind.JsonRpc.Modules.LogIndex;
+
+public class LogIndexRpcModule(ILogIndexStorage storage, ILogIndexService service, IBlockFinder blockFinder, IBlockchainBridge blockchainBridge) : ILogIndexRpcModule
+{
+    public ResultWrapper<Dictionary<byte[], int[]>> logIndex_keys(LogIndexKeysRequest request)
+    {
+        if (request.Address is { } address)
+        {
+            return ResultWrapper<Dictionary<byte[], int[]>>.Success(
+                storage.GetKeysFor(address, GetBlockNumber(request.FromBlock), GetBlockNumber(request.ToBlock), request.IncludeValues)
+            );
+        }
+
+        if (request.Topic is { } topic)
+        {
+            return ResultWrapper<Dictionary<byte[], int[]>>.Success(
+                storage.GetKeysFor(request.TopicIndex ?? 0, topic, GetBlockNumber(request.FromBlock), GetBlockNumber(request.ToBlock), request.IncludeValues)
+            );
+        }
+
+        return ResultWrapper<Dictionary<byte[], int[]>>.Fail("No address or topic specified.");
+    }
+
+    public ResultWrapper<int[]> logIndex_blockNumbers(Filter filter)
+    {
+        LogFilter logFilter = blockchainBridge.GetFilter(filter.FromBlock!, filter.ToBlock!, filter.Address, filter.Topics);
+
+        return ResultWrapper<int[]>.Success(
+            storage.GetBlockNumbersFor(logFilter, GetBlockNumber(logFilter.FromBlock), GetBlockNumber(logFilter.ToBlock)).ToArray()
+        );
+    }
+
+    public ResultWrapper<LogIndexStatus> logIndex_status()
+    {
+        return ResultWrapper<LogIndexStatus>.Success(new()
+        {
+            Current = new()
+            {
+                FromBlock = storage.GetMinBlockNumber(),
+                ToBlock = storage.GetMaxBlockNumber()
+            },
+            Target = new()
+            {
+                FromBlock = service.GetMinTargetBlockNumber(),
+                ToBlock = service.GetMaxTargetBlockNumber()
+            },
+            DbSize = storage.GetDbSize()
+        });
+    }
+
+    private int GetBlockNumber(BlockParameter parameter)
+    {
+        if (parameter.BlockNumber is { } number)
+            return (int)number;
+
+        return (int?)blockFinder.FindBlock(parameter)?.Number ?? 0;
+    }
+}
diff --git a/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexStatus.cs b/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexStatus.cs
new file mode 100644
index 0000000000..5eef4b0d2b
--- /dev/null
+++ b/src/Nethermind/Nethermind.JsonRpc/Modules/LogIndex/LogIndexStatus.cs
@@ -0,0 +1,18 @@
+// SPDX-FileCopyrightText: 2025 Demerzel Solutions Limited
+// SPDX-License-Identifier: LGPL-3.0-only
+
+namespace Nethermind.JsonRpc.Modules.LogIndex;
+
+// TODO: add forward/backward sync status?
+public class LogIndexStatus
+{
+    public class Range
+    {
+        public required int? FromBlock { get; init; }
+        public required int? ToBlock { get; init; }
+    }
+
+    public required Range Current { get; init; }
+    public required Range Target { get; init; }
+    public required string DbSize { get; init; }
+}
diff --git a/src/Nethermind/Nethermind.JsonRpc/Modules/ModuleType.cs b/src/Nethermind/Nethermind.JsonRpc/Modules/ModuleType.cs
index 33fc863029..ce9275039d 100644
--- a/src/Nethermind/Nethermind.JsonRpc/Modules/ModuleType.cs
+++ b/src/Nethermind/Nethermind.JsonRpc/Modules/ModuleType.cs
@@ -14,6 +14,7 @@ namespace Nethermind.JsonRpc.Modules
         public const string Debug = nameof(Debug);
         public const string Erc20 = nameof(Erc20);
         public const string Eth = nameof(Eth);
+        public const string LogIndex = nameof(LogIndex);
         public const string Evm = nameof(Evm);
         public const string Flashbots = nameof(Flashbots);
         public const string Net = nameof(Net);
@@ -40,6 +41,7 @@ namespace Nethermind.JsonRpc.Modules
             Debug,
             Erc20,
             Eth,
+            LogIndex,
             Evm,
             Flashbots,
             Net,
diff --git a/src/Nethermind/Nethermind.Runner.Test/Ethereum/ContextWithMocks.cs b/src/Nethermind/Nethermind.Runner.Test/Ethereum/ContextWithMocks.cs
index e27165c875..e7b17dea44 100644
--- a/src/Nethermind/Nethermind.Runner.Test/Ethereum/ContextWithMocks.cs
+++ b/src/Nethermind/Nethermind.Runner.Test/Ethereum/ContextWithMocks.cs
@@ -117,6 +117,7 @@ namespace Nethermind.Runner.Test.Ethereum
             api.EngineSigner = Substitute.For<ISigner>();
             api.FileSystem = Substitute.For<IFileSystem>();
             api.KeyStore = Substitute.For<IKeyStore>();
+            api.LogIndexStorage = Substitute.For<ILogIndexStorage>();
             api.LogFinder = Substitute.For<ILogFinder>();
             api.ProtocolsManager = Substitute.For<IProtocolsManager>();
             api.ProtocolValidator = Substitute.For<IProtocolValidator>();
diff --git a/src/Nethermind/Nethermind.Runner.Test/Ethereum/Steps/Migrations/ReceiptMigrationTests.cs b/src/Nethermind/Nethermind.Runner.Test/Ethereum/Steps/Migrations/ReceiptMigrationTests.cs
index 02a695382d..52a7aff42e 100644
--- a/src/Nethermind/Nethermind.Runner.Test/Ethereum/Steps/Migrations/ReceiptMigrationTests.cs
+++ b/src/Nethermind/Nethermind.Runner.Test/Ethereum/Steps/Migrations/ReceiptMigrationTests.cs
@@ -161,6 +161,7 @@ namespace Nethermind.Runner.Test.Ethereum.Steps.Migrations
 
 #pragma warning disable CS0067
             public event EventHandler<BlockReplacementEventArgs> ReceiptsInserted;
+            public event EventHandler<ReceiptsEventArgs> AnyReceiptsInserted;
 #pragma warning restore CS0067
         }
     }
diff --git a/src/Nethermind/Nethermind.Trie.Test/Pruning/TreeStoreTests.cs b/src/Nethermind/Nethermind.Trie.Test/Pruning/TreeStoreTests.cs
index bc54752d98..c89242ad2c 100644
--- a/src/Nethermind/Nethermind.Trie.Test/Pruning/TreeStoreTests.cs
+++ b/src/Nethermind/Nethermind.Trie.Test/Pruning/TreeStoreTests.cs
@@ -601,6 +601,11 @@ namespace Nethermind.Trie.Test.Pruning
                 {
                     _inBatched[key.ToArray()] = value;
                 }
+
+                public void Merge(ReadOnlySpan<byte> key, ReadOnlySpan<byte> value, WriteFlags flags = WriteFlags.None)
+                {
+                    throw new NotSupportedException("Merging is not supported by this implementation.");
+                }
             }
         }
 
